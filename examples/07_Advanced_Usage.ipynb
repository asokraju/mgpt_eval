{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Advanced Usage & Production Deployment\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers advanced topics for deploying MGPT-Eval in production environments, optimizing performance, and integrating with existing systems.\n",
    "\n",
    "## üè≠ Production Deployment\n",
    "\n",
    "### Production-Ready Configuration\n",
    "\n",
    "```yaml\n",
    "# production_config.yaml\n",
    "job:\n",
    "  name: \"production_mgpt_eval_${TIMESTAMP}\"  # Dynamic naming\n",
    "  output_dir: \"/data/mgpt_eval_results\"\n",
    "  random_seed: 42\n",
    "\n",
    "model_api:\n",
    "  base_url: \"${MGPT_API_URL}\"               # Environment variable\n",
    "  batch_size: 64                            # Optimized for production\n",
    "  timeout: 300\n",
    "  max_retries: 5                            # More resilient\n",
    "\n",
    "# Memory-optimized settings\n",
    "data_processing:\n",
    "  output_format: \"csv\"                      # Efficient format\n",
    "  max_sequence_length: 512\n",
    "\n",
    "embedding_generation:\n",
    "  batch_size: 32\n",
    "  save_interval: 50                         # Frequent checkpoints\n",
    "  resume_from_checkpoint: true\n",
    "\n",
    "# Production logging\n",
    "logging:\n",
    "  level: \"INFO\"\n",
    "  console_level: \"WARNING\"                  # Minimal console output\n",
    "  file: \"/logs/mgpt_eval_${TIMESTAMP}.log\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Deployment\n",
    "\n",
    "#### Dockerfile\n",
    "```dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    curl \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy requirements and install\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Create output directories\n",
    "RUN mkdir -p /data/outputs /data/logs\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONPATH=/app\n",
    "ENV MGPT_EVAL_OUTPUT_DIR=/data/outputs\n",
    "ENV MGPT_EVAL_LOG_DIR=/data/logs\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n",
    "  CMD python -c \"import sys; sys.exit(0)\"\n",
    "\n",
    "# Default command\n",
    "CMD [\"python\", \"main.py\", \"--help\"]\n",
    "```\n",
    "\n",
    "#### Docker Compose\n",
    "```yaml\n",
    "# docker-compose.yml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  mgpt-eval:\n",
    "    build: .\n",
    "    volumes:\n",
    "      - ./data:/data\n",
    "      - ./configs:/app/configs\n",
    "      - ./outputs:/data/outputs\n",
    "    environment:\n",
    "      - MGPT_API_URL=http://mgpt-server:8000\n",
    "      - TIMESTAMP=${TIMESTAMP:-$(date +%Y%m%d_%H%M%S)}\n",
    "    depends_on:\n",
    "      - mgpt-server\n",
    "    command: >\n",
    "      python main.py run-all \n",
    "      --config /app/configs/production_config.yaml\n",
    "\n",
    "  mgpt-server:\n",
    "    image: your-mgpt-server:latest\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - MODEL_PATH=/models/mgpt\n",
    "    volumes:\n",
    "      - ./models:/models\n",
    "```\n",
    "\n",
    "#### Deployment Commands\n",
    "```bash\n",
    "# Build and deploy\n",
    "export TIMESTAMP=$(date +%Y%m%d_%H%M%S)\n",
    "docker-compose up --build\n",
    "\n",
    "# Run specific job\n",
    "docker-compose run mgpt-eval python main.py run-all \\\n",
    "  --config /app/configs/diabetes_evaluation.yaml\n",
    "\n",
    "# Check logs\n",
    "docker-compose logs -f mgpt-eval\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kubernetes Deployment\n",
    "\n",
    "#### Job Template\n",
    "```yaml\n",
    "# k8s-job.yaml\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: mgpt-eval-job\n",
    "  labels:\n",
    "    app: mgpt-eval\n",
    "spec:\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: mgpt-eval\n",
    "    spec:\n",
    "      restartPolicy: Never\n",
    "      containers:\n",
    "      - name: mgpt-eval\n",
    "        image: your-registry/mgpt-eval:latest\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"4Gi\"\n",
    "            cpu: \"2\"\n",
    "          limits:\n",
    "            memory: \"8Gi\"\n",
    "            cpu: \"4\"\n",
    "        env:\n",
    "        - name: MGPT_API_URL\n",
    "          value: \"http://mgpt-service:8000\"\n",
    "        - name: JOB_NAME\n",
    "          value: \"evaluation-$(date +%Y%m%d-%H%M%S)\"\n",
    "        volumeMounts:\n",
    "        - name: data-volume\n",
    "          mountPath: /data\n",
    "        - name: config-volume\n",
    "          mountPath: /app/configs\n",
    "        command:\n",
    "        - python\n",
    "        - main.py\n",
    "        - run-all\n",
    "        - --config\n",
    "        - /app/configs/production_config.yaml\n",
    "      volumes:\n",
    "      - name: data-volume\n",
    "        persistentVolumeClaim:\n",
    "          claimName: mgpt-eval-data\n",
    "      - name: config-volume\n",
    "        configMap:\n",
    "          name: mgpt-eval-config\n",
    "```\n",
    "\n",
    "#### ConfigMap\n",
    "```yaml\n",
    "# k8s-configmap.yaml\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: mgpt-eval-config\n",
    "data:\n",
    "  production_config.yaml: |\n",
    "    job:\n",
    "      name: \"production_eval\"\n",
    "      output_dir: \"/data/outputs\"\n",
    "    model_api:\n",
    "      base_url: \"http://mgpt-service:8000\"\n",
    "      batch_size: 64\n",
    "    # ... rest of configuration\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Performance Optimization\n",
    "\n",
    "### Large-Scale Data Processing\n",
    "\n",
    "#### Chunked Processing for Very Large Datasets\n",
    "```python\n",
    "# scripts/chunk_processor.py\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def process_large_dataset(dataset_path, chunk_size=5000, output_dir=\"outputs\"):\n",
    "    \"\"\"Process large dataset in chunks to avoid memory issues.\"\"\"\n",
    "    \n",
    "    # Load and split dataset\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    total_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size else 0)\n",
    "    \n",
    "    chunk_results = []\n",
    "    \n",
    "    for i in range(total_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, len(df))\n",
    "        \n",
    "        # Create chunk\n",
    "        chunk = df[start_idx:end_idx]\n",
    "        chunk_file = f\"chunk_{i}.csv\"\n",
    "        chunk.to_csv(chunk_file, index=False)\n",
    "        \n",
    "        # Process chunk\n",
    "        config = create_chunk_config(chunk_file, f\"chunk_{i}_eval\")\n",
    "        result = subprocess.run([\n",
    "            \"python\", \"main.py\", \"run-all\", \"--config\", config\n",
    "        ], capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            chunk_results.append(f\"outputs/chunk_{i}_eval\")\n",
    "            print(f\"Chunk {i+1}/{total_chunks} completed successfully\")\n",
    "        else:\n",
    "            print(f\"Chunk {i+1}/{total_chunks} failed: {result.stderr}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        os.remove(chunk_file)\n",
    "    \n",
    "    # Aggregate results\n",
    "    aggregate_chunk_results(chunk_results, output_dir)\n",
    "\n",
    "def create_chunk_config(chunk_file, job_name):\n",
    "    \"\"\"Create configuration for chunk processing.\"\"\"\n",
    "    config = f\"\"\"\n",
    "input:\n",
    "  dataset_path: \"{chunk_file}\"\n",
    "  split_ratio: 0.8\n",
    "\n",
    "job:\n",
    "  name: \"{job_name}\"\n",
    "  output_dir: \"outputs\"\n",
    "\n",
    "# Optimized for chunks\n",
    "model_api:\n",
    "  batch_size: 32\n",
    "  timeout: 300\n",
    "\n",
    "pipeline_stages:\n",
    "  embeddings: true\n",
    "  classification: true\n",
    "  evaluation: true\n",
    "  target_word_eval: false  # Skip for chunks\n",
    "  summary_report: true\n",
    "  method_comparison: false\n",
    "\n",
    "data_processing:\n",
    "  output_format: \"csv\"  # Memory efficient\n",
    "\"\"\"\n",
    "    config_file = f\"chunk_config_{job_name}.yaml\"\n",
    "    with open(config_file, 'w') as f:\n",
    "        f.write(config)\n",
    "    return config_file\n",
    "```\n",
    "\n",
    "#### Parallel Processing\n",
    "```python\n",
    "# scripts/parallel_processor.py\n",
    "import concurrent.futures\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def run_parallel_evaluation(configs, max_workers=4):\n",
    "    \"\"\"Run multiple evaluations in parallel.\"\"\"\n",
    "    \n",
    "    def run_single_eval(config_path):\n",
    "        \"\"\"Run single evaluation job.\"\"\"\n",
    "        result = subprocess.run([\n",
    "            \"python\", \"main.py\", \"run-all\", \"--config\", config_path\n",
    "        ], capture_output=True, text=True)\n",
    "        \n",
    "        return {\n",
    "            \"config\": config_path,\n",
    "            \"success\": result.returncode == 0,\n",
    "            \"output\": result.stdout,\n",
    "            \"error\": result.stderr\n",
    "        }\n",
    "    \n",
    "    # Run evaluations in parallel\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(run_single_eval, config): config for config in configs}\n",
    "        \n",
    "        results = []\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            config = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                print(f\"Completed: {config} - {'Success' if result['success'] else 'Failed'}\")\n",
    "            except Exception as exc:\n",
    "                print(f\"Failed: {config} generated an exception: {exc}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage example\n",
    "configs = [\n",
    "    \"configs/diabetes_eval.yaml\",\n",
    "    \"configs/cardiovascular_eval.yaml\", \n",
    "    \"configs/respiratory_eval.yaml\"\n",
    "]\n",
    "\n",
    "results = run_parallel_evaluation(configs, max_workers=3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Monitoring & Optimization\n",
    "\n",
    "#### Memory Monitoring\n",
    "```python\n",
    "# scripts/resource_monitor.py\n",
    "import psutil\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class ResourceMonitor:\n",
    "    def __init__(self, log_file=\"resource_usage.log\"):\n",
    "        self.log_file = log_file\n",
    "        self.monitoring = False\n",
    "    \n",
    "    def start_monitoring(self, interval=30):\n",
    "        \"\"\"Start monitoring system resources.\"\"\"\n",
    "        self.monitoring = True\n",
    "        \n",
    "        with open(self.log_file, 'w') as f:\n",
    "            f.write(\"timestamp,cpu_percent,memory_percent,memory_used_gb,disk_usage_percent\\n\")\n",
    "        \n",
    "        while self.monitoring:\n",
    "            stats = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"cpu_percent\": psutil.cpu_percent(interval=1),\n",
    "                \"memory_percent\": psutil.virtual_memory().percent,\n",
    "                \"memory_used_gb\": psutil.virtual_memory().used / (1024**3),\n",
    "                \"disk_usage_percent\": psutil.disk_usage('/').percent\n",
    "            }\n",
    "            \n",
    "            with open(self.log_file, 'a') as f:\n",
    "                f.write(f\"{stats['timestamp']},{stats['cpu_percent']},{stats['memory_percent']},{stats['memory_used_gb']:.2f},{stats['disk_usage_percent']}\\n\")\n",
    "            \n",
    "            time.sleep(interval)\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Stop monitoring.\"\"\"\n",
    "        self.monitoring = False\n",
    "\n",
    "# Usage in production\n",
    "monitor = ResourceMonitor()\n",
    "monitor.start_monitoring(interval=60)  # Monitor every minute\n",
    "```\n",
    "\n",
    "#### Adaptive Batch Sizing\n",
    "```python\n",
    "# utils/adaptive_batching.py\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "class AdaptiveBatchSizer:\n",
    "    def __init__(self, initial_batch_size=32, memory_threshold=80):\n",
    "        self.current_batch_size = initial_batch_size\n",
    "        self.memory_threshold = memory_threshold\n",
    "        self.performance_history = []\n",
    "    \n",
    "    def get_optimal_batch_size(self):\n",
    "        \"\"\"Get optimal batch size based on current system state.\"\"\"\n",
    "        memory_usage = psutil.virtual_memory().percent\n",
    "        \n",
    "        if memory_usage > self.memory_threshold:\n",
    "            # Reduce batch size if memory usage is high\n",
    "            self.current_batch_size = max(1, self.current_batch_size // 2)\n",
    "            print(f\"Memory usage {memory_usage}% > {self.memory_threshold}%, reducing batch size to {self.current_batch_size}\")\n",
    "        elif memory_usage < self.memory_threshold - 20:\n",
    "            # Increase batch size if memory usage is low\n",
    "            self.current_batch_size = min(128, self.current_batch_size * 2)\n",
    "            print(f\"Memory usage {memory_usage}% is low, increasing batch size to {self.current_batch_size}\")\n",
    "        \n",
    "        return self.current_batch_size\n",
    "    \n",
    "    def record_performance(self, batch_size, processing_time, memory_peak):\n",
    "        \"\"\"Record performance metrics for optimization.\"\"\"\n",
    "        self.performance_history.append({\n",
    "            'batch_size': batch_size,\n",
    "            'processing_time': processing_time,\n",
    "            'memory_peak': memory_peak,\n",
    "            'throughput': batch_size / processing_time\n",
    "        })\n",
    "        \n",
    "        # Keep only recent history\n",
    "        if len(self.performance_history) > 100:\n",
    "            self.performance_history = self.performance_history[-100:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Automated Workflows\n",
    "\n",
    "### Scheduled Evaluations\n",
    "\n",
    "#### Cron-based Scheduling\n",
    "```bash\n",
    "# /etc/cron.d/mgpt-eval\n",
    "# Run daily evaluation at 2 AM\n",
    "0 2 * * * mgpt-user cd /opt/mgpt-eval && python scripts/daily_evaluation.py\n",
    "\n",
    "# Run weekly comprehensive evaluation on Sundays at midnight\n",
    "0 0 * * 0 mgpt-user cd /opt/mgpt-eval && python scripts/weekly_evaluation.py\n",
    "```\n",
    "\n",
    "#### Daily Evaluation Script\n",
    "```python\n",
    "# scripts/daily_evaluation.py\n",
    "import os\n",
    "import subprocess\n",
    "import smtplib\n",
    "from email.mime.text import MimeText\n",
    "from email.mime.multipart import MimeMultipart\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def run_daily_evaluation():\n",
    "    \"\"\"Run daily model evaluation and send report.\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    job_name = f\"daily_eval_{timestamp}\"\n",
    "    \n",
    "    # Configuration for daily evaluation\n",
    "    config = {\n",
    "        \"job\": {\n",
    "            \"name\": job_name,\n",
    "            \"output_dir\": \"/data/daily_evaluations\"\n",
    "        },\n",
    "        \"input\": {\n",
    "            \"dataset_path\": \"/data/latest/medical_claims.csv\"\n",
    "        },\n",
    "        \"pipeline_stages\": {\n",
    "            \"embeddings\": True,\n",
    "            \"classification\": True,\n",
    "            \"evaluation\": True,\n",
    "            \"target_word_eval\": True,\n",
    "            \"summary_report\": True,\n",
    "            \"method_comparison\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save configuration\n",
    "    config_file = f\"daily_config_{timestamp}.yaml\"\n",
    "    with open(config_file, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "    \n",
    "    try:\n",
    "        # Run evaluation\n",
    "        result = subprocess.run([\n",
    "            \"python\", \"main.py\", \"run-all\", \"--config\", config_file\n",
    "        ], capture_output=True, text=True, timeout=3600)  # 1 hour timeout\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            # Load results\n",
    "            summary_file = f\"/data/daily_evaluations/{job_name}/summary/pipeline_summary.json\"\n",
    "            with open(summary_file) as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            send_success_report(job_name, results)\n",
    "        else:\n",
    "            send_failure_report(job_name, result.stderr)\n",
    "    \n",
    "    except subprocess.TimeoutExpired:\n",
    "        send_failure_report(job_name, \"Evaluation timed out after 1 hour\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        send_failure_report(job_name, str(e))\n",
    "    \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        os.remove(config_file)\n",
    "\n",
    "def send_success_report(job_name, results):\n",
    "    \"\"\"Send success notification with results.\"\"\"\n",
    "    best_accuracy = results.get('best_results', {}).get('embedding_method', {}).get('accuracy', 0)\n",
    "    \n",
    "    subject = f\"‚úÖ Daily MGPT Evaluation Success - {job_name}\"\n",
    "    body = f\"\"\"\n",
    "Daily MGPT evaluation completed successfully!\n",
    "\n",
    "Job: {job_name}\n",
    "Best Accuracy: {best_accuracy:.1%}\n",
    "Total Samples: {results.get('data_summary', {}).get('total_samples', 'N/A')}\n",
    "\n",
    "Full results available at: /data/daily_evaluations/{job_name}/\n",
    "\"\"\"\n",
    "    \n",
    "    send_email(subject, body)\n",
    "\n",
    "def send_failure_report(job_name, error):\n",
    "    \"\"\"Send failure notification.\"\"\"\n",
    "    subject = f\"‚ùå Daily MGPT Evaluation Failed - {job_name}\"\n",
    "    body = f\"\"\"\n",
    "Daily MGPT evaluation failed!\n",
    "\n",
    "Job: {job_name}\n",
    "Error: {error}\n",
    "\n",
    "Please check the logs for more details.\n",
    "\"\"\"\n",
    "    \n",
    "    send_email(subject, body)\n",
    "\n",
    "def send_email(subject, body):\n",
    "    \"\"\"Send email notification.\"\"\"\n",
    "    # Email configuration from environment variables\n",
    "    smtp_server = os.getenv('SMTP_SERVER', 'localhost')\n",
    "    smtp_port = int(os.getenv('SMTP_PORT', '587'))\n",
    "    smtp_user = os.getenv('SMTP_USER')\n",
    "    smtp_password = os.getenv('SMTP_PASSWORD')\n",
    "    to_emails = os.getenv('NOTIFICATION_EMAILS', '').split(',')\n",
    "    \n",
    "    if not to_emails or not smtp_user:\n",
    "        print(\"Email configuration not found, skipping notification\")\n",
    "        return\n",
    "    \n",
    "    msg = MimeMultipart()\n",
    "    msg['From'] = smtp_user\n",
    "    msg['To'] = ', '.join(to_emails)\n",
    "    msg['Subject'] = subject\n",
    "    \n",
    "    msg.attach(MimeText(body, 'plain'))\n",
    "    \n",
    "    try:\n",
    "        server = smtplib.SMTP(smtp_server, smtp_port)\n",
    "        server.starttls()\n",
    "        server.login(smtp_user, smtp_password)\n",
    "        text = msg.as_string()\n",
    "        server.sendmail(smtp_user, to_emails, text)\n",
    "        server.quit()\n",
    "        print(\"Notification email sent successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send email: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_daily_evaluation()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CI/CD Integration\n",
    "\n",
    "#### GitHub Actions Workflow\n",
    "```yaml\n",
    "# .github/workflows/model-evaluation.yml\n",
    "name: Model Evaluation Pipeline\n",
    "\n",
    "on:\n",
    "  schedule:\n",
    "    - cron: '0 6 * * *'  # Daily at 6 AM UTC\n",
    "  workflow_dispatch:     # Manual trigger\n",
    "    inputs:\n",
    "      dataset_path:\n",
    "        description: 'Path to dataset file'\n",
    "        required: true\n",
    "        default: 'data/medical_claims.csv'\n",
    "      target_codes:\n",
    "        description: 'Target codes (comma-separated)'\n",
    "        required: true\n",
    "        default: 'E119,I10,N6320'\n",
    "\n",
    "jobs:\n",
    "  evaluate-model:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    strategy:\n",
    "      matrix:\n",
    "        evaluation_type: [\"embeddings\", \"target_words\", \"full_pipeline\"]\n",
    "    \n",
    "    steps:\n",
    "    - name: Checkout code\n",
    "      uses: actions/checkout@v3\n",
    "    \n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v4\n",
    "      with:\n",
    "        python-version: '3.9'\n",
    "    \n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        pip install -r requirements.txt\n",
    "    \n",
    "    - name: Download test data\n",
    "      run: |\n",
    "        # Download or generate test data\n",
    "        wget ${{ secrets.TEST_DATA_URL }} -O data/test_claims.csv\n",
    "    \n",
    "    - name: Start mock MGPT server\n",
    "      run: |\n",
    "        python tests/fake_api_server.py &\n",
    "        sleep 10  # Wait for server to start\n",
    "    \n",
    "    - name: Run evaluation\n",
    "      env:\n",
    "        MGPT_API_URL: http://localhost:8000\n",
    "      run: |\n",
    "        python main.py run-all --config configs/templates/04_full_pipeline.yaml\n",
    "    \n",
    "    - name: Upload results\n",
    "      uses: actions/upload-artifact@v3\n",
    "      with:\n",
    "        name: evaluation-results-${{ matrix.evaluation_type }}\n",
    "        path: outputs/\n",
    "    \n",
    "    - name: Publish results\n",
    "      if: success()\n",
    "      run: |\n",
    "        # Parse results and create summary\n",
    "        python scripts/create_github_summary.py\n",
    "    \n",
    "    - name: Notify on failure\n",
    "      if: failure()\n",
    "      uses: 8398a7/action-slack@v3\n",
    "      with:\n",
    "        status: failure\n",
    "        text: 'MGPT Evaluation failed for ${{ matrix.evaluation_type }}'\n",
    "      env:\n",
    "        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n",
    "```\n",
    "\n",
    "#### Jenkins Pipeline\n",
    "```groovy\n",
    "// Jenkinsfile\n",
    "pipeline {\n",
    "    agent any\n",
    "    \n",
    "    parameters {\n",
    "        choice(\n",
    "            name: 'EVALUATION_TYPE',\n",
    "            choices: ['full_pipeline', 'embeddings_only', 'target_words_only'],\n",
    "            description: 'Type of evaluation to run'\n",
    "        )\n",
    "        string(\n",
    "            name: 'DATASET_PATH',\n",
    "            defaultValue: 'data/medical_claims.csv',\n",
    "            description: 'Path to dataset file'\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    environment {\n",
    "        MGPT_API_URL = credentials('mgpt-api-url')\n",
    "        NOTIFICATION_EMAILS = credentials('notification-emails')\n",
    "    }\n",
    "    \n",
    "    stages {\n",
    "        stage('Checkout') {\n",
    "            steps {\n",
    "                checkout scm\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        stage('Setup') {\n",
    "            steps {\n",
    "                sh '''\n",
    "                    python -m venv venv\n",
    "                    source venv/bin/activate\n",
    "                    pip install -r requirements.txt\n",
    "                '''\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        stage('Data Validation') {\n",
    "            steps {\n",
    "                script {\n",
    "                    def dataExists = fileExists(params.DATASET_PATH)\n",
    "                    if (!dataExists) {\n",
    "                        error \"Dataset file not found: ${params.DATASET_PATH}\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        stage('Run Evaluation') {\n",
    "            steps {\n",
    "                sh '''\n",
    "                    source venv/bin/activate\n",
    "                    python main.py run-all --config configs/templates/${EVALUATION_TYPE}.yaml\n",
    "                '''\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        stage('Archive Results') {\n",
    "            steps {\n",
    "                archiveArtifacts artifacts: 'outputs/**/*', fingerprint: true\n",
    "                publishHTML([\n",
    "                    allowMissing: false,\n",
    "                    alwaysLinkToLastBuild: true,\n",
    "                    keepAll: true,\n",
    "                    reportDir: 'outputs',\n",
    "                    reportFiles: '*/summary/pipeline_summary.json',\n",
    "                    reportName: 'Evaluation Results'\n",
    "                ])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    post {\n",
    "        success {\n",
    "            emailext (\n",
    "                subject: \"‚úÖ MGPT Evaluation Success - ${env.BUILD_NUMBER}\",\n",
    "                body: \"\"\"MGPT evaluation completed successfully!\n",
    "                \n",
    "                Build: ${env.BUILD_NUMBER}\n",
    "                Type: ${params.EVALUATION_TYPE}\n",
    "                Dataset: ${params.DATASET_PATH}\n",
    "                \n",
    "                View results: ${env.BUILD_URL}\"\"\",\n",
    "                to: \"${env.NOTIFICATION_EMAILS}\"\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        failure {\n",
    "            emailext (\n",
    "                subject: \"‚ùå MGPT Evaluation Failed - ${env.BUILD_NUMBER}\",\n",
    "                body: \"\"\"MGPT evaluation failed!\n",
    "                \n",
    "                Build: ${env.BUILD_NUMBER}\n",
    "                Type: ${params.EVALUATION_TYPE}\n",
    "                \n",
    "                View logs: ${env.BUILD_URL}\"\"\",\n",
    "                to: \"${env.NOTIFICATION_EMAILS}\"\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        always {\n",
    "            cleanWs()\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Integration with Analytics Platforms\n",
    "\n",
    "### MLflow Integration\n",
    "\n",
    "```python\n",
    "# utils/mlflow_integration.py\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class MLflowLogger:\n",
    "    def __init__(self, experiment_name=\"mgpt-eval\"):\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    def log_evaluation_run(self, job_name, config, results_dir):\n",
    "        \"\"\"Log complete evaluation run to MLflow.\"\"\"\n",
    "        \n",
    "        with mlflow.start_run(run_name=job_name):\n",
    "            # Log configuration parameters\n",
    "            self._log_config_params(config)\n",
    "            \n",
    "            # Log metrics from all methods\n",
    "            self._log_classification_metrics(results_dir)\n",
    "            self._log_target_word_metrics(results_dir)\n",
    "            \n",
    "            # Log models\n",
    "            self._log_trained_models(results_dir)\n",
    "            \n",
    "            # Log artifacts\n",
    "            mlflow.log_artifacts(str(results_dir), \"evaluation_results\")\n",
    "    \n",
    "    def _log_config_params(self, config):\n",
    "        \"\"\"Log configuration parameters.\"\"\"\n",
    "        # Flatten nested config for logging\n",
    "        flat_params = self._flatten_dict(config)\n",
    "        \n",
    "        for key, value in flat_params.items():\n",
    "            if isinstance(value, (str, int, float, bool)):\n",
    "                mlflow.log_param(key, value)\n",
    "    \n",
    "    def _log_classification_metrics(self, results_dir):\n",
    "        \"\"\"Log classification metrics.\"\"\"\n",
    "        metrics_dir = Path(results_dir) / \"metrics\"\n",
    "        \n",
    "        for classifier_dir in metrics_dir.glob(\"*\"):\n",
    "            if classifier_dir.is_dir() and classifier_dir.name != \"target_word_evaluation\":\n",
    "                metrics_file = classifier_dir / \"metrics.json\"\n",
    "                if metrics_file.exists():\n",
    "                    with open(metrics_file) as f:\n",
    "                        metrics = json.load(f)\n",
    "                    \n",
    "                    classifier_name = classifier_dir.name\n",
    "                    test_metrics = metrics.get(\"test_performance\", {})\n",
    "                    \n",
    "                    for metric, value in test_metrics.items():\n",
    "                        mlflow.log_metric(f\"{classifier_name}_{metric}\", value)\n",
    "    \n",
    "    def _log_target_word_metrics(self, results_dir):\n",
    "        \"\"\"Log target word evaluation metrics.\"\"\"\n",
    "        target_metrics_file = Path(results_dir) / \"metrics\" / \"target_word_evaluation\" / \"target_word_eval_summary.json\"\n",
    "        \n",
    "        if target_metrics_file.exists():\n",
    "            with open(target_metrics_file) as f:\n",
    "                metrics = json.load(f)\n",
    "            \n",
    "            overall_metrics = metrics.get(\"overall_metrics\", {})\n",
    "            for metric, value in overall_metrics.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    mlflow.log_metric(f\"target_word_{metric}\", value)\n",
    "    \n",
    "    def _log_trained_models(self, results_dir):\n",
    "        \"\"\"Log trained models.\"\"\"\n",
    "        models_dir = Path(results_dir) / \"models\"\n",
    "        \n",
    "        for model_file in models_dir.glob(\"*.pkl\"):\n",
    "            # Load and log model\n",
    "            import pickle\n",
    "            with open(model_file, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            \n",
    "            model_name = model_file.stem.replace('_model', '')\n",
    "            mlflow.sklearn.log_model(model, model_name)\n",
    "    \n",
    "    def _flatten_dict(self, d, parent_key='', sep='_'):\n",
    "        \"\"\"Flatten nested dictionary.\"\"\"\n",
    "        items = []\n",
    "        for k, v in d.items():\n",
    "            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "            if isinstance(v, dict):\n",
    "                items.extend(self._flatten_dict(v, new_key, sep=sep).items())\n",
    "            else:\n",
    "                items.append((new_key, v))\n",
    "        return dict(items)\n",
    "\n",
    "# Usage example\n",
    "logger = MLflowLogger(\"mgpt-diabetes-evaluation\")\n",
    "logger.log_evaluation_run(\n",
    "    job_name=\"diabetes_eval_20240115\",\n",
    "    config=config_dict,\n",
    "    results_dir=\"outputs/diabetes_eval_20240115\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Weights & Biases Integration\n",
    "\n",
    "```python\n",
    "# utils/wandb_integration.py\n",
    "import wandb\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "class WandBLogger:\n",
    "    def __init__(self, project_name=\"mgpt-eval\"):\n",
    "        self.project_name = project_name\n",
    "    \n",
    "    def log_evaluation_run(self, job_name, config, results_dir):\n",
    "        \"\"\"Log evaluation run to Weights & Biases.\"\"\"\n",
    "        \n",
    "        run = wandb.init(\n",
    "            project=self.project_name,\n",
    "            name=job_name,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Log metrics and artifacts\n",
    "            self._log_metrics(results_dir)\n",
    "            self._log_visualizations(results_dir)\n",
    "            self._log_data_summary(results_dir)\n",
    "            \n",
    "        finally:\n",
    "            wandb.finish()\n",
    "    \n",
    "    def _log_metrics(self, results_dir):\n",
    "        \"\"\"Log all metrics to wandb.\"\"\"\n",
    "        # Classification metrics\n",
    "        metrics_dir = Path(results_dir) / \"metrics\"\n",
    "        \n",
    "        for classifier_dir in metrics_dir.glob(\"*\"):\n",
    "            if classifier_dir.is_dir() and classifier_dir.name != \"target_word_evaluation\":\n",
    "                metrics_file = classifier_dir / \"metrics.json\"\n",
    "                if metrics_file.exists():\n",
    "                    with open(metrics_file) as f:\n",
    "                        metrics = json.load(f)\n",
    "                    \n",
    "                    classifier_name = classifier_dir.name\n",
    "                    wandb.log({f\"{classifier_name}/{k}\": v for k, v in metrics[\"test_performance\"].items()})\n",
    "        \n",
    "        # Target word metrics\n",
    "        target_file = metrics_dir / \"target_word_evaluation\" / \"target_word_eval_summary.json\"\n",
    "        if target_file.exists():\n",
    "            with open(target_file) as f:\n",
    "                target_metrics = json.load(f)\n",
    "            \n",
    "            wandb.log({f\"target_word/{k}\": v for k, v in target_metrics[\"overall_metrics\"].items() if isinstance(v, (int, float))})\n",
    "    \n",
    "    def _log_visualizations(self, results_dir):\n",
    "        \"\"\"Log plots and visualizations.\"\"\"\n",
    "        metrics_dir = Path(results_dir) / \"metrics\"\n",
    "        \n",
    "        # Log confusion matrices and ROC curves\n",
    "        for classifier_dir in metrics_dir.glob(\"*\"):\n",
    "            if classifier_dir.is_dir() and classifier_dir.name != \"target_word_evaluation\":\n",
    "                \n",
    "                # Confusion matrix\n",
    "                cm_file = classifier_dir / \"confusion_matrix.png\"\n",
    "                if cm_file.exists():\n",
    "                    wandb.log({f\"{classifier_dir.name}/confusion_matrix\": wandb.Image(str(cm_file))})\n",
    "                \n",
    "                # ROC curve\n",
    "                roc_file = classifier_dir / \"roc_curve.png\"\n",
    "                if roc_file.exists():\n",
    "                    wandb.log({f\"{classifier_dir.name}/roc_curve\": wandb.Image(str(roc_file))})\n",
    "    \n",
    "    def _log_data_summary(self, results_dir):\n",
    "        \"\"\"Log data summary and predictions.\"\"\"\n",
    "        # Log predictions table\n",
    "        target_predictions = Path(results_dir) / \"metrics\" / \"target_word_evaluation\" / \"target_word_predictions.csv\"\n",
    "        \n",
    "        if target_predictions.exists():\n",
    "            df = pd.read_csv(target_predictions)\n",
    "            \n",
    "            # Create wandb table\n",
    "            table = wandb.Table(dataframe=df.head(100))  # Log first 100 rows\n",
    "            wandb.log({\"target_word_predictions\": table})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîê Security & Compliance\n",
    "\n",
    "### Data Security\n",
    "\n",
    "#### Encryption at Rest\n",
    "```bash\n",
    "# Encrypt sensitive data files\n",
    "gpg --symmetric --cipher-algo AES256 data/medical_claims.csv\n",
    "\n",
    "# Decrypt for processing\n",
    "gpg --decrypt data/medical_claims.csv.gpg > /tmp/claims.csv\n",
    "\n",
    "# Use temporary file in config\n",
    "python main.py run-all --config configs/secure_config.yaml\n",
    "\n",
    "# Clean up\n",
    "shred -vfz /tmp/claims.csv\n",
    "```\n",
    "\n",
    "#### Secure Configuration\n",
    "```yaml\n",
    "# secure_config.yaml\n",
    "model_api:\n",
    "  base_url: \"${MGPT_API_URL}\"          # From environment\n",
    "  headers:\n",
    "    Authorization: \"Bearer ${API_TOKEN}\" # Secure token\n",
    "\n",
    "input:\n",
    "  dataset_path: \"${ENCRYPTED_DATA_PATH}\" # Encrypted data\n",
    "\n",
    "output:\n",
    "  embeddings_dir: \"/secure/outputs/embeddings\"\n",
    "  models_dir: \"/secure/outputs/models\"\n",
    "\n",
    "logging:\n",
    "  level: \"INFO\"\n",
    "  file: \"/secure/logs/pipeline.log\"\n",
    "  # Disable sensitive data logging\n",
    "  mask_sensitive_data: true\n",
    "```\n",
    "\n",
    "### HIPAA Compliance\n",
    "\n",
    "#### Audit Logging\n",
    "```python\n",
    "# utils/audit_logger.py\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "class HIPAAAuditLogger:\n",
    "    def __init__(self, audit_file=\"/secure/audit/mgpt_eval_audit.log\"):\n",
    "        self.audit_file = audit_file\n",
    "        self.logger = logging.getLogger(\"hipaa_audit\")\n",
    "        \n",
    "        # Configure audit logger\n",
    "        handler = logging.FileHandler(audit_file)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        self.logger.addHandler(handler)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "    \n",
    "    def log_data_access(self, user_id, dataset_path, action=\"read\"):\n",
    "        \"\"\"Log data access for HIPAA compliance.\"\"\"\n",
    "        \n",
    "        # Hash dataset path for privacy\n",
    "        dataset_hash = hashlib.sha256(dataset_path.encode()).hexdigest()[:16]\n",
    "        \n",
    "        audit_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"event_type\": \"data_access\",\n",
    "            \"user_id\": user_id,\n",
    "            \"dataset_hash\": dataset_hash,\n",
    "            \"action\": action,\n",
    "            \"compliance\": \"HIPAA\"\n",
    "        }\n",
    "        \n",
    "        self.logger.info(json.dumps(audit_entry))\n",
    "    \n",
    "    def log_model_inference(self, user_id, sample_count, model_type):\n",
    "        \"\"\"Log model inference events.\"\"\"\n",
    "        \n",
    "        audit_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"event_type\": \"model_inference\",\n",
    "            \"user_id\": user_id,\n",
    "            \"sample_count\": sample_count,\n",
    "            \"model_type\": model_type,\n",
    "            \"compliance\": \"HIPAA\"\n",
    "        }\n",
    "        \n",
    "        self.logger.info(json.dumps(audit_entry))\n",
    "    \n",
    "    def log_data_export(self, user_id, export_path, record_count):\n",
    "        \"\"\"Log data export events.\"\"\"\n",
    "        \n",
    "        export_hash = hashlib.sha256(export_path.encode()).hexdigest()[:16]\n",
    "        \n",
    "        audit_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"event_type\": \"data_export\",\n",
    "            \"user_id\": user_id,\n",
    "            \"export_hash\": export_hash,\n",
    "            \"record_count\": record_count,\n",
    "            \"compliance\": \"HIPAA\"\n",
    "        }\n",
    "        \n",
    "        self.logger.info(json.dumps(audit_entry))\n",
    "\n",
    "# Usage in production\n",
    "audit = HIPAAAuditLogger()\n",
    "audit.log_data_access(\"user123\", \"data/patient_claims.csv\", \"read\")\n",
    "audit.log_model_inference(\"user123\", 1000, \"mgpt_embedding\")\n",
    "```\n",
    "\n",
    "#### Data Anonymization\n",
    "```python\n",
    "# utils/anonymization.py\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "class DataAnonymizer:\n",
    "    def __init__(self, salt=\"mgpt_eval_2024\"):\n",
    "        self.salt = salt\n",
    "    \n",
    "    def anonymize_dataset(self, input_path, output_path):\n",
    "        \"\"\"Anonymize medical claims dataset.\"\"\"\n",
    "        \n",
    "        df = pd.read_csv(input_path)\n",
    "        \n",
    "        # Anonymize MCIDs\n",
    "        df['mcid'] = df['mcid'].apply(self._hash_id)\n",
    "        \n",
    "        # Remove any potential PHI patterns\n",
    "        df['claims'] = df['claims'].apply(self._remove_phi_patterns)\n",
    "        \n",
    "        # Save anonymized dataset\n",
    "        df.to_csv(output_path, index=False)\n",
    "        \n",
    "        return output_path\n",
    "    \n",
    "    def _hash_id(self, original_id):\n",
    "        \"\"\"Create deterministic anonymous ID.\"\"\"\n",
    "        hash_input = f\"{original_id}{self.salt}\"\n",
    "        return hashlib.sha256(hash_input.encode()).hexdigest()[:12]\n",
    "    \n",
    "    def _remove_phi_patterns(self, text):\n",
    "        \"\"\"Remove potential PHI patterns from text.\"\"\"\n",
    "        \n",
    "        # Remove social security numbers\n",
    "        text = re.sub(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', '[SSN]', text)\n",
    "        \n",
    "        # Remove phone numbers\n",
    "        text = re.sub(r'\\b\\d{3}-\\d{3}-\\d{4}\\b', '[PHONE]', text)\n",
    "        \n",
    "        # Remove dates (be careful not to remove medical codes)\n",
    "        text = re.sub(r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b', '[DATE]', text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "# Usage\n",
    "anonymizer = DataAnonymizer()\n",
    "anonymized_path = anonymizer.anonymize_dataset(\n",
    "    \"data/raw_claims.csv\",\n",
    "    \"data/anonymized_claims.csv\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Best Practices Summary\n",
    "\n",
    "### Production Deployment Checklist\n",
    "\n",
    "#### Infrastructure\n",
    "- [ ] **Containerization**: Docker images for consistent deployment\n",
    "- [ ] **Orchestration**: Kubernetes/Docker Compose for scaling\n",
    "- [ ] **Monitoring**: Resource usage and performance metrics\n",
    "- [ ] **Logging**: Structured logging with appropriate levels\n",
    "- [ ] **Backup**: Regular backup of results and configurations\n",
    "\n",
    "#### Security\n",
    "- [ ] **Data encryption**: At rest and in transit\n",
    "- [ ] **Access control**: Role-based access to data and systems\n",
    "- [ ] **Audit logging**: HIPAA-compliant audit trails\n",
    "- [ ] **Data anonymization**: Remove PHI before processing\n",
    "- [ ] **Secure secrets**: Environment variables for sensitive data\n",
    "\n",
    "#### Performance\n",
    "- [ ] **Resource optimization**: Memory and CPU usage tuning\n",
    "- [ ] **Batch processing**: Optimal batch sizes for throughput\n",
    "- [ ] **Parallel processing**: Multi-threading where applicable\n",
    "- [ ] **Caching**: Cache embeddings and models when possible\n",
    "- [ ] **Checkpointing**: Resume capability for long-running jobs\n",
    "\n",
    "#### Quality Assurance\n",
    "- [ ] **Data validation**: Input data quality checks\n",
    "- [ ] **Model validation**: Performance threshold monitoring\n",
    "- [ ] **Result validation**: Output quality verification\n",
    "- [ ] **Regression testing**: Automated testing pipeline\n",
    "- [ ] **Documentation**: Comprehensive operational documentation\n",
    "\n",
    "### Operational Excellence\n",
    "\n",
    "#### Monitoring & Alerting\n",
    "```yaml\n",
    "# monitoring_config.yaml\n",
    "monitoring:\n",
    "  metrics:\n",
    "    - accuracy_threshold: 0.80        # Alert if below 80%\n",
    "    - processing_time_max: 3600       # Alert if exceeds 1 hour\n",
    "    - memory_usage_max: 85            # Alert if memory > 85%\n",
    "    - error_rate_max: 0.05            # Alert if error rate > 5%\n",
    "  \n",
    "  notifications:\n",
    "    email: [\"ops@company.com\"]\n",
    "    slack: \"#mgpt-alerts\"\n",
    "    pagerduty: \"mgpt-service\"\n",
    "```\n",
    "\n",
    "#### Automated Recovery\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# scripts/auto_recovery.sh\n",
    "\n",
    "# Monitor evaluation jobs and restart if needed\n",
    "while true; do\n",
    "    # Check if evaluation is running\n",
    "    if ! pgrep -f \"main.py run-all\" > /dev/null; then\n",
    "        echo \"No evaluation running, checking for pending jobs...\"\n",
    "        \n",
    "        # Check for pending datasets\n",
    "        if ls /data/pending/*.csv 1> /dev/null 2>&1; then\n",
    "            echo \"Found pending datasets, starting evaluation...\"\n",
    "            python scripts/process_pending.py\n",
    "        fi\n",
    "    fi\n",
    "    \n",
    "    sleep 300  # Check every 5 minutes\n",
    "done\n",
    "```\n",
    "\n",
    "### Future Enhancements\n",
    "\n",
    "1. **Real-time Processing**: Stream processing for live data\n",
    "2. **Auto-scaling**: Dynamic resource allocation based on load\n",
    "3. **Multi-model Support**: Compare multiple MGPT model versions\n",
    "4. **Active Learning**: Identify samples that need manual review\n",
    "5. **Federated Learning**: Distributed training across institutions\n",
    "6. **Explainable AI**: Better interpretation of model decisions\n",
    "7. **AutoML**: Automated hyperparameter optimization\n",
    "8. **Edge Deployment**: On-premise deployment for sensitive data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Summary\n",
    "\n",
    "This advanced usage guide covered:\n",
    "\n",
    "### ‚úÖ Production Deployment\n",
    "- Docker and Kubernetes deployment strategies\n",
    "- Environment configuration and secrets management\n",
    "- Health checks and monitoring setup\n",
    "\n",
    "### ‚ö° Performance Optimization\n",
    "- Large-scale data processing techniques\n",
    "- Parallel and distributed processing\n",
    "- Resource monitoring and adaptive optimization\n",
    "\n",
    "### üîÑ Automated Workflows\n",
    "- Scheduled evaluation pipelines\n",
    "- CI/CD integration with GitHub Actions and Jenkins\n",
    "- Email and Slack notifications\n",
    "\n",
    "### üìä Analytics Integration\n",
    "- MLflow experiment tracking\n",
    "- Weights & Biases logging\n",
    "- Custom metrics dashboards\n",
    "\n",
    "### üîê Security & Compliance\n",
    "- HIPAA-compliant audit logging\n",
    "- Data encryption and anonymization\n",
    "- Secure configuration management\n",
    "\n",
    "### Quick Start for Production:\n",
    "\n",
    "```bash\n",
    "# 1. Create production configuration\n",
    "cp configs/templates/04_full_pipeline.yaml production_config.yaml\n",
    "\n",
    "# 2. Set environment variables\n",
    "export MGPT_API_URL=\"https://your-mgpt-api.com\"\n",
    "export NOTIFICATION_EMAILS=\"ops@company.com\"\n",
    "\n",
    "# 3. Deploy with Docker\n",
    "docker-compose up --build\n",
    "\n",
    "# 4. Monitor logs\n",
    "docker-compose logs -f mgpt-eval\n",
    "```\n",
    "\n",
    "You now have everything needed to deploy MGPT-Eval in a production environment with enterprise-grade reliability, security, and monitoring!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}