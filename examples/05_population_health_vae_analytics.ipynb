{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population Health Analytics with Pythae VAE\n",
    "\n",
    "This notebook demonstrates advanced population health analytics using Variational Autoencoders (VAE) from the Pythae library. We'll analyze member embeddings to identify risk patterns, health phenotypes, and care opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Pythae imports\n",
    "from pythae.models import VAE, BetaVAE, VAE_LinNF, RHVAE\n",
    "from pythae.models.base.base_utils import ModelOutput\n",
    "from pythae.trainers import BaseTrainer, BaseTrainerConfig\n",
    "from pythae.data.preprocessors import DataProcessor\n",
    "\n",
    "# Local imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from pipelines.embedding_pipeline import EmbeddingPipeline\n",
    "from models.config_models import PipelineConfig\n",
    "from utils.logging_utils import get_logger\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Healthcare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load medical claims data\n",
    "data_path = Path('data/medical_claims_complete.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Loaded {len(df)} member records\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution:\\n{df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Member Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure embedding pipeline\n",
    "config = {\n",
    "    'pipeline': {\n",
    "        'job_name': 'population_health_vae',\n",
    "        'log_level': 'INFO'\n",
    "    },\n",
    "    'data': {\n",
    "        'data_path': str(data_path.absolute()),\n",
    "        'claim_column': 'claim',\n",
    "        'label_column': 'label',\n",
    "        'mcid_column': 'mcid'\n",
    "    },\n",
    "    'llm': {\n",
    "        'model_url': 'http://localhost:8000',\n",
    "        'batch_size': 32,\n",
    "        'max_retries': 3\n",
    "    },\n",
    "    'outputs': {\n",
    "        'output_dir': 'outputs/population_health_vae',\n",
    "        'save_embeddings': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run embedding pipeline\n",
    "pipeline_config = PipelineConfig(**config)\n",
    "embedding_pipeline = EmbeddingPipeline(pipeline_config)\n",
    "embeddings_df = embedding_pipeline.run()\n",
    "\n",
    "print(f\"Generated embeddings shape: {embeddings_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for VAE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embedding features\n",
    "embedding_cols = [col for col in embeddings_df.columns if col.startswith('embedding_')]\n",
    "X = embeddings_df[embedding_cols].values\n",
    "y = embeddings_df['label'].values\n",
    "member_ids = embeddings_df['mcid'].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.FloatTensor(X_scaled)\n",
    "\n",
    "print(f\"Data shape: {X_tensor.shape}\")\n",
    "print(f\"Number of members: {len(np.unique(member_ids))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Population Health VAE Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE architectures for population health\n",
    "input_dim = X_tensor.shape[1]\n",
    "latent_dim = 32  # Latent space for health phenotypes\n",
    "\n",
    "# Model 1: Standard VAE for baseline\n",
    "vae_config = VAE.default_config()\n",
    "vae_config.input_dim = input_dim\n",
    "vae_config.latent_dim = latent_dim\n",
    "vae_model = VAE(vae_config)\n",
    "\n",
    "# Model 2: Beta-VAE for disentangled health factors\n",
    "beta_vae_config = BetaVAE.default_config()\n",
    "beta_vae_config.input_dim = input_dim\n",
    "beta_vae_config.latent_dim = latent_dim\n",
    "beta_vae_config.beta = 4.0  # Encourage disentanglement\n",
    "beta_vae_model = BetaVAE(beta_vae_config)\n",
    "\n",
    "# Model 3: RHVAE for complex health patterns\n",
    "rhvae_config = RHVAE.default_config()\n",
    "rhvae_config.input_dim = input_dim\n",
    "rhvae_config.latent_dim = latent_dim\n",
    "rhvae_model = RHVAE(rhvae_config)\n",
    "\n",
    "models = {\n",
    "    'VAE': vae_model,\n",
    "    'Beta-VAE': beta_vae_model,\n",
    "    'RHVAE': rhvae_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "trainer_config = BaseTrainerConfig(\n",
    "    num_epochs=50,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    steps_saving=10\n",
    ")\n",
    "\n",
    "trained_models = {}\n",
    "training_losses = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    trainer = BaseTrainer(\n",
    "        model=model,\n",
    "        train_dataset=X_tensor,\n",
    "        training_config=trainer_config\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trained_models[model_name] = trainer.model\n",
    "    training_losses[model_name] = trainer.training_logs['train_loss']\n",
    "    \n",
    "    print(f\"{model_name} training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Member Risk Stratification Using Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction errors for risk assessment\n",
    "def calculate_reconstruction_error(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model_output = model(data)\n",
    "        recon_x = model_output.recon_x\n",
    "        errors = torch.mean((data - recon_x) ** 2, dim=1)\n",
    "    return errors.numpy()\n",
    "\n",
    "# Calculate errors for each model\n",
    "reconstruction_errors = {}\n",
    "for model_name, model in trained_models.items():\n",
    "    errors = calculate_reconstruction_error(model, X_tensor)\n",
    "    reconstruction_errors[model_name] = errors\n",
    "\n",
    "# Create risk stratification dataframe\n",
    "risk_df = pd.DataFrame({\n",
    "    'mcid': member_ids,\n",
    "    'label': y,\n",
    "    **{f'{model}_error': errors for model, errors in reconstruction_errors.items()}\n",
    "})\n",
    "\n",
    "# Calculate risk scores (higher error = higher risk)\n",
    "for model_name in reconstruction_errors.keys():\n",
    "    risk_df[f'{model_name}_risk_percentile'] = risk_df[f'{model_name}_error'].rank(pct=True) * 100\n",
    "\n",
    "print(\"Risk Stratification Summary:\")\n",
    "print(risk_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, (model_name, errors) in enumerate(reconstruction_errors.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot by label\n",
    "    for label in np.unique(y):\n",
    "        mask = y == label\n",
    "        ax.hist(errors[mask], bins=30, alpha=0.6, label=f'Label {label}', density=True)\n",
    "    \n",
    "    ax.set_title(f'{model_name} Risk Distribution')\n",
    "    ax.set_xlabel('Reconstruction Error')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify high-risk members\n",
    "high_risk_threshold = 95  # Top 5% risk\n",
    "high_risk_members = risk_df[risk_df['VAE_risk_percentile'] >= high_risk_threshold]\n",
    "print(f\"\\nIdentified {len(high_risk_members)} high-risk members (top 5%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Health Phenotype Discovery Through Latent Space Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latent representations\n",
    "def get_latent_representations(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.encoder(data)\n",
    "        if hasattr(encoder_output, 'embedding'):\n",
    "            z = encoder_output.embedding\n",
    "        else:\n",
    "            z = encoder_output.z\n",
    "    return z.numpy()\n",
    "\n",
    "# Get latent representations from Beta-VAE (best for disentanglement)\n",
    "latent_representations = get_latent_representations(trained_models['Beta-VAE'], X_tensor)\n",
    "\n",
    "# Determine optimal number of clusters\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(latent_representations)\n",
    "    score = silhouette_score(latent_representations, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "optimal_k = K_range[np.argmax(silhouette_scores)]\n",
    "print(f\"Optimal number of health phenotypes: {optimal_k}\")\n",
    "\n",
    "# Perform clustering with optimal k\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "phenotype_clusters = kmeans.fit_predict(latent_representations)\n",
    "\n",
    "# Add to risk dataframe\n",
    "risk_df['phenotype'] = phenotype_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize phenotypes in 2D\n",
    "pca = PCA(n_components=2)\n",
    "latent_2d = pca.fit_transform(latent_representations)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], \n",
    "                     c=phenotype_clusters, cmap='tab10', \n",
    "                     alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "plt.colorbar(scatter, label='Health Phenotype')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('Member Health Phenotypes in Latent Space')\n",
    "\n",
    "# Add cluster centers\n",
    "centers_2d = pca.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centers_2d[:, 0], centers_2d[:, 1], \n",
    "           marker='*', s=300, c='red', edgecolors='black', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Phenotype characteristics\n",
    "print(\"\\nPhenotype Distribution:\")\n",
    "phenotype_stats = risk_df.groupby('phenotype').agg({\n",
    "    'mcid': 'count',\n",
    "    'label': lambda x: (x == 1).mean(),\n",
    "    'VAE_risk_percentile': 'mean'\n",
    "}).round(2)\n",
    "phenotype_stats.columns = ['Member Count', 'Positive Rate', 'Avg Risk Score']\n",
    "print(phenotype_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Member Journey Analysis Through Latent Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze transitions between health states\n",
    "def interpolate_latent_path(z_start, z_end, steps=10):\n",
    "    \"\"\"Interpolate between two latent representations\"\"\"\n",
    "    alphas = np.linspace(0, 1, steps)\n",
    "    path = []\n",
    "    for alpha in alphas:\n",
    "        z_interp = (1 - alpha) * z_start + alpha * z_end\n",
    "        path.append(z_interp)\n",
    "    return np.array(path)\n",
    "\n",
    "# Find example members from different phenotypes\n",
    "phenotype_examples = {}\n",
    "for phenotype in range(optimal_k):\n",
    "    members = risk_df[risk_df['phenotype'] == phenotype]\n",
    "    # Get a low-risk member from this phenotype\n",
    "    example_idx = members['VAE_risk_percentile'].idxmin()\n",
    "    phenotype_examples[phenotype] = example_idx\n",
    "\n",
    "# Create transition paths between phenotypes\n",
    "transition_paths = {}\n",
    "for p1 in range(optimal_k):\n",
    "    for p2 in range(p1 + 1, optimal_k):\n",
    "        idx1 = phenotype_examples[p1]\n",
    "        idx2 = phenotype_examples[p2]\n",
    "        \n",
    "        z1 = latent_representations[idx1]\n",
    "        z2 = latent_representations[idx2]\n",
    "        \n",
    "        path = interpolate_latent_path(z1, z2)\n",
    "        transition_paths[f'P{p1}_to_P{p2}'] = path\n",
    "\n",
    "# Visualize a sample transition\n",
    "sample_transition = list(transition_paths.keys())[0]\n",
    "path = transition_paths[sample_transition]\n",
    "path_2d = pca.transform(path)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plot all members\n",
    "plt.scatter(latent_2d[:, 0], latent_2d[:, 1], \n",
    "           c=phenotype_clusters, cmap='tab10', alpha=0.3)\n",
    "# Plot transition path\n",
    "plt.plot(path_2d[:, 0], path_2d[:, 1], 'r-', linewidth=2, label=sample_transition)\n",
    "plt.scatter(path_2d[0, 0], path_2d[0, 1], marker='o', s=200, c='green', label='Start')\n",
    "plt.scatter(path_2d[-1, 0], path_2d[-1, 1], marker='s', s=200, c='red', label='End')\n",
    "\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('Health State Transition Path Analysis')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Population Health Insights Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive population health dashboard\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 1. Risk Distribution by Phenotype\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "risk_by_phenotype = risk_df.groupby('phenotype')['VAE_risk_percentile'].apply(list)\n",
    "ax1.boxplot(risk_by_phenotype.values, labels=risk_by_phenotype.index)\n",
    "ax1.set_xlabel('Health Phenotype')\n",
    "ax1.set_ylabel('Risk Percentile')\n",
    "ax1.set_title('Risk Distribution by Phenotype')\n",
    "\n",
    "# 2. Phenotype Size and Composition\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "phenotype_sizes = risk_df.groupby(['phenotype', 'label']).size().unstack(fill_value=0)\n",
    "phenotype_sizes.plot(kind='bar', stacked=True, ax=ax2)\n",
    "ax2.set_xlabel('Health Phenotype')\n",
    "ax2.set_ylabel('Member Count')\n",
    "ax2.set_title('Phenotype Composition')\n",
    "ax2.legend(['Negative', 'Positive'])\n",
    "\n",
    "# 3. High-Risk Member Distribution\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "high_risk_dist = risk_df[risk_df['VAE_risk_percentile'] >= 90].groupby('phenotype').size()\n",
    "ax3.pie(high_risk_dist.values, labels=high_risk_dist.index, autopct='%1.1f%%')\n",
    "ax3.set_title('High-Risk Members by Phenotype')\n",
    "\n",
    "# 4. Latent Space Visualization\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "scatter = ax4.scatter(latent_2d[:, 0], latent_2d[:, 1], \n",
    "                     c=risk_df['VAE_risk_percentile'], cmap='RdYlBu_r',\n",
    "                     alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "plt.colorbar(scatter, ax=ax4, label='Risk Percentile')\n",
    "ax4.set_xlabel('First Principal Component')\n",
    "ax4.set_ylabel('Second Principal Component')\n",
    "ax4.set_title('Risk Landscape in Latent Space')\n",
    "\n",
    "# 5. Model Comparison\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "model_performance = pd.DataFrame({\n",
    "    model: risk_df[f'{model}_error'].describe()\n",
    "    for model in reconstruction_errors.keys()\n",
    "})\n",
    "model_performance.loc[['mean', 'std', 'min', 'max']].plot(kind='bar', ax=ax5)\n",
    "ax5.set_ylabel('Reconstruction Error')\n",
    "ax5.set_title('Model Performance Comparison')\n",
    "ax5.legend(loc='upper right')\n",
    "\n",
    "# 6. Risk Score Correlations\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "risk_cols = [col for col in risk_df.columns if 'risk_percentile' in col]\n",
    "correlation_matrix = risk_df[risk_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=ax6)\n",
    "ax6.set_title('Risk Score Correlations Across Models')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/population_health_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Actionable Insights for Care Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate care management recommendations\n",
    "def generate_care_recommendations(risk_df, top_n=10):\n",
    "    recommendations = []\n",
    "    \n",
    "    # 1. Highest risk members needing immediate intervention\n",
    "    immediate_intervention = risk_df.nlargest(top_n, 'VAE_risk_percentile')[['mcid', 'phenotype', 'VAE_risk_percentile']]\n",
    "    \n",
    "    # 2. Members showing anomalous patterns (high reconstruction error)\n",
    "    anomaly_threshold = risk_df['VAE_error'].quantile(0.95)\n",
    "    anomalous_members = risk_df[risk_df['VAE_error'] > anomaly_threshold]\n",
    "    \n",
    "    # 3. Phenotype-specific interventions\n",
    "    phenotype_risks = risk_df.groupby('phenotype')['VAE_risk_percentile'].mean().sort_values(ascending=False)\n",
    "    high_risk_phenotypes = phenotype_risks[phenotype_risks > 70].index.tolist()\n",
    "    \n",
    "    return {\n",
    "        'immediate_intervention': immediate_intervention,\n",
    "        'anomalous_patterns': anomalous_members[['mcid', 'phenotype', 'VAE_error']].head(top_n),\n",
    "        'high_risk_phenotypes': high_risk_phenotypes,\n",
    "        'phenotype_risk_scores': phenotype_risks\n",
    "    }\n",
    "\n",
    "recommendations = generate_care_recommendations(risk_df)\n",
    "\n",
    "print(\"=== CARE MANAGEMENT RECOMMENDATIONS ===\")\n",
    "print(\"\\n1. Members Requiring Immediate Intervention:\")\n",
    "print(recommendations['immediate_intervention'])\n",
    "\n",
    "print(\"\\n2. Members with Anomalous Health Patterns:\")\n",
    "print(recommendations['anomalous_patterns'])\n",
    "\n",
    "print(\"\\n3. High-Risk Phenotypes for Targeted Programs:\")\n",
    "print(f\"Phenotypes: {recommendations['high_risk_phenotypes']}\")\n",
    "print(\"\\nPhenotype Risk Scores:\")\n",
    "print(recommendations['phenotype_risk_scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results for Clinical Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare comprehensive member risk profile\n",
    "member_risk_profile = risk_df[[\n",
    "    'mcid', 'label', 'phenotype',\n",
    "    'VAE_risk_percentile', 'Beta-VAE_risk_percentile', 'RHVAE_risk_percentile'\n",
    "]].copy()\n",
    "\n",
    "# Add risk categories\n",
    "member_risk_profile['risk_category'] = pd.cut(\n",
    "    member_risk_profile['VAE_risk_percentile'],\n",
    "    bins=[0, 50, 80, 95, 100],\n",
    "    labels=['Low', 'Moderate', 'High', 'Critical']\n",
    ")\n",
    "\n",
    "# Add phenotype descriptions\n",
    "phenotype_descriptions = {\n",
    "    i: f\"Phenotype_{i}_{'HighRisk' if i in recommendations['high_risk_phenotypes'] else 'Standard'}\"\n",
    "    for i in range(optimal_k)\n",
    "}\n",
    "member_risk_profile['phenotype_description'] = member_risk_profile['phenotype'].map(phenotype_descriptions)\n",
    "\n",
    "# Save results\n",
    "output_dir = Path('outputs/population_health_vae')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save member risk profiles\n",
    "member_risk_profile.to_csv(output_dir / 'member_risk_profiles.csv', index=False)\n",
    "\n",
    "# Save model artifacts\n",
    "for model_name, model in trained_models.items():\n",
    "    torch.save(model.state_dict(), output_dir / f'{model_name.lower()}_weights.pt')\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    'total_members': len(risk_df),\n",
    "    'phenotypes_identified': optimal_k,\n",
    "    'high_risk_members': len(risk_df[risk_df['risk_category'] == 'Critical']),\n",
    "    'anomalous_members': len(risk_df[risk_df['VAE_error'] > risk_df['VAE_error'].quantile(0.95)]),\n",
    "    'model_performance': {\n",
    "        model: float(risk_df[f'{model}_error'].mean())\n",
    "        for model in reconstruction_errors.keys()\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(output_dir / 'population_health_summary.json', 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(\"\\n=== EXPORT COMPLETE ===\")\n",
    "print(f\"Results saved to: {output_dir}\")\n",
    "print(f\"\\nSummary:\")\n",
    "for key, value in summary_stats.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"{key}:\")\n",
    "        for k, v in value.items():\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated advanced population health analytics using Pythae VAE models:\n",
    "\n",
    "1. **Risk Stratification**: Identified high-risk members using reconstruction errors\n",
    "2. **Phenotype Discovery**: Found distinct health phenotypes through latent space clustering\n",
    "3. **Journey Analysis**: Analyzed potential health state transitions\n",
    "4. **Actionable Insights**: Generated specific recommendations for care management\n",
    "\n",
    "The VAE approach provides a powerful framework for understanding complex health patterns and enabling proactive, personalized care management at the population level."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}