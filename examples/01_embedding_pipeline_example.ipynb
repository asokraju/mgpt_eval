{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Pipeline Example\n",
    "\n",
    "This notebook demonstrates how to use the **Embedding Pipeline** to generate embeddings from medical claims using the MediClaimGPT model.\n",
    "\n",
    "## Overview\n",
    "The embedding pipeline converts text claims into dense vector representations that capture semantic meaning. These embeddings can then be used for downstream tasks like classification.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to configure the embedding pipeline using YAML\n",
    "2. How to process medical claims data\n",
    "3. How to generate embeddings using the MediClaimGPT API\n",
    "4. How to save and inspect the generated embeddings\n",
    "\n",
    "## Prerequisites\n",
    "- MediClaimGPT model server running on `http://localhost:8000`\n",
    "- Input CSV file with columns: `mcid`, `claims`, `label`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport yaml\nimport json\n\n# Add the project root to Python path\nproject_root = Path().absolute().parent\nsys.path.insert(0, str(project_root))\n\nfrom models.config_models import PipelineConfig\nfrom pipelines.embedding_pipeline import EmbeddingPipeline\n\nprint(\"âœ… Imports successful\")\nprint(f\"ğŸ“ Project root: {project_root}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Inspect Sample Data\n",
    "\n",
    "Let's first look at our sample dataset to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Dataset shape: (20, 3)\n",
      "ğŸ“‹ Columns: ['mcid', 'claims', 'label']\n",
      "ğŸ·ï¸  Label distribution:\n",
      "label\n",
      "1    10\n",
      "0    10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“ Sample claims:\n",
      "\n",
      "1. [Evidence-based] Patients with diabetes should monitor their blood glucose levels regularly to maintain optimal glyce...\n",
      "\n",
      "2. [Evidence-based] Regular aerobic exercise for 30 minutes daily significantly reduces cardiovascular disease risk...\n",
      "\n",
      "3. [Evidence-based] Smoking cessation dramatically decreases lung cancer incidence within 5-10 years...\n"
     ]
    }
   ],
   "source": [
    "# Load sample data\n",
    "data_file = \"data/medical_claims_sample.csv\"\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "print(f\"ğŸ“Š Dataset shape: {df.shape}\")\n",
    "print(f\"ğŸ“‹ Columns: {list(df.columns)}\")\n",
    "print(f\"ğŸ·ï¸  Label distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "print(\"\\nğŸ“ Sample claims:\")\n",
    "for i, row in df.head(3).iterrows():\n",
    "    label_text = \"Evidence-based\" if row['label'] == 1 else \"Pseudoscientific\"\n",
    "    print(f\"\\n{i+1}. [{label_text}] {row['claims'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Configuration\n",
    "\n",
    "We'll use a YAML configuration file to set up the embedding pipeline. This approach makes it easy to modify settings without changing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Configuration loaded successfully\n",
      "ğŸ“ Job name: embedding_generation_example\n",
      "ğŸŒ API endpoint: http://localhost:8000\n",
      "ğŸ“¦ Batch size: 8\n",
      "ğŸ§  Tokenizer: /home/kosaraju/mgpt-serve/tokenizer\n",
      "\n",
      "âš™ï¸  Pipeline stages enabled:\n",
      "  âœ… embeddings\n",
      "  âŒ classification\n",
      "  âŒ evaluation\n",
      "  âŒ target_word_eval\n",
      "  âŒ summary_report\n",
      "  âŒ method_comparison\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from YAML file\n",
    "config_file = \"configs/embedding_example_config.yaml\"\n",
    "\n",
    "with open(config_file, 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "print(\"ğŸ”§ Configuration loaded successfully\")\n",
    "print(f\"ğŸ“ Job name: {config_data['job']['name']}\")\n",
    "print(f\"ğŸŒ API endpoint: {config_data['model_api']['base_url']}\")\n",
    "print(f\"ğŸ“¦ Batch size: {config_data['embedding_generation']['batch_size']}\")\n",
    "print(f\"ğŸ§  Tokenizer: {config_data['embedding_generation']['tokenizer_path']}\")\n",
    "\n",
    "# Display key configuration sections\n",
    "print(\"\\nâš™ï¸  Pipeline stages enabled:\")\n",
    "for stage, enabled in config_data['pipeline_stages'].items():\n",
    "    status = \"âœ…\" if enabled else \"âŒ\"\n",
    "    print(f\"  {status} {stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test API Connection\n",
    "\n",
    "Before running the full pipeline, let's verify that the MediClaimGPT API is accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API connection successful\n",
      "ğŸ“ Embedding dimension: 768\n",
      "ğŸ”¢ Sample embedding (first 5 values): [-0.5630092024803162, 0.10324104130268097, -0.797333836555481, 0.17846648395061493, 0.05581314116716385]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Test API connection\n",
    "api_url = config_data['model_api']['base_url']\n",
    "test_endpoint = api_url + config_data['model_api']['endpoints']['embeddings']\n",
    "\n",
    "try:\n",
    "    # Test with a simple claim\n",
    "    test_payload = {\n",
    "        \"claims\": [\"Regular exercise improves cardiovascular health\"]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(test_endpoint, json=test_payload, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    result = response.json()\n",
    "    embedding_dim = len(result['embeddings'][0])\n",
    "    \n",
    "    print(\"âœ… API connection successful\")\n",
    "    print(f\"ğŸ“ Embedding dimension: {embedding_dim}\")\n",
    "    print(f\"ğŸ”¢ Sample embedding (first 5 values): {result['embeddings'][0][:5]}\")\n",
    "    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"âŒ API connection failed: {e}\")\n",
    "    print(\"Please ensure the MediClaimGPT server is running on http://localhost:8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize and Run Embedding Pipeline\n",
    "\n",
    "Now we'll create the pipeline configuration object and run the embedding generation process."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create directories for outputs\nos.makedirs(\"outputs/embeddings\", exist_ok=True)\nos.makedirs(\"outputs/logs\", exist_ok=True)\n\n# Initialize the pipeline with configuration\ntry:\n    # Create PipelineConfig object from YAML data\n    config = PipelineConfig(**config_data)\n    print(\"âœ… Configuration validated successfully\")\n    \n    # Initialize embedding pipeline\n    embedding_pipeline = EmbeddingPipeline(config)\n    print(\"âœ… Embedding pipeline initialized\")\n    \nexcept Exception as e:\n    print(f\"âŒ Pipeline initialization failed: {e}\")\n    import traceback\n    traceback.print_exc()\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the embedding generation\n",
    "dataset_path = data_file\n",
    "output_path = \"outputs/embeddings/sample_embeddings.csv\"\n",
    "\n",
    "print(\"ğŸš€ Starting embedding generation...\")\n",
    "print(f\"ğŸ“Š Input: {dataset_path}\")\n",
    "print(f\"ğŸ’¾ Output: {output_path}\")\n",
    "\n",
    "try:\n",
    "    # Run the pipeline\n",
    "    results = embedding_pipeline.run(\n",
    "        dataset_path=dataset_path,\n",
    "        output_path=output_path\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… Embedding generation completed successfully!\")\n",
    "    print(f\"ğŸ“ˆ Processed samples: {results['n_samples']}\")\n",
    "    print(f\"ğŸ“ Embedding dimension: {results['embedding_dim']}\")\n",
    "    print(f\"ğŸ’¾ Output file: {results['output_path']}\")\n",
    "    print(f\"â±ï¸  Timestamp: {results['timestamp']}\")\n",
    "    \n",
    "    # Display embedding statistics if available\n",
    "    if 'embedding_stats' in results and results['embedding_stats']:\n",
    "        stats = results['embedding_stats']\n",
    "        print(\"\\nğŸ“Š Embedding Statistics:\")\n",
    "        print(f\"  ğŸ“Š Mean norm: {stats.get('mean_norm', 'N/A'):.3f}\")\n",
    "        print(f\"  ğŸ“Š Std norm: {stats.get('std_norm', 'N/A'):.3f}\")\n",
    "        print(f\"  ğŸ“Š Min norm: {stats.get('min_norm', 'N/A'):.3f}\")\n",
    "        print(f\"  ğŸ“Š Max norm: {stats.get('max_norm', 'N/A'):.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Embedding generation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Inspect Generated Embeddings\n",
    "\n",
    "Let's examine the generated embeddings to understand their structure and properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the generated embeddings\n",
    "embeddings_df = pd.read_csv(output_path)\n",
    "\n",
    "print(f\"ğŸ“Š Embeddings dataset shape: {embeddings_df.shape}\")\n",
    "print(f\"ğŸ“‹ Columns: {list(embeddings_df.columns)}\")\n",
    "\n",
    "# Parse the first embedding to check dimension\n",
    "first_embedding = json.loads(embeddings_df.iloc[0]['embedding'])\n",
    "print(f\"ğŸ“ Embedding dimension: {len(first_embedding)}\")\n",
    "print(f\"ğŸ”¢ First 10 values: {first_embedding[:10]}\")\n",
    "\n",
    "# Display sample rows\n",
    "print(\"\\nğŸ“ Sample embeddings data:\")\n",
    "display_df = embeddings_df[['mcid', 'label']].copy()\n",
    "display_df['embedding_preview'] = embeddings_df['embedding'].apply(\n",
    "    lambda x: str(json.loads(x)[:5]) + \"...\"\n",
    ")\n",
    "print(display_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Embedding Analysis\n",
    "\n",
    "Let's perform some basic analysis on the generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Parse all embeddings into a matrix\n",
    "embeddings_matrix = np.array([\n",
    "    json.loads(emb) for emb in embeddings_df['embedding']\n",
    "])\n",
    "labels = embeddings_df['label'].values\n",
    "\n",
    "print(f\"ğŸ“Š Embeddings matrix shape: {embeddings_matrix.shape}\")\n",
    "\n",
    "# Calculate basic statistics\n",
    "norms = np.linalg.norm(embeddings_matrix, axis=1)\n",
    "print(f\"\\nğŸ“Š Embedding Norms Statistics:\")\n",
    "print(f\"  Mean: {np.mean(norms):.3f}\")\n",
    "print(f\"  Std:  {np.std(norms):.3f}\")\n",
    "print(f\"  Min:  {np.min(norms):.3f}\")\n",
    "print(f\"  Max:  {np.max(norms):.3f}\")\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. Norm distribution\n",
    "axes[0].hist(norms, bins=10, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Embedding Norm Distribution')\n",
    "axes[0].set_xlabel('L2 Norm')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# 2. PCA visualization\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_pca = pca.fit_transform(embeddings_matrix)\n",
    "\n",
    "colors = ['red' if label == 0 else 'blue' for label in labels]\n",
    "axes[1].scatter(embeddings_pca[:, 0], embeddings_pca[:, 1], c=colors, alpha=0.7)\n",
    "axes[1].set_title('PCA Visualization')\n",
    "axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[1].legend(['Pseudoscientific', 'Evidence-based'])\n",
    "\n",
    "# 3. Embedding magnitude by class\n",
    "class_0_norms = norms[labels == 0]\n",
    "class_1_norms = norms[labels == 1]\n",
    "\n",
    "axes[2].boxplot([class_0_norms, class_1_norms], labels=['Pseudoscientific', 'Evidence-based'])\n",
    "axes[2].set_title('Embedding Norms by Class')\n",
    "axes[2].set_ylabel('L2 Norm')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/embedding_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¾ Analysis plots saved to 'outputs/embedding_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Embeddings for Classification\n",
    "\n",
    "The generated embeddings are now ready to be used in the classification pipeline. Let's create train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create stratified train/test split\n",
    "train_df, test_df = train_test_split(\n",
    "    embeddings_df, \n",
    "    test_size=0.2, \n",
    "    stratify=embeddings_df['label'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Save splits\n",
    "train_file = \"outputs/embeddings/train_embeddings.csv\"\n",
    "test_file = \"outputs/embeddings/test_embeddings.csv\"\n",
    "\n",
    "train_df.to_csv(train_file, index=False)\n",
    "test_df.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"ğŸ“Š Train set: {len(train_df)} samples\")\n",
    "print(f\"  Class 0: {sum(train_df['label'] == 0)}\")\n",
    "print(f\"  Class 1: {sum(train_df['label'] == 1)}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Test set: {len(test_df)} samples\")\n",
    "print(f\"  Class 0: {sum(test_df['label'] == 0)}\")\n",
    "print(f\"  Class 1: {sum(test_df['label'] == 1)}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Files saved:\")\n",
    "print(f\"  Train: {train_file}\")\n",
    "print(f\"  Test: {test_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "ğŸ‰ **Congratulations!** You have successfully:\n",
    "\n",
    "1. âœ… **Configured** the embedding pipeline using YAML\n",
    "2. âœ… **Loaded** medical claims data\n",
    "3. âœ… **Generated** embeddings using MediClaimGPT API\n",
    "4. âœ… **Analyzed** embedding properties and distributions\n",
    "5. âœ… **Created** train/test splits for classification\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "The generated embeddings are now ready for use in:\n",
    "- **Classification Pipeline**: Train machine learning models to classify medical claims\n",
    "- **Similarity Analysis**: Find similar claims based on embedding distances\n",
    "- **Clustering**: Group claims by semantic similarity\n",
    "\n",
    "## Key Files Generated\n",
    "\n",
    "- `outputs/embeddings/sample_embeddings.csv`: Complete embeddings dataset\n",
    "- `outputs/embeddings/train_embeddings.csv`: Training split for classification\n",
    "- `outputs/embeddings/test_embeddings.csv`: Test split for evaluation\n",
    "- `outputs/embedding_analysis.png`: Visualization plots\n",
    "- `outputs/logs/embedding_pipeline.log`: Detailed execution logs\n",
    "\n",
    "## Configuration Highlights\n",
    "\n",
    "The YAML configuration approach provides:\n",
    "- **Reproducibility**: Same results with same config\n",
    "- **Flexibility**: Easy parameter tuning\n",
    "- **Documentation**: Self-documenting pipeline settings\n",
    "- **Version Control**: Track configuration changes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgpt-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}