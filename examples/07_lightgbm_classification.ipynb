{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Classification with Medical Claim Embeddings\n",
    "\n",
    "This notebook demonstrates training LightGBM models using embeddings generated from medical claims data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LightGBM imports\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Local imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from pipelines.embedding_pipeline import EmbeddingPipeline\n",
    "from models.config_models import PipelineConfig\n",
    "from utils.logging_utils import get_logger\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate or Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for embedding pipeline\n",
    "data_path = Path('data/medical_claims_complete.csv')\n",
    "embeddings_path = Path('outputs/lightgbm_embeddings.csv')\n",
    "\n",
    "# Check if embeddings already exist\n",
    "if embeddings_path.exists():\n",
    "    print(\"Loading existing embeddings...\")\n",
    "    embeddings_df = pd.read_csv(embeddings_path)\n",
    "else:\n",
    "    print(\"Generating new embeddings...\")\n",
    "    config = {\n",
    "        'pipeline': {\n",
    "            'job_name': 'lightgbm_embeddings',\n",
    "            'log_level': 'INFO'\n",
    "        },\n",
    "        'data': {\n",
    "            'data_path': str(data_path.absolute()),\n",
    "            'claim_column': 'claim',\n",
    "            'label_column': 'label',\n",
    "            'mcid_column': 'mcid'\n",
    "        },\n",
    "        'llm': {\n",
    "            'model_url': 'http://localhost:8000',\n",
    "            'batch_size': 32,\n",
    "            'max_retries': 3\n",
    "        },\n",
    "        'outputs': {\n",
    "            'output_dir': 'outputs',\n",
    "            'save_embeddings': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    pipeline_config = PipelineConfig(**config)\n",
    "    embedding_pipeline = EmbeddingPipeline(pipeline_config)\n",
    "    embeddings_df = embedding_pipeline.run()\n",
    "    embeddings_df.to_csv(embeddings_path, index=False)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings_df.shape}\")\n",
    "print(f\"Columns: {embeddings_df.columns.tolist()[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "embedding_cols = [col for col in embeddings_df.columns if col.startswith('embedding_')]\n",
    "X = embeddings_df[embedding_cols].values\n",
    "y = embeddings_df['label'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Class distribution - Train: {np.bincount(y_train)}\")\n",
    "print(f\"Class distribution - Test: {np.bincount(y_test)}\")\n",
    "\n",
    "# Create LightGBM datasets\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Basic LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': ['binary_logloss', 'auc'],\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train model\n",
    "print(\"Training basic LightGBM model...\")\n",
    "lgb_model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'eval'],\n",
    "    num_boost_round=100,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(10)]\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall': recall_score(y_test, y_pred),\n",
    "    'f1': f1_score(y_test, y_pred),\n",
    "    'auc_roc': roc_auc_score(y_test, y_pred_proba)\n",
    "}\n",
    "\n",
    "print(\"\\nBasic Model Performance:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning with Scikit-learn API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for LightGBM\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'num_leaves': [31, 50, 70],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'min_child_samples': [20, 30, 40]\n",
    "}\n",
    "\n",
    "# Create LightGBM classifier\n",
    "lgbm_clf = LGBMClassifier(\n",
    "    objective='binary',\n",
    "    boosting_type='gbdt',\n",
    "    metric='auc',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Grid search with cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    lgbm_clf,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics_best = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_best),\n",
    "    'precision': precision_score(y_test, y_pred_best),\n",
    "    'recall': recall_score(y_test, y_pred_best),\n",
    "    'f1': f1_score(y_test, y_pred_best),\n",
    "    'auc_roc': roc_auc_score(y_test, y_pred_proba_best)\n",
    "}\n",
    "\n",
    "print(\"\\nTuned Model Performance:\")\n",
    "for metric, value in metrics_best.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from best model\n",
    "importance = best_model.feature_importances_\n",
    "indices = np.argsort(importance)[::-1]\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_n = 20\n",
    "plt.bar(range(top_n), importance[indices[:top_n]])\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Feature Importance (Split)')\n",
    "plt.title('Top 20 Most Important Features (LightGBM)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(importance, bins=50, edgecolor='black')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# LightGBM native feature importance plot\n",
    "lgb.plot_importance(lgb_model, max_num_features=20, figsize=(10, 8), importance_type='gain')\n",
    "plt.title('Feature Importance by Gain')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Top 10 feature indices: {indices[:10]}\")\n",
    "print(f\"Top 10 importance scores: {importance[indices[:10]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - LightGBM')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {metrics_best[\"auc_roc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - LightGBM')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics from basic model\n",
    "results = lgb_model.evals_result_\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Log Loss\n",
    "ax1.plot(results['train']['binary_logloss'], label='Train')\n",
    "ax1.plot(results['eval']['binary_logloss'], label='Validation')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Log Loss')\n",
    "ax1.set_title('LightGBM Training - Log Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# AUC\n",
    "ax2.plot(results['train']['auc'], label='Train')\n",
    "ax2.plot(results['eval']['auc'], label='Validation')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('AUC')\n",
    "ax2.set_title('LightGBM Training - AUC')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best iteration: {lgb_model.best_iteration}\")\n",
    "print(f\"Best validation AUC: {max(results['eval']['auc']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced LightGBM Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP value analysis (if shap is installed)\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    # Create SHAP explainer\n",
    "    explainer = shap.Explainer(best_model, X_train)\n",
    "    shap_values = explainer(X_test[:100])  # Calculate for subset\n",
    "    \n",
    "    # Summary plot\n",
    "    shap.summary_plot(shap_values, X_test[:100], show=False)\n",
    "    plt.title('SHAP Summary Plot - Top Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"SHAP not installed. Install with: pip install shap\")\n",
    "\n",
    "# Learning curve analysis\n",
    "train_sizes = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "for size in train_sizes:\n",
    "    n_samples = int(size * len(X_train))\n",
    "    X_subset = X_train[:n_samples]\n",
    "    y_subset = y_train[:n_samples]\n",
    "    \n",
    "    model = LGBMClassifier(**grid_search.best_params_, random_state=42)\n",
    "    model.fit(X_subset, y_subset)\n",
    "    \n",
    "    train_scores.append(model.score(X_subset, y_subset))\n",
    "    val_scores.append(model.score(X_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores, 'o-', label='Training score')\n",
    "plt.plot(train_sizes, val_scores, 'o-', label='Validation score')\n",
    "plt.xlabel('Training Set Size (fraction)')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Learning Curve - LightGBM')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('outputs/lightgbm_model')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model (native format)\n",
    "model_path = output_dir / f'lightgbm_model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt'\n",
    "lgb_model.save_model(str(model_path))\n",
    "print(f\"Native model saved to: {model_path}\")\n",
    "\n",
    "# Save sklearn API model\n",
    "import joblib\n",
    "sklearn_model_path = output_dir / f'lightgbm_sklearn_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pkl'\n",
    "joblib.dump(best_model, sklearn_model_path)\n",
    "print(f\"Sklearn model saved to: {sklearn_model_path}\")\n",
    "\n",
    "# Save metrics and results\n",
    "results = {\n",
    "    'model_type': 'LightGBM',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'best_parameters': grid_search.best_params_,\n",
    "    'cv_score': float(grid_search.best_score_),\n",
    "    'test_metrics': metrics_best,\n",
    "    'feature_importance': {\n",
    "        'top_features': indices[:20].tolist(),\n",
    "        'importance_scores': importance[indices[:20]].tolist()\n",
    "    },\n",
    "    'training_info': {\n",
    "        'best_iteration': lgb_model.best_iteration,\n",
    "        'num_trees': best_model.n_estimators,\n",
    "        'num_leaves': best_model.num_leaves\n",
    "    },\n",
    "    'data_info': {\n",
    "        'n_train': len(X_train),\n",
    "        'n_test': len(X_test),\n",
    "        'n_features': X_train.shape[1],\n",
    "        'class_distribution': {\n",
    "            'train': np.bincount(y_train).tolist(),\n",
    "            'test': np.bincount(y_test).tolist()\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics_path = output_dir / f'lightgbm_metrics_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n=== LightGBM Model Summary ===\")\n",
    "print(f\"Best AUC-ROC: {metrics_best['auc_roc']:.4f}\")\n",
    "print(f\"Best F1 Score: {metrics_best['f1']:.4f}\")\n",
    "print(f\"Number of trees: {best_model.n_estimators}\")\n",
    "print(f\"Number of leaves: {best_model.num_leaves}\")\n",
    "print(f\"Learning rate: {best_model.learning_rate}\")\n",
    "print(f\"Best iteration (early stopping): {lgb_model.best_iteration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "comparison_data = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC'],\n",
    "    'Basic LightGBM': [metrics[k] for k in ['accuracy', 'precision', 'recall', 'f1', 'auc_roc']],\n",
    "    'Tuned LightGBM': [metrics_best[k] for k in ['accuracy', 'precision', 'recall', 'f1', 'auc_roc']]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.set_index('Metric')\n",
    "\n",
    "# Plot comparison\n",
    "ax = comparison_df.plot(kind='bar', figsize=(10, 6), rot=0)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('LightGBM Model Performance Comparison')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "# Add value labels on bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print improvement summary\n",
    "print(\"\\n=== Performance Improvement ===\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1', 'auc_roc']:\n",
    "    improvement = (metrics_best[metric] - metrics[metric]) * 100\n",
    "    print(f\"{metric.capitalize()}: {improvement:+.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}