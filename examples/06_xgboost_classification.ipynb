{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classification with Medical Claim Embeddings\n",
    "\n",
    "This notebook demonstrates training XGBoost models using embeddings generated from medical claims data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# XGBoost imports\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Local imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from pipelines.embedding_pipeline import EmbeddingPipeline\n",
    "from models.config_models import PipelineConfig\n",
    "from utils.logging_utils import get_logger\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate or Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for embedding pipeline\n",
    "data_path = Path('data/medical_claims_complete.csv')\n",
    "embeddings_path = Path('outputs/xgboost_embeddings.csv')\n",
    "\n",
    "# Check if embeddings already exist\n",
    "if embeddings_path.exists():\n",
    "    print(\"Loading existing embeddings...\")\n",
    "    embeddings_df = pd.read_csv(embeddings_path)\n",
    "else:\n",
    "    print(\"Generating new embeddings...\")\n",
    "    config = {\n",
    "        'pipeline': {\n",
    "            'job_name': 'xgboost_embeddings',\n",
    "            'log_level': 'INFO'\n",
    "        },\n",
    "        'data': {\n",
    "            'data_path': str(data_path.absolute()),\n",
    "            'claim_column': 'claim',\n",
    "            'label_column': 'label',\n",
    "            'mcid_column': 'mcid'\n",
    "        },\n",
    "        'llm': {\n",
    "            'model_url': 'http://localhost:8000',\n",
    "            'batch_size': 32,\n",
    "            'max_retries': 3\n",
    "        },\n",
    "        'outputs': {\n",
    "            'output_dir': 'outputs',\n",
    "            'save_embeddings': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    pipeline_config = PipelineConfig(**config)\n",
    "    embedding_pipeline = EmbeddingPipeline(pipeline_config)\n",
    "    embeddings_df = embedding_pipeline.run()\n",
    "    embeddings_df.to_csv(embeddings_path, index=False)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings_df.shape}\")\n",
    "print(f\"Columns: {embeddings_df.columns.tolist()[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "embedding_cols = [col for col in embeddings_df.columns if col.startswith('embedding_')]\n",
    "X = embeddings_df[embedding_cols].values\n",
    "y = embeddings_df['label'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Class distribution - Train: {np.bincount(y_train)}\")\n",
    "print(f\"Class distribution - Test: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Basic XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train basic XGBoost model\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    objective='binary:logistic',\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train with evaluation\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=eval_set,\n",
    "    eval_metric=['logloss', 'auc'],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall': recall_score(y_test, y_pred),\n",
    "    'f1': f1_score(y_test, y_pred),\n",
    "    'auc_roc': roc_auc_score(y_test, y_pred_proba)\n",
    "}\n",
    "\n",
    "print(\"\\nBasic Model Performance:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Create XGBoost classifier\n",
    "xgb_tune = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Grid search with cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_tune,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics_best = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_best),\n",
    "    'precision': precision_score(y_test, y_pred_best),\n",
    "    'recall': recall_score(y_test, y_pred_best),\n",
    "    'f1': f1_score(y_test, y_pred_best),\n",
    "    'auc_roc': roc_auc_score(y_test, y_pred_proba_best)\n",
    "}\n",
    "\n",
    "print(\"\\nTuned Model Performance:\")\n",
    "for metric, value in metrics_best.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance = best_model.feature_importances_\n",
    "indices = np.argsort(importance)[::-1]\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_n = 20\n",
    "plt.bar(range(top_n), importance[indices[:top_n]])\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Feature Importance')\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(importance, bins=50, edgecolor='black')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Top 10 feature indices: {indices[:10]}\")\n",
    "print(f\"Top 10 importance scores: {importance[indices[:10]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {metrics_best[\"auc_roc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "results = xgb_model.evals_result()\n",
    "epochs = len(results['validation_0']['logloss'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Log Loss\n",
    "ax1.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "ax1.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "ax1.set_xlabel('Boosting Round')\n",
    "ax1.set_ylabel('Log Loss')\n",
    "ax1.set_title('XGBoost Log Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# AUC\n",
    "ax2.plot(x_axis, results['validation_0']['auc'], label='Train')\n",
    "ax2.plot(x_axis, results['validation_1']['auc'], label='Test')\n",
    "ax2.set_xlabel('Boosting Round')\n",
    "ax2.set_ylabel('AUC')\n",
    "ax2.set_title('XGBoost AUC')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('outputs/xgboost_model')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = output_dir / f'xgboost_model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "best_model.save_model(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save metrics\n",
    "results = {\n",
    "    'model_type': 'XGBoost',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'best_parameters': grid_search.best_params_,\n",
    "    'cv_score': float(grid_search.best_score_),\n",
    "    'test_metrics': metrics_best,\n",
    "    'feature_importance': {\n",
    "        'top_features': indices[:20].tolist(),\n",
    "        'importance_scores': importance[indices[:20]].tolist()\n",
    "    },\n",
    "    'data_info': {\n",
    "        'n_train': len(X_train),\n",
    "        'n_test': len(X_test),\n",
    "        'n_features': X_train.shape[1],\n",
    "        'class_distribution': {\n",
    "            'train': np.bincount(y_train).tolist(),\n",
    "            'test': np.bincount(y_test).tolist()\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics_path = output_dir / f'xgboost_metrics_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n=== XGBoost Model Summary ===\")\n",
    "print(f\"Best AUC-ROC: {metrics_best['auc_roc']:.4f}\")\n",
    "print(f\"Best F1 Score: {metrics_best['f1']:.4f}\")\n",
    "print(f\"Number of trees: {best_model.n_estimators}\")\n",
    "print(f\"Max depth: {best_model.max_depth}\")\n",
    "print(f\"Learning rate: {best_model.learning_rate}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}