{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  Embedding Pipeline Debug Testing\n",
    "\n",
    "## Purpose\n",
    "This notebook tests the embedding pipeline step-by-step using the actual pipeline code to debug embedding generation, checkpoint mechanisms, and data processing issues.\n",
    "\n",
    "## What This Tests\n",
    "- Embedding pipeline initialization and configuration\n",
    "- Data loading and batch processing\n",
    "- API communication for embedding generation\n",
    "- Checkpoint saving and resume functionality\n",
    "- Memory usage and performance monitoring\n",
    "- Error handling and retry mechanisms\n",
    "- Output file generation and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Add the mgpt_eval directory to Python path\n",
    "mgpt_eval_path = Path.cwd().parent if Path.cwd().name == 'examples' else Path.cwd()\n",
    "sys.path.insert(0, str(mgpt_eval_path))\n",
    "\n",
    "# Import actual pipeline modules\n",
    "from models.config_models import PipelineConfig\n",
    "from models.data_models import DataSample, DataBatch\n",
    "from pipelines.embedding_pipeline import EmbeddingPipeline\n",
    "from utils.logging_utils import setup_logging\n",
    "\n",
    "print(f\"âœ… Working directory: {Path.cwd()}\")\n",
    "print(f\"âœ… MGPT-eval path: {mgpt_eval_path}\")\n",
    "print(f\"âœ… Imports successful\")\n",
    "\n",
    "# Setup logging for debugging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('embedding_debug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Test Data and Configuration\n",
    "Set up test data and configuration for embedding pipeline testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data for embedding pipeline\n",
    "def create_embedding_test_data(num_samples=20):\n",
    "    \"\"\"Create test data specifically for embedding pipeline testing.\"\"\"\n",
    "    \n",
    "    test_data = {\n",
    "        'mcid': [f'TEST_EMBED_{i:03d}' for i in range(num_samples)],\n",
    "        'claims': [\n",
    "            'N6320 G0378 |eoc| Z91048 M1710',\n",
    "            'E119 76642 |eoc| K9289 O0903',\n",
    "            'I10 E785 |eoc| Z1239 M549',\n",
    "            'E119 N6320 |eoc| K9289 76642',\n",
    "            'O0903 Z91048 |eoc| M1710 G0378',\n",
    "            'K9289 I10 |eoc| E785 N6320',\n",
    "            'Z1239 E119 |eoc| 76642 M549',\n",
    "            'M1710 O0903 |eoc| G0378 Z91048',\n",
    "            'E785 K9289 |eoc| I10 N6320',\n",
    "            '76642 Z1239 |eoc| E119 M549'\n",
    "        ] * (num_samples // 10 + 1),  # Repeat patterns\n",
    "        'label': [i % 2 for i in range(num_samples)]  # Alternating 0, 1\n",
    "    }\n",
    "    \n",
    "    # Truncate to exact number of samples\n",
    "    for key in test_data:\n",
    "        test_data[key] = test_data[key][:num_samples]\n",
    "    \n",
    "    return pd.DataFrame(test_data)\n",
    "\n",
    "# Create test dataset\n",
    "test_df = create_embedding_test_data(20)\n",
    "test_data_path = mgpt_eval_path / \"examples\" / \"embedding_test_data.csv\"\n",
    "test_df.to_csv(test_data_path, index=False)\n",
    "\n",
    "print(f\"ğŸ“Š Created test data: {test_data_path}\")\n",
    "print(f\"ğŸ“Š Shape: {test_df.shape}\")\n",
    "print(f\"ğŸ“‹ First 3 samples:\")\n",
    "print(test_df.head(3))\n",
    "print(f\"ğŸ“Š Label distribution: {test_df['label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test configuration for embedding pipeline\n",
    "def create_embedding_test_config(data_path: str):\n",
    "    \"\"\"Create configuration optimized for embedding pipeline testing.\"\"\"\n",
    "    \n",
    "    test_config = {\n",
    "        'input': {\n",
    "            'dataset_path': str(data_path),\n",
    "            'split_ratio': 0.8\n",
    "        },\n",
    "        'job': {\n",
    "            'name': 'embedding_pipeline_test',\n",
    "            'output_dir': str(mgpt_eval_path / \"examples\" / \"test_outputs\"),\n",
    "            'random_seed': 42\n",
    "        },\n",
    "        'model_api': {\n",
    "            'base_url': 'http://localhost:8000',  # âš ï¸ Update this for your API\n",
    "            'batch_size': 4,  # Small batches for testing\n",
    "            'timeout': 30,  # Shorter timeout for faster debugging\n",
    "            'max_retries': 2\n",
    "        },\n",
    "        'pipeline_stages': {\n",
    "            'embeddings': True,\n",
    "            'classification': False,  # Focus on embeddings only\n",
    "            'evaluation': False,\n",
    "            'target_word_eval': False,\n",
    "            'summary_report': False,\n",
    "            'method_comparison': False\n",
    "        },\n",
    "        'data_processing': {\n",
    "            'random_seed': 42,\n",
    "            'max_sequence_length': 256,  # Shorter for faster processing\n",
    "            'include_mcid': True,\n",
    "            'output_format': 'json',  # JSON for easier debugging\n",
    "            'train_test_split': 0.8\n",
    "        },\n",
    "        'embedding_generation': {\n",
    "            'batch_size': 3,  # Small batches for detailed debugging\n",
    "            'save_interval': 2,  # Frequent checkpoints for testing\n",
    "            'checkpoint_dir': str(mgpt_eval_path / \"examples\" / \"test_checkpoints\"),\n",
    "            'resume_from_checkpoint': True,\n",
    "            'tokenizer_path': '/app/tokenizer'  # May not be used\n",
    "        },\n",
    "        'classification': {\n",
    "            'models': ['logistic_regression'],  # Minimal for testing\n",
    "            'cross_validation': {'n_folds': 2, 'scoring': 'accuracy', 'n_jobs': 1}\n",
    "        },\n",
    "        'evaluation': {\n",
    "            'metrics': ['accuracy'],\n",
    "            'visualization': {'generate_plots': False}\n",
    "        },\n",
    "        'target_word_evaluation': {\n",
    "            'enable': False,\n",
    "            'target_codes': ['E119']\n",
    "        },\n",
    "        'output': {\n",
    "            'embeddings_dir': 'embeddings',\n",
    "            'models_dir': 'models',\n",
    "            'metrics_dir': 'metrics',\n",
    "            'logs_dir': 'logs',\n",
    "            'save_best_model_only': False,\n",
    "            'model_format': 'pickle'\n",
    "        },\n",
    "        'logging': {\n",
    "            'level': 'DEBUG',  # Verbose logging for debugging\n",
    "            'console_level': 'DEBUG',\n",
    "            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            'file': 'logs/embedding_test.log'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return test_config\n",
    "\n",
    "# Create and validate test configuration\n",
    "test_config_dict = create_embedding_test_config(test_data_path)\n",
    "\n",
    "try:\n",
    "    test_config = PipelineConfig(**test_config_dict)\n",
    "    print(f\"âœ… Test configuration validated\")\n",
    "    \n",
    "    print(f\"\\nğŸ”§ Configuration summary:\")\n",
    "    print(f\"   Job name: {test_config.job.name}\")\n",
    "    print(f\"   Output dir: {test_config.job.output_dir}\")\n",
    "    print(f\"   API URL: {test_config.model_api.base_url}\")\n",
    "    print(f\"   API batch size: {test_config.model_api.batch_size}\")\n",
    "    print(f\"   Processing batch size: {test_config.embedding_generation.batch_size}\")\n",
    "    print(f\"   Checkpoint interval: {test_config.embedding_generation.save_interval}\")\n",
    "    print(f\"   Max sequence length: {test_config.data_processing.max_sequence_length}\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"âŒ Configuration validation failed: {e}\")\n",
    "    print(f\"\\nError details:\")\n",
    "    traceback.print_exc()\n",
    "    test_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Embedding Pipeline\n",
    "Create and initialize the embedding pipeline using the actual pipeline code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding pipeline\n",
    "def initialize_embedding_pipeline(config: PipelineConfig):\n",
    "    \"\"\"Initialize the embedding pipeline with debugging.\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸš€ Initializing embedding pipeline...\")\n",
    "    \n",
    "    try:\n",
    "        # Create output directories\n",
    "        output_dir = Path(config.job.output_dir) / config.job.name\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        embeddings_dir = output_dir / config.output.embeddings_dir\n",
    "        embeddings_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        checkpoint_dir = Path(config.embedding_generation.checkpoint_dir)\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"   ğŸ“ Output directory: {output_dir}\")\n",
    "        print(f\"   ğŸ“ Embeddings directory: {embeddings_dir}\")\n",
    "        print(f\"   ğŸ“ Checkpoint directory: {checkpoint_dir}\")\n",
    "        \n",
    "        # Initialize the actual embedding pipeline\n",
    "        pipeline = EmbeddingPipeline(config)\n",
    "        \n",
    "        print(f\"   âœ… Pipeline initialized successfully\")\n",
    "        return pipeline, output_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Pipeline initialization failed: {e}\")\n",
    "        print(f\"\\nError details:\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Initialize pipeline if config is valid\n",
    "if test_config:\n",
    "    embedding_pipeline, output_directory = initialize_embedding_pipeline(test_config)\nelse:\n",
    "    print(f\"âš ï¸  Skipping pipeline initialization due to config issues\")\n",
    "    embedding_pipeline = None\n",
    "    output_directory = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Data Loading and Preprocessing\n",
    "Test the data loading functionality within the embedding pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data loading using pipeline methods\n",
    "def test_pipeline_data_loading(pipeline: EmbeddingPipeline, config: PipelineConfig):\n",
    "    \"\"\"Test data loading using the actual pipeline data loading methods.\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Testing pipeline data loading...\")\n",
    "    \n",
    "    try:\n",
    "        # Load and validate data (using pipeline's internal methods)\n",
    "        print(f\"   ğŸ“‚ Loading data from: {config.input.dataset_path}\")\n",
    "        \n",
    "        # This mimics the pipeline's data loading process\n",
    "        data_df = pd.read_csv(config.input.dataset_path)\n",
    "        print(f\"   âœ… Data loaded: {data_df.shape}\")\n",
    "        \n",
    "        # Split data (mimics pipeline splitting)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        train_df, test_df = train_test_split(\n",
    "            data_df,\n",
    "            test_size=1 - config.input.split_ratio,\n",
    "            random_state=config.job.random_seed,\n",
    "            stratify=data_df['label']\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… Data split: train={len(train_df)}, test={len(test_df)}\")\n",
    "        \n",
    "        # Convert to DataBatch format (mimics pipeline processing)\n",
    "        train_samples = []\n",
    "        for _, row in train_df.iterrows():\n",
    "            sample = DataSample(\n",
    "                mcid=row['mcid'],\n",
    "                claims=row['claims'],\n",
    "                label=int(row['label'])\n",
    "            )\n",
    "            train_samples.append(sample)\n",
    "        \n",
    "        train_batch = DataBatch(samples=train_samples)\n",
    "        print(f\"   âœ… Train batch created: {len(train_batch.samples)} samples\")\n",
    "        \n",
    "        # Test batch methods\n",
    "        mcids = train_batch.get_mcids()\n",
    "        claims = train_batch.get_claims()\n",
    "        labels = train_batch.get_labels()\n",
    "        \n",
    "        print(f\"   ğŸ“‹ Sample MCIDs: {mcids[:3]}...\")\n",
    "        print(f\"   ğŸ“‹ Sample claims: {claims[0][:50]}...\")\n",
    "        print(f\"   ğŸ“‹ Label distribution: {pd.Series(labels).value_counts().to_dict()}\")\n",
    "        \n",
    "        return train_batch, test_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Data loading failed: {e}\")\n",
    "        print(f\"\\nError details:\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Test data loading\n",
    "if embedding_pipeline and test_config:\n",
    "    train_data_batch, test_data_df = test_pipeline_data_loading(embedding_pipeline, test_config)\nelse:\n",
    "    print(f\"âš ï¸  Skipping data loading test\")\n",
    "    train_data_batch = None\n",
    "    test_data_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test API Communication\n",
    "Test API communication using the pipeline's API client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API communication with small batches\n",
    "def test_api_communication(pipeline: EmbeddingPipeline, batch: DataBatch, config: PipelineConfig):\n",
    "    \"\"\"Test API communication using pipeline's API client.\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸŒ Testing API communication...\")\n",
    "    \n",
    "    if batch is None:\n",
    "        print(f\"   âŒ No data batch available for testing\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Test with a very small batch (first 2 samples)\n",
    "        test_samples = batch.samples[:2]\n",
    "        test_claims = [sample.claims for sample in test_samples]\n",
    "        test_mcids = [sample.mcid for sample in test_samples]\n",
    "        \n",
    "        print(f\"   ğŸ§ª Testing with {len(test_claims)} claims\")\n",
    "        print(f\"   ğŸ“‹ MCIDs: {test_mcids}\")\n",
    "        print(f\"   ğŸ“‹ Claims: {[claim[:30] + '...' for claim in test_claims]}\")\n",
    "        \n",
    "        # âš ï¸ Note: This will call the actual API - make sure your server is running!\n",
    "        # If you don't have a server, comment out the next section and use mock data\n",
    "        \n",
    "        # Test API call using pipeline's method\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # This is where the actual API call would happen\n",
    "        # We'll try to call it, but handle errors gracefully\n",
    "        try:\n",
    "            # Attempt real API call (comment out if no server available)\n",
    "            import requests\n",
    "            \n",
    "            api_url = f\"{config.model_api.base_url}/embeddings_batch\"\n",
    "            payload = {\"texts\": test_claims}\n",
    "            \n",
    "            response = requests.post(\n",
    "                api_url,\n",
    "                json=payload,\n",
    "                timeout=config.model_api.timeout,\n",
    "                headers={\"Content-Type\": \"application/json\"}\n",
    "            )\n",
    "            \n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                embeddings_data = response.json()\n",
    "                embeddings = embeddings_data.get('embeddings', [])\n",
    "                \n",
    "                print(f\"   âœ… API call successful\")\n",
    "                print(f\"   â±ï¸  Response time: {response_time:.2f}s\")\n",
    "                print(f\"   ğŸ“Š Received {len(embeddings)} embeddings\")\n",
    "                if embeddings:\n",
    "                    print(f\"   ğŸ“Š Embedding dimension: {len(embeddings[0])}\")\n",
    "                    print(f\"   ğŸ“Š Sample values: {embeddings[0][:3]}...\")\n",
    "                \n",
    "                return True\n",
    "            else:\n",
    "                print(f\"   âŒ API error: {response.status_code}\")\n",
    "                print(f\"   ğŸ“„ Response: {response.text[:200]}\")\n",
    "                return False\n",
    "                \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"   âš ï¸  API server not available - using mock data for testing\")\n",
    "            \n",
    "            # Create mock embeddings for testing pipeline logic\n",
    "            mock_embeddings = [\n",
    "                [0.1, 0.2, 0.3] * 256,  # 768-dim mock embedding\n",
    "                [0.4, 0.5, 0.6] * 256\n",
    "            ]\n",
    "            \n",
    "            print(f\"   ğŸ­ Using mock embeddings: {len(mock_embeddings)} embeddings\")\n",
    "            print(f\"   ğŸ“Š Mock embedding dimension: {len(mock_embeddings[0])}\")\n",
    "            \n",
    "            return \"mock\"  # Return special value to indicate mock mode\n",
    "            \n",
    "        except Exception as api_error:\n",
    "            print(f\"   âŒ API call failed: {api_error}\")\n",
    "            return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ API test failed: {e}\")\n",
    "        print(f\"\\nError details:\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Test API communication\n",
    "if embedding_pipeline and train_data_batch and test_config:\n",
    "    api_test_result = test_api_communication(embedding_pipeline, train_data_batch, test_config)\nelse:\n",
    "    print(f\"âš ï¸  Skipping API communication test\")\n",
    "    api_test_result = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Batch Processing Logic\n",
    "Test the batch processing and checkpoint mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batch processing logic\n",
    "def test_batch_processing(pipeline: EmbeddingPipeline, batch: DataBatch, config: PipelineConfig, use_mock=False):\n",
    "    \"\"\"Test batch processing logic with checkpoint simulation.\"\"\"\n",
    "    \n",
    "    print(f\"\\nâš™ï¸ Testing batch processing logic...\")\n",
    "    \n",
    "    if batch is None:\n",
    "        print(f\"   âŒ No data batch available\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Simulate batch processing (like the actual pipeline does)\n",
    "        batch_size = config.embedding_generation.batch_size\n",
    "        save_interval = config.embedding_generation.save_interval\n",
    "        \n",
    "        print(f\"   ğŸ“¦ Processing batch size: {batch_size}\")\n",
    "        print(f\"   ğŸ’¾ Save interval: {save_interval}\")\n",
    "        print(f\"   ğŸ“Š Total samples: {len(batch.samples)}\")\n",
    "        \n",
    "        # Calculate expected batches\n",
    "        total_batches = (len(batch.samples) + batch_size - 1) // batch_size\n",
    "        print(f\"   ğŸ“Š Expected batches: {total_batches}\")\n",
    "        \n",
    "        # Simulate processing each batch\n",
    "        all_embeddings = []\n",
    "        all_mcids = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for batch_idx in range(total_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(batch.samples))\n",
    "            \n",
    "            batch_samples = batch.samples[start_idx:end_idx]\n",
    "            batch_claims = [sample.claims for sample in batch_samples]\n",
    "            batch_mcids = [sample.mcid for sample in batch_samples]\n",
    "            batch_labels = [sample.label for sample in batch_samples]\n",
    "            \n",
    "            print(f\"\\n   ğŸ”„ Processing batch {batch_idx + 1}/{total_batches}\")\n",
    "            print(f\"      Samples: {len(batch_samples)} ({start_idx}-{end_idx-1})\")\n",
    "            print(f\"      MCIDs: {batch_mcids}\")\n",
    "            \n",
    "            # Simulate embedding generation\n",
    "            if use_mock or api_test_result == \"mock\":\n",
    "                # Generate mock embeddings\n",
    "                batch_embeddings = []\n",
    "                for i, claim in enumerate(batch_claims):\n",
    "                    # Create deterministic mock embedding based on claim content\n",
    "                    embedding_seed = hash(claim) % 1000\n",
    "                    mock_embedding = [(embedding_seed + j) / 1000.0 for j in range(768)]\n",
    "                    batch_embeddings.append(mock_embedding)\n",
    "                \n",
    "                print(f\"      ğŸ­ Generated {len(batch_embeddings)} mock embeddings\")\n",
    "            else:\n",
    "                # Real API call would go here\n",
    "                print(f\"      ğŸŒ Would call API with {len(batch_claims)} claims\")\n",
    "                # For testing, create mock embeddings\n",
    "                batch_embeddings = [[0.1] * 768 for _ in batch_claims]\n",
    "            \n",
    "            # Accumulate results\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            all_mcids.extend(batch_mcids)\n",
    "            all_labels.extend(batch_labels)\n",
    "            \n",
    "            # Simulate checkpoint saving\n",
    "            if (batch_idx + 1) % save_interval == 0 or batch_idx == total_batches - 1:\n",
    "                checkpoint_data = {\n",
    "                    'mcids': all_mcids,\n",
    "                    'labels': all_labels,\n",
    "                    'embeddings': all_embeddings,\n",
    "                    'processed_samples': len(all_embeddings),\n",
    "                    'batch_index': batch_idx + 1\n",
    "                }\n",
    "                \n",
    "                print(f\"      ğŸ’¾ Checkpoint: {len(all_embeddings)} embeddings saved\")\n",
    "        \n",
    "        # Final results\n",
    "        print(f\"\\n   âœ… Batch processing completed\")\n",
    "        print(f\"   ğŸ“Š Total embeddings: {len(all_embeddings)}\")\n",
    "        print(f\"   ğŸ“Š Embedding dimension: {len(all_embeddings[0]) if all_embeddings else 0}\")\n",
    "        print(f\"   ğŸ“‹ MCIDs: {all_mcids}\")\n",
    "        print(f\"   ğŸ“‹ Labels: {all_labels}\")\n",
    "        \n",
    "        return {\n",
    "            'mcids': all_mcids,\n",
    "            'labels': all_labels,\n",
    "            'embeddings': all_embeddings\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Batch processing failed: {e}\")\n",
    "        print(f\"\\nError details:\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Test batch processing\n",
    "if embedding_pipeline and train_data_batch and test_config:\n",
    "    use_mock_data = (api_test_result == \"mock\" or api_test_result == False)\n",
    "    batch_results = test_batch_processing(embedding_pipeline, train_data_batch, test_config, use_mock_data)\nelse:\n",
    "    print(f\"âš ï¸  Skipping batch processing test\")\n",
    "    batch_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Checkpoint and Resume Functionality\n",
    "Test the checkpoint saving and resume mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test checkpoint and resume functionality\n",
    "def test_checkpoint_resume(config: PipelineConfig, results_data: Dict):\n",
    "    \"\"\"Test checkpoint saving and resume functionality.\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Testing checkpoint and resume functionality...\")\n",
    "    \n",
    "    if results_data is None:\n",
    "        print(f\"   âŒ No results data available for testing\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        checkpoint_dir = Path(config.embedding_generation.checkpoint_dir)\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Test checkpoint saving\n",
    "        print(f\"   ğŸ’¾ Testing checkpoint saving...\")\n",
    "        \n",
    "        checkpoint_file = checkpoint_dir / \"test_checkpoint.json\"\n",
    "        \n",
    "        # Create checkpoint data (simulating pipeline checkpoint format)\n",
    "        checkpoint_data = {\n",
    "            'job_info': {\n",
    "                'job_name': config.job.name,\n",
    "                'timestamp': time.time(),\n",
    "                'config_hash': hash(str(config.dict()))\n",
    "            },\n",
    "            'progress': {\n",
    "                'total_samples': len(results_data['mcids']),\n",
    "                'processed_samples': len(results_data['mcids']),\n",
    "                'current_batch': 1,\n",
    "                'completion_percentage': 100.0\n",
    "            },\n",
    "            'data': {\n",
    "                'mcids': results_data['mcids'],\n",
    "                'labels': results_data['labels'],\n",
    "                'embeddings': results_data['embeddings'][:5]  # Save only first 5 for checkpoint test\n",
    "            },\n",
    "            'metadata': {\n",
    "                'embedding_dimension': len(results_data['embeddings'][0]) if results_data['embeddings'] else 0,\n",
    "                'format_version': '1.0'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save checkpoint\n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint_data, f, indent=2)\n",
    "        \n",
    "        print(f\"      âœ… Checkpoint saved: {checkpoint_file}\")\n",
    "        print(f\"      ğŸ“Š File size: {checkpoint_file.stat().st_size / 1024:.1f} KB\")\n",
    "        \n",
    "        # Test checkpoint loading\n",
    "        print(f\"   ğŸ“‚ Testing checkpoint loading...\")\n",
    "        \n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            loaded_checkpoint = json.load(f)\n",
    "        \n",
    "        print(f\"      âœ… Checkpoint loaded successfully\")\n",
    "        print(f\"      ğŸ“Š Job: {loaded_checkpoint['job_info']['job_name']}\")\n",
    "        print(f\"      ğŸ“Š Processed: {loaded_checkpoint['progress']['processed_samples']} samples\")\n",
    "        print(f\"      ğŸ“Š Embeddings in checkpoint: {len(loaded_checkpoint['data']['embeddings'])}\")\n",
    "        print(f\"      ğŸ“Š Embedding dimension: {loaded_checkpoint['metadata']['embedding_dimension']}\")\n",
    "        \n",
    "        # Test resume logic simulation\n",
    "        print(f\"   ğŸ”„ Testing resume logic...\")\n",
    "        \n",
    "        # Simulate resume scenario\n",
    "        total_samples = len(results_data['mcids'])\n",
    "        checkpoint_samples = len(loaded_checkpoint['data']['embeddings'])\n",
    "        remaining_samples = total_samples - checkpoint_samples\n",
    "        \n",
    "        print(f\"      ğŸ“Š Total samples: {total_samples}\")\n",
    "        print(f\"      ğŸ“Š Checkpoint samples: {checkpoint_samples}\")\n",
    "        print(f\"      ğŸ“Š Remaining samples: {remaining_samples}\")\n",
    "        \n",
    "        if remaining_samples > 0:\n",
    "            print(f\"      ğŸ”„ Would resume processing from sample {checkpoint_samples + 1}\")\n",
    "            print(f\"      ğŸ“‹ Next MCIDs to process: {results_data['mcids'][checkpoint_samples:checkpoint_samples+3]}\")\n",
    "        else:\n",
    "            print(f\"      âœ… All samples already processed in checkpoint\")\n",
    "        \n",
    "        # Test data consistency\n",
    "        print(f\"   ğŸ” Testing data consistency...\")\n",
    "        \n",
    "        original_mcids = results_data['mcids'][:checkpoint_samples]\n",
    "        checkpoint_mcids = loaded_checkpoint['data']['mcids'][:checkpoint_samples]\n",
    "        \n",
    "        if original_mcids == checkpoint_mcids:\n",
    "            print(f\"      âœ… MCID consistency check passed\")\n",
    "        else:\n",
    "            print(f\"      âŒ MCID consistency check failed\")\n",
    "            print(f\"         Original: {original_mcids[:3]}...\")\n",
    "            print(f\"         Checkpoint: {checkpoint_mcids[:3]}...\")\n",
    "        \n",
    "        # Cleanup\n",
    "        checkpoint_file.unlink()\n",
    "        print(f\"   ğŸ§¹ Cleaned up test checkpoint\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Checkpoint test failed: {e}\")\n",
    "        print(f\"\\nError details:\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Test checkpoint functionality\n",
    "if test_config and batch_results:\n",
    "    checkpoint_test_result = test_checkpoint_resume(test_config, batch_results)\nelse:\n",
    "    print(f\"âš ï¸  Skipping checkpoint test\")\n",
    "    checkpoint_test_result = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Output File Generation\n",
    "Test the final output file generation and format validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test output file generation\n",
    "def test_output_generation(config: PipelineConfig, results_data: Dict, output_dir: Path):\n",
    "    \"\"\"Test final output file generation.\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ“„ Testing output file generation...\")\n",
    "    \n",
    "    if results_data is None or output_dir is None:\n",
    "        print(f\"   âŒ No results data or output directory available\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Create embeddings output directory\n",
    "        embeddings_dir = output_dir / config.output.embeddings_dir\n",
    "        embeddings_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        output_format = config.data_processing.output_format\n",
    "        print(f\"   ğŸ“Š Output format: {output_format}\")\n",
    "        print(f\"   ğŸ“ Output directory: {embeddings_dir}\")\n",
    "        \n",
    "        # Test JSON output format\n",
    "        if output_format == \"json\":\n",
    "            print(f\"   ğŸ“ Testing JSON output format...\")\n",
    "            \n",
    "            train_output_file = embeddings_dir / \"train_embeddings.json\"\n",
    "            \n",
    "            output_data = {\n",
    "                'job_info': {\n",
    "                    'job_name': config.job.name,\n",
    "                    'timestamp': time.time(),\n",
    "                    'total_samples': len(results_data['mcids'])\n",
    "                },\n",
    "                'data': {\n",
    "                    'mcids': results_data['mcids'],\n",
    "                    'labels': results_data['labels'],\n",
    "                    'embeddings': results_data['embeddings']\n",
    "                },\n",
    "                'metadata': {\n",
    "                    'embedding_dimension': len(results_data['embeddings'][0]) if results_data['embeddings'] else 0,\n",
    "                    'format': 'json',\n",
    "                    'version': '1.0'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Save JSON file\n",
    "            with open(train_output_file, 'w') as f:\n",
    "                json.dump(output_data, f, indent=2)\n",
    "            \n",
    "            file_size = train_output_file.stat().st_size\n",
    "            print(f\"      âœ… JSON file saved: {train_output_file}\")\n",
    "            print(f\"      ğŸ“Š File size: {file_size / 1024:.1f} KB\")\n",
    "            print(f\"      ğŸ“Š Size per sample: {file_size / len(results_data['mcids']):.0f} bytes\")\n",
    "            \n",
    "            # Test loading JSON file\n",
    "            with open(train_output_file, 'r') as f:\n",
    "                loaded_data = json.load(f)\n",
    "            \n",
    "            print(f\"      âœ… JSON file loaded successfully\")\n",
    "            print(f\"      ğŸ“Š Samples in file: {len(loaded_data['data']['mcids'])}\")\n",
    "            print(f\"      ğŸ“Š Embedding dimension: {loaded_data['metadata']['embedding_dimension']}\")\n",
    "            \n",
    "        # Test CSV output format\n",
    "        print(f\"   ğŸ“ Testing CSV output format...\")\n",
    "        \n",
    "        train_csv_file = embeddings_dir / \"train_embeddings.csv\"\n",
    "        \n",
    "        # Create CSV format (flattened embeddings)\n",
    "        csv_data = []\n",
    "        for i, (mcid, label, embedding) in enumerate(zip(\n",
    "            results_data['mcids'], \n",
    "            results_data['labels'], \n",
    "            results_data['embeddings']\n",
    "        )):\n",
    "            row = {'mcid': mcid, 'label': label}\n",
    "            # Add embedding dimensions as separate columns\n",
    "            for j, emb_val in enumerate(embedding):\n",
    "                row[f'emb_{j}'] = emb_val\n",
    "            csv_data.append(row)\n",
    "        \n",
    "        csv_df = pd.DataFrame(csv_data)\n",
    "        csv_df.to_csv(train_csv_file, index=False)\n",
    "        \n",
    "        csv_file_size = train_csv_file.stat().st_size\n",
    "        print(f\"      âœ… CSV file saved: {train_csv_file}\")\n",
    "        print(f\"      ğŸ“Š File size: {csv_file_size / 1024:.1f} KB\")\n",
    "        print(f\"      ğŸ“Š CSV shape: {csv_df.shape}\")\n",
    "        print(f\"      ğŸ“Š CSV columns: {list(csv_df.columns)[:5]}... (+{len(csv_df.columns)-5} more)\")\n",
    "        \n",
    "        # Test loading CSV file\n",
    "        loaded_csv = pd.read_csv(train_csv_file)\n",
    "        print(f\"      âœ… CSV file loaded successfully\")\n",
    "        print(f\"      ğŸ“Š Loaded shape: {loaded_csv.shape}\")\n",
    "        \n",
    "        # Compare file sizes\n",
    "        if output_format == \"json\":\n",
    "            json_size = train_output_file.stat().st_size\n",
    "            csv_size = train_csv_file.stat().st_size\n",
    "            \n",
    "            print(f\"\\n   ğŸ“Š Format comparison:\")\n",
    "            print(f\"      JSON: {json_size / 1024:.1f} KB\")\n",
    "            print(f\"      CSV:  {csv_size / 1024:.1f} KB\")\n",
    "            print(f\"      Ratio: CSV is {csv_size / json_size:.1f}x the size of JSON\")\n",
    "        \n",
    "        # Test file integrity\n",
    "        print(f\"   ğŸ” Testing file integrity...\")\n",
    "        \n",
    "        # Check that we can reconstruct embeddings from CSV\n",
    "        embedding_cols = [col for col in loaded_csv.columns if col.startswith('emb_')]\n",
    "        reconstructed_embeddings = loaded_csv[embedding_cols].values.tolist()\n",
    "        \n",
    "        if len(reconstructed_embeddings) == len(results_data['embeddings']):\n",
    "            print(f\"      âœ… Embedding count matches: {len(reconstructed_embeddings)}\")\n",
    "        else:\n",
    "            print(f\"      âŒ Embedding count mismatch: {len(reconstructed_embeddings)} vs {len(results_data['embeddings'])}\")\n",
    "        \n",
    "        if len(reconstructed_embeddings) > 0 and len(reconstructed_embeddings[0]) == len(results_data['embeddings'][0]):\n",
    "            print(f\"      âœ… Embedding dimension matches: {len(reconstructed_embeddings[0])}\")\n",
    "        else:\n",
    "            print(f\"      âŒ Embedding dimension mismatch\")\n",
    "        \n",
    "        # Cleanup test files\n",
    "        if output_format == \"json\" and train_output_file.exists():\n",
    "            train_output_file.unlink()\n",
    "        if train_csv_file.exists():\n",
    "            train_csv_file.unlink()\n",
    "        \n",
    "        print(f\"   ğŸ§¹ Cleaned up test output files\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Output generation test failed: {e}\")\n",
    "        print(f\"\\nError details:\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Test output generation\n",
    "if test_config and batch_results and output_directory:\n",
    "    output_test_result = test_output_generation(test_config, batch_results, output_directory)\nelse:\n",
    "    print(f\"âš ï¸  Skipping output generation test\")\n",
    "    output_test_result = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Memory and Performance Analysis\n",
    "Analyze memory usage and performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test memory usage and performance\n",
    "def test_memory_performance(results_data: Dict, config: PipelineConfig):\n",
    "    \"\"\"Test memory usage and performance characteristics.\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ” Testing memory usage and performance...\")\n",
    "    \n",
    "    if results_data is None:\n",
    "        print(f\"   âŒ No results data available\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        import sys\n",
    "        \n",
    "        # Calculate memory usage\n",
    "        embeddings = results_data['embeddings']\n",
    "        mcids = results_data['mcids']\n",
    "        labels = results_data['labels']\n",
    "        \n",
    "        # Memory analysis\n",
    "        print(f\"   ğŸ“Š Memory usage analysis:\")\n",
    "        \n",
    "        # Calculate sizes\n",
    "        embeddings_size = sys.getsizeof(embeddings)\n",
    "        mcids_size = sys.getsizeof(mcids)\n",
    "        labels_size = sys.getsizeof(labels)\n",
    "        \n",
    "        # Estimate individual embedding size\n",
    "        if embeddings:\n",
    "            single_embedding_size = sys.getsizeof(embeddings[0])\n",
    "            embedding_dimension = len(embeddings[0])\n",
    "            \n",
    "            print(f\"      ğŸ§  Embeddings list: {embeddings_size / 1024:.1f} KB\")\n",
    "            print(f\"      ğŸ§  MCIDs list: {mcids_size / 1024:.1f} KB\")\n",
    "            print(f\"      ğŸ§  Labels list: {labels_size / 1024:.1f} KB\")\n",
    "            print(f\"      ğŸ§  Single embedding: {single_embedding_size} bytes\")\n",
    "            print(f\"      ğŸ§  Total memory: {(embeddings_size + mcids_size + labels_size) / 1024:.1f} KB\")\n",
    "            \n",
    "            # Extrapolate for larger datasets\n",
    "            samples_per_mb = (1024 * 1024) // single_embedding_size\n",
    "            print(f\"      ğŸ“Š Approx samples per MB: {samples_per_mb}\")\n",
    "            \n",
    "            # Calculate memory for different dataset sizes\n",
    "            for dataset_size in [1000, 10000, 100000]:\n",
    "                estimated_mb = (dataset_size * single_embedding_size) / (1024 * 1024)\n",
    "                print(f\"      ğŸ“Š {dataset_size:,} samples â‰ˆ {estimated_mb:.1f} MB\")\n",
    "        \n",
    "        # Performance analysis\n",
    "        print(f\"\\n   âš¡ Performance analysis:\")\n",
    "        \n",
    "        batch_size = config.embedding_generation.batch_size\n",
    "        api_batch_size = config.model_api.batch_size\n",
    "        \n",
    "        print(f\"      ğŸ“¦ Processing batch size: {batch_size}\")\n",
    "        print(f\"      ğŸŒ API batch size: {api_batch_size}\")\n",
    "        \n",
    "        # Calculate batching efficiency\n",
    "        if batch_size <= api_batch_size:\n",
    "            api_calls_per_batch = 1\n",
    "            efficiency = \"Optimal (1 API call per processing batch)\"\n",
    "        else:\n",
    "            api_calls_per_batch = (batch_size + api_batch_size - 1) // api_batch_size\n",
    "            efficiency = f\"Suboptimal ({api_calls_per_batch} API calls per processing batch)\"\n",
    "        \n",
    "        print(f\"      ğŸ“Š API calls per processing batch: {api_calls_per_batch}\")\n",
    "        print(f\"      ğŸ“Š Efficiency: {efficiency}\")\n",
    "        \n",
    "        # Estimate processing time for larger datasets\n",
    "        samples_processed = len(embeddings)\n",
    "        if samples_processed > 0:\n",
    "            # Assume 1 second per API call (rough estimate)\n",
    "            estimated_time_per_sample = api_calls_per_batch / batch_size  # API calls per sample\n",
    "            \n",
    "            print(f\"\\n   â±ï¸  Time estimates (assuming 1s per API call):\")\n",
    "            for dataset_size in [100, 1000, 10000]:\n",
    "                total_batches = (dataset_size + batch_size - 1) // batch_size\n",
    "                total_api_calls = total_batches * api_calls_per_batch\n",
    "                estimated_minutes = total_api_calls / 60\n",
    "                \n",
    "                print(f\"      ğŸ“Š {dataset_size:,} samples: {total_api_calls} API calls â‰ˆ {estimated_minutes:.1f} minutes\")\n",
    "        \n",
    "        # Configuration recommendations\n",
    "        print(f\"\\n   ğŸ’¡ Configuration recommendations:\")\n",
    "        \n",
    "        if batch_size > api_batch_size:\n",
    "            recommended_batch_size = api_batch_size\n",
    "            print(f\"      ğŸ”§ Consider reducing batch_size to {recommended_batch_size} for optimal API usage\")\n",
    "        elif batch_size < api_batch_size // 2:\n",
    "            recommended_batch_size = api_batch_size\n",
    "            print(f\"      ğŸ”§ Consider increasing batch_size to {recommended_batch_size} for better throughput\")\n",
    "        else:\n",
    "            print(f\"      âœ… Current batch_size ({batch_size}) is well-optimized\")\n",
    "        \n",
    "        # Memory recommendations\n",
    "        if embeddings:\n",
    "            total_memory_kb = (embeddings_size + mcids_size + labels_size) / 1024\n",
    "            if total_memory_kb > 100 * 1024:  # > 100 MB\n",
    "                print(f\"      ğŸ”§ Consider using CSV output format for large datasets (more memory efficient)\")\n",
    "            if config.embedding_generation.save_interval > 100:\n",
    "                print(f\"      ğŸ”§ Consider more frequent checkpoints (save_interval < 100) for large datasets\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Memory/performance analysis failed: {e}\")\n",
    "        print(f\"\\nError details:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Test memory and performance\n",
    "if batch_results and test_config:\n",
    "    test_memory_performance(batch_results, test_config)\nelse:\n",
    "    print(f\"âš ï¸  Skipping memory/performance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Summary and Recommendations\n",
    "Summarize all test results and provide actionable recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize embedding pipeline test results\n",
    "print(\"\\nğŸ“‹ Embedding Pipeline Debug Test Summary:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Collect test results\n",
    "test_results = [\n",
    "    (\"Configuration Validation\", test_config is not None),\n",
    "    (\"Pipeline Initialization\", embedding_pipeline is not None),\n",
    "    (\"Data Loading\", train_data_batch is not None),\n",
    "    (\"API Communication\", api_test_result in [True, \"mock\"]),\n",
    "    (\"Batch Processing\", batch_results is not None),\n",
    "    (\"Checkpoint Functionality\", checkpoint_test_result),\n",
    "    (\"Output Generation\", output_test_result)\n",
    "]\n",
    "\n",
    "# Calculate success rate\n",
    "passed_tests = [name for name, result in test_results if result]\n",
    "failed_tests = [name for name, result in test_results if not result]\n",
    "success_rate = len(passed_tests) / len(test_results) * 100\n",
    "\n",
    "print(f\"\\nğŸ¯ Test Results Summary:\")\n",
    "for test_name, result in test_results:\n",
    "    status = \"âœ… PASS\" if result else \"âŒ FAIL\"\n",
    "    print(f\"   {status} {test_name}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Overall Success Rate: {success_rate:.0f}%\")\n",
    "\n",
    "# Detailed analysis\n",
    "print(f\"\\nğŸ” Detailed Analysis:\")\n",
    "\n",
    "if api_test_result == \"mock\":\n",
    "    print(f\"   âš ï¸  API testing used mock data (server not available)\")\n",
    "    print(f\"      ğŸ”§ Start your MGPT server and rerun for full API testing\")\n",
    "elif api_test_result == True:\n",
    "    print(f\"   âœ… API communication successful with real server\")\n",
    "elif api_test_result == False:\n",
    "    print(f\"   âŒ API communication failed\")\n",
    "    print(f\"      ğŸ”§ Check server status and configuration\")\n",
    "\n",
    "if batch_results:\n",
    "    num_embeddings = len(batch_results['embeddings'])\n",
    "    embedding_dim = len(batch_results['embeddings'][0]) if batch_results['embeddings'] else 0\n",
    "    print(f\"   ğŸ“Š Successfully processed {num_embeddings} samples\")\n",
    "    print(f\"   ğŸ“Š Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# Recommendations based on results\n",
    "print(f\"\\nğŸ’¡ Recommendations:\")\n",
    "\n",
    "if success_rate >= 85:\n",
    "    print(f\"   ğŸ‰ Embedding pipeline is working well!\")\n",
    "    if api_test_result == \"mock\":\n",
    "        print(f\"   â¡ï¸  Next: Start your MGPT server and test with real API\")\n",
    "    else:\n",
    "        print(f\"   â¡ï¸  Next: Test classification pipeline with generated embeddings\")\n",
    "        print(f\"   â¡ï¸  Or: Run with larger dataset to test scalability\")\nelif success_rate >= 60:\n",
    "    print(f\"   âš ï¸  Most components working, but some issues need attention\")\n",
    "    print(f\"   ğŸ”§ Focus on fixing the failed tests listed above\")\nelse:\n",
    "    print(f\"   ğŸš¨ Significant issues detected - troubleshooting needed\")\n",
    "    print(f\"   ğŸ”§ Address configuration and API connectivity issues first\")\n",
    "\n",
    "# Configuration recommendations\n",
    "if test_config:\n",
    "    print(f\"\\nğŸ”§ Configuration for next steps:\")\n",
    "    print(f\"   # Update this configuration with your actual values\")\n",
    "    print(f\"   input:\")\n",
    "    print(f\"     dataset_path: \\\"{test_data_path}\\\"\")\n",
    "    print(f\"     split_ratio: {test_config.input.split_ratio}\")\n",
    "    print(f\"   model_api:\")\n",
    "    print(f\"     base_url: \\\"{test_config.model_api.base_url}\\\"\")\n",
    "    print(f\"     batch_size: {test_config.model_api.batch_size}\")\n",
    "    print(f\"   embedding_generation:\")\n",
    "    print(f\"     batch_size: {test_config.embedding_generation.batch_size}\")\n",
    "    print(f\"     save_interval: {test_config.embedding_generation.save_interval}\")\n",
    "\n",
    "print(f\"\\nğŸ“š Next testing steps:\")\n",
    "if success_rate >= 85:\n",
    "    print(f\"   1. Run test_04_Classification_Pipeline_Debug.ipynb\")\n",
    "    print(f\"   2. Test with your actual dataset\")\n",
    "    print(f\"   3. Run full end-to-end pipeline\")\nelse:\n",
    "    print(f\"   1. Fix failed tests in this notebook\")\n",
    "    print(f\"   2. Verify API server is running and accessible\")\n",
    "    print(f\"   3. Re-run this notebook until all tests pass\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nğŸ§¹ Cleaning up test files...\")\n",
    "try:\n",
    "    if test_data_path.exists():\n",
    "        test_data_path.unlink()\n",
    "        print(f\"   ğŸ—‘ï¸  Removed: {test_data_path.name}\")\n",
    "    \n",
    "    # Clean up any remaining test directories\n",
    "    test_output_dir = mgpt_eval_path / \"examples\" / \"test_outputs\"\n",
    "    if test_output_dir.exists():\n",
    "        import shutil\n",
    "        shutil.rmtree(test_output_dir)\n",
    "        print(f\"   ğŸ—‘ï¸  Removed: test_outputs directory\")\n",
    "    \n",
    "    test_checkpoint_dir = mgpt_eval_path / \"examples\" / \"test_checkpoints\"\n",
    "    if test_checkpoint_dir.exists():\n",
    "        import shutil\n",
    "        shutil.rmtree(test_checkpoint_dir)\n",
    "        print(f\"   ğŸ—‘ï¸  Removed: test_checkpoints directory\")\n",
    "        \nexcept Exception as e:\n",
    "    print(f\"   âš ï¸  Cleanup warning: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… Embedding pipeline debugging complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Debug Cell (Run if needed)\n",
    "Use this cell to test specific scenarios or debug issues found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug cell - modify as needed for specific testing\n",
    "\n",
    "# Example: Test with different batch sizes\n",
    "# if test_config and train_data_batch:\n",
    "#     print(\"Testing different batch sizes:\")\n",
    "#     for batch_size in [1, 2, 4, 8]:\n",
    "#         test_config_copy = test_config.copy(deep=True)\n",
    "#         test_config_copy.embedding_generation.batch_size = batch_size\n",
    "#         print(f\"\\nBatch size {batch_size}:\")\n",
    "#         batch_results = test_batch_processing(embedding_pipeline, train_data_batch, test_config_copy, True)\n",
    "\n",
    "# Example: Test API with custom claims\n",
    "# if embedding_pipeline:\n",
    "#     custom_claims = [\"E119 I10 N6320\", \"K9289 Z1239 M549\"]\n",
    "#     print(f\"Testing API with custom claims: {custom_claims}\")\n",
    "#     # Add your custom API test here\n",
    "\n",
    "# Example: Test memory usage with larger datasets\n",
    "# if batch_results:\n",
    "#     print(\"Testing memory scaling:\")\n",
    "#     for multiplier in [10, 100, 1000]:\n",
    "#         scaled_data = {\n",
    "#             'mcids': batch_results['mcids'] * multiplier,\n",
    "#             'labels': batch_results['labels'] * multiplier,\n",
    "#             'embeddings': batch_results['embeddings'] * multiplier\n",
    "#         }\n",
    "#         test_memory_performance(scaled_data, test_config)\n",
    "\n",
    "# Example: Debug specific configuration issues\n",
    "# if test_config:\n",
    "#     print(\"Current configuration:\")\n",
    "#     print(f\"API URL: {test_config.model_api.base_url}\")\n",
    "#     print(f\"API batch size: {test_config.model_api.batch_size}\")\n",
    "#     print(f\"Processing batch size: {test_config.embedding_generation.batch_size}\")\n",
    "#     print(f\"Save interval: {test_config.embedding_generation.save_interval}\")\n",
    "\n",
    "print(\"ğŸ’¡ Use this cell to run custom embedding pipeline tests and debug specific issues.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}