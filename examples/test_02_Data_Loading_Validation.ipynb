{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š Data Loading & Validation Testing\n",
    "\n",
    "## Purpose\n",
    "This notebook tests data loading, validation, and preprocessing step-by-step to debug data format issues, validate CSV structure, and ensure proper train/test splitting.\n",
    "\n",
    "## What This Tests\n",
    "- CSV file loading and validation\n",
    "- Required column checking (mcid, claims, label)\n",
    "- Data type validation and conversion\n",
    "- Train/test splitting logic\n",
    "- Data preprocessing and cleaning\n",
    "- Edge cases (empty data, missing values, invalid labels)\n",
    "- Configuration-based data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add the mgpt_eval directory to Python path\n",
    "mgpt_eval_path = Path.cwd().parent if Path.cwd().name == 'examples' else Path.cwd()\n",
    "sys.path.insert(0, str(mgpt_eval_path))\n",
    "\n",
    "# Import actual pipeline modules\n",
    "from models.config_models import PipelineConfig\n",
    "from models.data_models import DataSample, DataBatch\n",
    "from utils.logging_utils import setup_logging\n",
    "\n",
    "print(f\"âœ… Working directory: {Path.cwd()}\")\n",
    "print(f\"âœ… MGPT-eval path: {mgpt_eval_path}\")\n",
    "print(f\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Sample Test Data\n",
    "First, we'll create sample test data to ensure we have valid data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample test data with various scenarios\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample medical claims data for testing.\"\"\"\n",
    "    \n",
    "    sample_data = {\n",
    "        'mcid': [\n",
    "            'CLAIM_001', 'CLAIM_002', 'CLAIM_003', 'CLAIM_004', 'CLAIM_005',\n",
    "            'CLAIM_006', 'CLAIM_007', 'CLAIM_008', 'CLAIM_009', 'CLAIM_010',\n",
    "            'CLAIM_011', 'CLAIM_012', 'CLAIM_013', 'CLAIM_014', 'CLAIM_015'\n",
    "        ],\n",
    "        'claims': [\n",
    "            'N6320 G0378 |eoc| Z91048 M1710',\n",
    "            'E119 76642 |eoc| K9289 O0903',\n",
    "            'I10 E785 |eoc| Z1239 M549',\n",
    "            'E119 N6320 |eoc| K9289 76642',\n",
    "            'O0903 Z91048 |eoc| M1710 G0378',\n",
    "            'K9289 I10 |eoc| E785 N6320',\n",
    "            'Z1239 E119 |eoc| 76642 M549',\n",
    "            'M1710 O0903 |eoc| G0378 Z91048',\n",
    "            'E785 K9289 |eoc| I10 N6320',\n",
    "            '76642 Z1239 |eoc| E119 M549',\n",
    "            'G0378 M1710 |eoc| O0903 Z91048',\n",
    "            'N6320 E785 |eoc| I10 K9289',\n",
    "            'M549 76642 |eoc| Z1239 E119',\n",
    "            'Z91048 G0378 |eoc| M1710 O0903',\n",
    "            'K9289 N6320 |eoc| E785 I10'\n",
    "        ],\n",
    "        'label': [1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1]\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(sample_data)\n",
    "\n",
    "# Create and save sample data\n",
    "sample_df = create_sample_data()\n",
    "sample_data_path = mgpt_eval_path / \"examples\" / \"sample_test_data.csv\"\n",
    "sample_df.to_csv(sample_data_path, index=False)\n",
    "\n",
    "print(f\"ğŸ“„ Created sample data: {sample_data_path}\")\n",
    "print(f\"ğŸ“Š Data shape: {sample_df.shape}\")\n",
    "print(f\"ğŸ“‹ Columns: {list(sample_df.columns)}\")\n",
    "print(f\"\\nğŸ” First 3 rows:\")\n",
    "print(sample_df.head(3))\n",
    "print(f\"\\nğŸ“Š Label distribution:\")\n",
    "print(sample_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Configuration\n",
    "Load configuration and test data path resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration for testing\n",
    "config_path = mgpt_eval_path / \"configs\" / \"templates\" / \"04_full_pipeline.yaml\"\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Override with our test data path\n",
    "config_dict['input']['dataset_path'] = str(sample_data_path)\n",
    "\n",
    "# Validate configuration\n",
    "try:\n",
    "    config = PipelineConfig(**config_dict)\n",
    "    print(\"âœ… Configuration loaded and validated\")\n",
    "    \n",
    "    # Extract input configuration\n",
    "    input_config = config.input\n",
    "    print(f\"\\nğŸ“ Input Configuration:\")\n",
    "    print(f\"  Dataset path: {input_config.dataset_path}\")\n",
    "    if hasattr(input_config, 'split_ratio'):\n",
    "        print(f\"  Split ratio: {input_config.split_ratio}\")\n",
    "    if hasattr(input_config, 'train_dataset_path'):\n",
    "        print(f\"  Train dataset: {input_config.train_dataset_path}\")\n",
    "    if hasattr(input_config, 'test_dataset_path'):\n",
    "        print(f\"  Test dataset: {input_config.test_dataset_path}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Configuration validation failed: {e}\")\n",
    "    config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Basic Data Loading\n",
    "Test the basic CSV loading functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic data loading function (mimics pipeline data loading)\n",
    "def load_and_validate_data(file_path: str):\n",
    "    \"\"\"Load and validate data similar to the pipeline.\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ“‚ Loading data from: {file_path}\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Dataset file not found: {file_path}\")\n",
    "    \n",
    "    # Load CSV\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"âœ… CSV loaded successfully\")\n",
    "        print(f\"ğŸ“Š Shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load CSV: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Check required columns\n",
    "    required_columns = ['mcid', 'claims', 'label']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"âŒ Missing required columns: {missing_columns}\")\n",
    "        print(f\"ğŸ“‹ Available columns: {list(df.columns)}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"âœ… All required columns present: {required_columns}\")\n",
    "    \n",
    "    # Check data types and basic validation\n",
    "    print(f\"\\nğŸ” Data validation:\")\n",
    "    \n",
    "    # Check for empty dataframe\n",
    "    if len(df) == 0:\n",
    "        print(f\"âŒ Dataset is empty\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"âœ… Dataset has {len(df)} rows\")\n",
    "    \n",
    "    # Check for null values\n",
    "    null_counts = df.isnull().sum()\n",
    "    if null_counts.any():\n",
    "        print(f\"âš ï¸  Null values found:\")\n",
    "        for col, count in null_counts.items():\n",
    "            if count > 0:\n",
    "                print(f\"   {col}: {count} null values\")\n",
    "    else:\n",
    "        print(f\"âœ… No null values found\")\n",
    "    \n",
    "    # Check label values\n",
    "    unique_labels = df['label'].unique()\n",
    "    print(f\"ğŸ“Š Unique labels: {sorted(unique_labels)}\")\n",
    "    \n",
    "    if not all(label in [0, 1] for label in unique_labels):\n",
    "        print(f\"âŒ Invalid label values. Expected 0 and 1, found: {unique_labels}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"âœ… Valid binary labels\")\n",
    "    \n",
    "    # Check if all labels are the same\n",
    "    if len(unique_labels) == 1:\n",
    "        print(f\"âš ï¸  All labels are the same value: {unique_labels[0]}\")\n",
    "        print(f\"   This will cause issues with classification\")\n",
    "    \n",
    "    # Check MCID uniqueness\n",
    "    duplicate_mcids = df['mcid'].duplicated().sum()\n",
    "    if duplicate_mcids > 0:\n",
    "        print(f\"âš ï¸  Found {duplicate_mcids} duplicate MCIDs\")\n",
    "    else:\n",
    "        print(f\"âœ… All MCIDs are unique\")\n",
    "    \n",
    "    # Check claims format\n",
    "    empty_claims = df['claims'].str.strip().eq('').sum()\n",
    "    if empty_claims > 0:\n",
    "        print(f\"âš ï¸  Found {empty_claims} empty claims\")\n",
    "    else:\n",
    "        print(f\"âœ… No empty claims found\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test data loading\n",
    "loaded_df = load_and_validate_data(str(sample_data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Train/Test Splitting\n",
    "Test the train/test splitting logic used by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train/test splitting (mimics pipeline splitting logic)\n",
    "def test_train_test_split(df, split_ratio=0.8, random_seed=42):\n",
    "    \"\"\"Test train/test splitting with stratification.\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ”„ Testing train/test split:\")\n",
    "    print(f\"   Split ratio: {split_ratio} (train) / {1-split_ratio} (test)\")\n",
    "    print(f\"   Random seed: {random_seed}\")\n",
    "    \n",
    "    if df is None:\n",
    "        print(f\"âŒ Cannot split: DataFrame is None\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Check if stratification is possible\n",
    "        label_counts = df['label'].value_counts()\n",
    "        min_class_count = label_counts.min()\n",
    "        test_size = 1 - split_ratio\n",
    "        min_test_samples = int(min_class_count * test_size)\n",
    "        \n",
    "        print(f\"ğŸ“Š Label distribution: {dict(label_counts)}\")\n",
    "        print(f\"ğŸ“Š Minimum class count: {min_class_count}\")\n",
    "        print(f\"ğŸ“Š Expected test samples per class: ~{min_test_samples}\")\n",
    "        \n",
    "        if min_test_samples < 1:\n",
    "            print(f\"âš ï¸  Warning: Very small test set for minority class\")\n",
    "            print(f\"   Consider using a smaller split ratio or more data\")\n",
    "        \n",
    "        # Perform stratified split\n",
    "        train_df, test_df = train_test_split(\n",
    "            df,\n",
    "            test_size=test_size,\n",
    "            random_state=random_seed,\n",
    "            stratify=df['label']  # Maintain label distribution\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nâœ… Split successful:\")\n",
    "        print(f\"   Train set: {len(train_df)} samples\")\n",
    "        print(f\"   Test set: {len(test_df)} samples\")\n",
    "        \n",
    "        # Verify label distributions\n",
    "        train_dist = train_df['label'].value_counts(normalize=True).sort_index()\n",
    "        test_dist = test_df['label'].value_counts(normalize=True).sort_index()\n",
    "        original_dist = df['label'].value_counts(normalize=True).sort_index()\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Label distribution comparison:\")\n",
    "        print(f\"   Original: {dict(original_dist.round(3))}\")\n",
    "        print(f\"   Train:    {dict(train_dist.round(3))}\")\n",
    "        print(f\"   Test:     {dict(test_dist.round(3))}\")\n",
    "        \n",
    "        # Check if distributions are similar (within 5%)\n",
    "        dist_diff = abs(train_dist - test_dist).max()\n",
    "        if dist_diff > 0.05:\n",
    "            print(f\"âš ï¸  Warning: Label distributions differ by {dist_diff:.3f}\")\n",
    "        else:\n",
    "            print(f\"âœ… Label distributions are well balanced\")\n",
    "        \n",
    "        return train_df, test_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Split failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Test splitting with configuration values\n",
    "if loaded_df is not None and config:\n",
    "    split_ratio = getattr(config.input, 'split_ratio', 0.8)\n",
    "    random_seed = getattr(config.job, 'random_seed', 42)\n",
    "    \n",
    "    train_df, test_df = test_train_test_split(loaded_df, split_ratio, random_seed)\nelse:\n",
    "    print(\"âš ï¸  Skipping split test due to data loading or config issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Data Model Validation\n",
    "Test the Pydantic data models used by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DataSample and DataBatch models\n",
    "def test_data_models(df):\n",
    "    \"\"\"Test Pydantic data models with sample data.\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ§ª Testing Pydantic data models:\")\n",
    "    \n",
    "    if df is None or len(df) == 0:\n",
    "        print(f\"âŒ No data available for testing\")\n",
    "        return\n",
    "    \n",
    "    # Test DataSample model\n",
    "    print(f\"\\n1. Testing DataSample model:\")\n",
    "    try:\n",
    "        # Create DataSample from first row\n",
    "        first_row = df.iloc[0]\n",
    "        sample = DataSample(\n",
    "            mcid=first_row['mcid'],\n",
    "            claims=first_row['claims'],\n",
    "            label=int(first_row['label'])\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… DataSample created successfully\")\n",
    "        print(f\"   ğŸ“‹ MCID: {sample.mcid}\")\n",
    "        print(f\"   ğŸ“‹ Claims: {sample.claims[:50]}...\")\n",
    "        print(f\"   ğŸ“‹ Label: {sample.label}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ DataSample creation failed: {e}\")\n",
    "    \n",
    "    # Test DataBatch model\n",
    "    print(f\"\\n2. Testing DataBatch model:\")\n",
    "    try:\n",
    "        # Create DataBatch from multiple rows\n",
    "        batch_size = min(3, len(df))\n",
    "        batch_rows = df.head(batch_size)\n",
    "        \n",
    "        samples = []\n",
    "        for _, row in batch_rows.iterrows():\n",
    "            sample = DataSample(\n",
    "                mcid=row['mcid'],\n",
    "                claims=row['claims'],\n",
    "                label=int(row['label'])\n",
    "            )\n",
    "            samples.append(sample)\n",
    "        \n",
    "        batch = DataBatch(samples=samples)\n",
    "        \n",
    "        print(f\"   âœ… DataBatch created successfully\")\n",
    "        print(f\"   ğŸ“Š Batch size: {len(batch.samples)}\")\n",
    "        print(f\"   ğŸ“‹ MCIDs: {[s.mcid for s in batch.samples]}\")\n",
    "        print(f\"   ğŸ“‹ Labels: {[s.label for s in batch.samples]}\")\n",
    "        \n",
    "        # Test batch properties\n",
    "        mcids = batch.get_mcids()\n",
    "        claims = batch.get_claims()\n",
    "        labels = batch.get_labels()\n",
    "        \n",
    "        print(f\"   âœ… Batch methods work correctly\")\n",
    "        print(f\"   ğŸ“Š MCIDs extracted: {len(mcids)}\")\n",
    "        print(f\"   ğŸ“Š Claims extracted: {len(claims)}\")\n",
    "        print(f\"   ğŸ“Š Labels extracted: {len(labels)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ DataBatch creation failed: {e}\")\n",
    "    \n",
    "    # Test edge cases\n",
    "    print(f\"\\n3. Testing edge cases:\")\n",
    "    \n",
    "    # Test empty claims\n",
    "    try:\n",
    "        empty_sample = DataSample(\n",
    "            mcid=\"TEST_EMPTY\",\n",
    "            claims=\"\",\n",
    "            label=0\n",
    "        )\n",
    "        print(f\"   âš ï¸  Empty claims accepted (may cause issues downstream)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ… Empty claims rejected: {e}\")\n",
    "    \n",
    "    # Test invalid label\n",
    "    try:\n",
    "        invalid_sample = DataSample(\n",
    "            mcid=\"TEST_INVALID\",\n",
    "            claims=\"E119 I10\",\n",
    "            label=2  # Invalid label (should be 0 or 1)\n",
    "        )\n",
    "        print(f\"   âš ï¸  Invalid label (2) accepted\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ… Invalid label rejected: {e}\")\n",
    "\n",
    "# Test data models\n",
    "if loaded_df is not None:\n",
    "    test_data_models(loaded_df)\nelse:\n",
    "    print(\"âš ï¸  Skipping data model tests due to loading issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Edge Cases and Error Scenarios\n",
    "Test various problematic data scenarios to ensure robust error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create problematic test datasets\n",
    "def create_problematic_datasets():\n",
    "    \"\"\"Create datasets with various issues for testing.\"\"\"\n",
    "    \n",
    "    test_datasets = {}\n",
    "    base_path = mgpt_eval_path / \"examples\"\n",
    "    \n",
    "    # 1. Missing columns\n",
    "    missing_cols_df = pd.DataFrame({\n",
    "        'id': ['C001', 'C002'],  # Wrong column name\n",
    "        'text': ['E119 I10', 'N6320 K9289'],  # Wrong column name\n",
    "        'target': [1, 0]  # Wrong column name\n",
    "    })\n",
    "    missing_cols_path = base_path / \"test_missing_columns.csv\"\n",
    "    missing_cols_df.to_csv(missing_cols_path, index=False)\n",
    "    test_datasets['missing_columns'] = missing_cols_path\n",
    "    \n",
    "    # 2. Invalid labels\n",
    "    invalid_labels_df = pd.DataFrame({\n",
    "        'mcid': ['C001', 'C002', 'C003'],\n",
    "        'claims': ['E119 I10', 'N6320 K9289', 'Z1239 M549'],\n",
    "        'label': [1, 2, 0]  # Invalid label '2'\n",
    "    })\n",
    "    invalid_labels_path = base_path / \"test_invalid_labels.csv\"\n",
    "    invalid_labels_df.to_csv(invalid_labels_path, index=False)\n",
    "    test_datasets['invalid_labels'] = invalid_labels_path\n",
    "    \n",
    "    # 3. Null values\n",
    "    null_values_df = pd.DataFrame({\n",
    "        'mcid': ['C001', None, 'C003'],\n",
    "        'claims': ['E119 I10', 'N6320 K9289', None],\n",
    "        'label': [1, 0, 1]\n",
    "    })\n",
    "    null_values_path = base_path / \"test_null_values.csv\"\n",
    "    null_values_df.to_csv(null_values_path, index=False)\n",
    "    test_datasets['null_values'] = null_values_path\n",
    "    \n",
    "    # 4. Single class\n",
    "    single_class_df = pd.DataFrame({\n",
    "        'mcid': ['C001', 'C002', 'C003'],\n",
    "        'claims': ['E119 I10', 'N6320 K9289', 'Z1239 M549'],\n",
    "        'label': [1, 1, 1]  # All same label\n",
    "    })\n",
    "    single_class_path = base_path / \"test_single_class.csv\"\n",
    "    single_class_df.to_csv(single_class_path, index=False)\n",
    "    test_datasets['single_class'] = single_class_path\n",
    "    \n",
    "    # 5. Empty dataset\n",
    "    empty_df = pd.DataFrame(columns=['mcid', 'claims', 'label'])\n",
    "    empty_path = base_path / \"test_empty.csv\"\n",
    "    empty_df.to_csv(empty_path, index=False)\n",
    "    test_datasets['empty'] = empty_path\n",
    "    \n",
    "    # 6. Duplicate MCIDs\n",
    "    duplicate_mcids_df = pd.DataFrame({\n",
    "        'mcid': ['C001', 'C001', 'C002'],  # Duplicate MCID\n",
    "        'claims': ['E119 I10', 'N6320 K9289', 'Z1239 M549'],\n",
    "        'label': [1, 0, 1]\n",
    "    })\n",
    "    duplicate_path = base_path / \"test_duplicate_mcids.csv\"\n",
    "    duplicate_mcids_df.to_csv(duplicate_path, index=False)\n",
    "    test_datasets['duplicate_mcids'] = duplicate_path\n",
    "    \n",
    "    return test_datasets\n",
    "\n",
    "# Create and test problematic datasets\n",
    "print(\"ğŸ§ª Creating problematic test datasets:\")\n",
    "problematic_datasets = create_problematic_datasets()\n",
    "\n",
    "for dataset_name, dataset_path in problematic_datasets.items():\n",
    "    print(f\"   ğŸ“„ {dataset_name}: {dataset_path.name}\")\n",
    "\n",
    "print(f\"\\nğŸ” Testing error handling:\")\n",
    "\n",
    "# Test each problematic dataset\n",
    "for dataset_name, dataset_path in problematic_datasets.items():\n",
    "    print(f\"\\n--- Testing {dataset_name} ---\")\n",
    "    try:\n",
    "        result_df = load_and_validate_data(str(dataset_path))\n",
    "        if result_df is not None:\n",
    "            print(f\"   âš ï¸  Dataset loaded despite issues (may cause problems later)\")\n",
    "        else:\n",
    "            print(f\"   âœ… Dataset correctly rejected\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ… Exception caught: {e}\")\n",
    "\n",
    "# Cleanup test files\n",
    "print(f\"\\nğŸ§¹ Cleaning up test files...\")\n",
    "for dataset_path in problematic_datasets.values():\n",
    "    try:\n",
    "        dataset_path.unlink()\n",
    "        print(f\"   ğŸ—‘ï¸  Deleted: {dataset_path.name}\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Alternative Input Configurations\n",
    "Test different input configuration scenarios (separate train/test files, embeddings files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test separate train/test file configuration\n",
    "def test_separate_files_config():\n",
    "    \"\"\"Test configuration with separate train and test files.\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ”„ Testing separate train/test files configuration:\")\n",
    "    \n",
    "    if train_df is None or test_df is None:\n",
    "        print(f\"   âŒ No train/test data available\")\n",
    "        return\n",
    "    \n",
    "    # Save separate files\n",
    "    train_path = mgpt_eval_path / \"examples\" / \"test_train.csv\"\n",
    "    test_path = mgpt_eval_path / \"examples\" / \"test_test.csv\"\n",
    "    \n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "    \n",
    "    print(f\"   ğŸ“„ Train file: {train_path} ({len(train_df)} samples)\")\n",
    "    print(f\"   ğŸ“„ Test file: {test_path} ({len(test_df)} samples)\")\n",
    "    \n",
    "    # Test configuration with separate files\n",
    "    separate_config = {\n",
    "        'input': {\n",
    "            'train_dataset_path': str(train_path),\n",
    "            'test_dataset_path': str(test_path)\n",
    "            # Note: split_ratio should be ignored\n",
    "        },\n",
    "        'job': {'name': 'test_separate_files', 'output_dir': 'outputs'},\n",
    "        'model_api': {'base_url': 'http://localhost:8000', 'batch_size': 32, 'timeout': 300, 'max_retries': 3},\n",
    "        'pipeline_stages': {'embeddings': True, 'classification': True, 'evaluation': True, 'target_word_eval': False, 'summary_report': True, 'method_comparison': False},\n",
    "        'data_processing': {'random_seed': 42, 'max_sequence_length': 512, 'include_mcid': True, 'output_format': 'json'},\n",
    "        'embedding_generation': {'batch_size': 16, 'save_interval': 100, 'checkpoint_dir': 'outputs/checkpoints', 'resume_from_checkpoint': True, 'tokenizer_path': '/app/tokenizer'},\n",
    "        'classification': {'models': ['logistic_regression'], 'cross_validation': {'n_folds': 3, 'scoring': 'roc_auc', 'n_jobs': 1}},\n",
    "        'evaluation': {'metrics': ['accuracy', 'precision', 'recall', 'f1_score'], 'visualization': {'generate_plots': False}},\n",
    "        'target_word_evaluation': {'enable': False, 'target_codes': ['E119']},\n",
    "        'output': {'embeddings_dir': 'outputs/embeddings', 'models_dir': 'outputs/models', 'metrics_dir': 'outputs/metrics', 'logs_dir': 'outputs/logs'},\n",
    "        'logging': {'level': 'INFO', 'console_level': 'INFO', 'format': '%(asctime)s - %(levelname)s - %(message)s', 'file': 'outputs/logs/pipeline.log'}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        separate_config_obj = PipelineConfig(**separate_config)\n",
    "        print(f\"   âœ… Separate files configuration validated\")\n",
    "        \n",
    "        # Test loading both files\n",
    "        train_loaded = load_and_validate_data(str(train_path))\n",
    "        test_loaded = load_and_validate_data(str(test_path))\n",
    "        \n",
    "        if train_loaded is not None and test_loaded is not None:\n",
    "            print(f\"   âœ… Both files loaded successfully\")\n",
    "            print(f\"   ğŸ“Š Train: {len(train_loaded)} samples\")\n",
    "            print(f\"   ğŸ“Š Test: {len(test_loaded)} samples\")\n",
    "        else:\n",
    "            print(f\"   âŒ Failed to load one or both files\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Configuration validation failed: {e}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    try:\n",
    "        train_path.unlink()\n",
    "        test_path.unlink()\n",
    "        print(f\"   ğŸ§¹ Cleaned up test files\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Test embeddings file configuration\n",
    "def test_embeddings_config():\n",
    "    \"\"\"Test configuration with pre-computed embeddings.\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ”„ Testing embeddings files configuration:\")\n",
    "    \n",
    "    # Create mock embeddings data\n",
    "    mock_embeddings_data = {\n",
    "        'mcids': ['C001', 'C002', 'C003'],\n",
    "        'labels': [1, 0, 1],\n",
    "        'embeddings': [\n",
    "            [0.1, 0.2, 0.3] * 256,  # 768-dim mock embedding\n",
    "            [0.4, 0.5, 0.6] * 256,\n",
    "            [0.7, 0.8, 0.9] * 256\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save mock embeddings\n",
    "    train_emb_path = mgpt_eval_path / \"examples\" / \"test_train_embeddings.json\"\n",
    "    test_emb_path = mgpt_eval_path / \"examples\" / \"test_test_embeddings.json\"\n",
    "    \n",
    "    with open(train_emb_path, 'w') as f:\n",
    "        json.dump(mock_embeddings_data, f)\n",
    "    \n",
    "    with open(test_emb_path, 'w') as f:\n",
    "        json.dump(mock_embeddings_data, f)\n",
    "    \n",
    "    print(f\"   ğŸ“„ Train embeddings: {train_emb_path}\")\n",
    "    print(f\"   ğŸ“„ Test embeddings: {test_emb_path}\")\n",
    "    \n",
    "    # Test embeddings configuration\n",
    "    embeddings_config = {\n",
    "        'input': {\n",
    "            'train_embeddings_path': str(train_emb_path),\n",
    "            'test_embeddings_path': str(test_emb_path)\n",
    "        },\n",
    "        'job': {'name': 'test_embeddings', 'output_dir': 'outputs'},\n",
    "        'model_api': {'base_url': 'http://localhost:8000', 'batch_size': 32, 'timeout': 300, 'max_retries': 3},\n",
    "        'pipeline_stages': {'embeddings': False, 'classification': True, 'evaluation': True, 'target_word_eval': False, 'summary_report': True, 'method_comparison': False},\n",
    "        'data_processing': {'random_seed': 42, 'max_sequence_length': 512, 'include_mcid': True, 'output_format': 'json'},\n",
    "        'embedding_generation': {'batch_size': 16, 'save_interval': 100, 'checkpoint_dir': 'outputs/checkpoints', 'resume_from_checkpoint': True, 'tokenizer_path': '/app/tokenizer'},\n",
    "        'classification': {'models': ['logistic_regression'], 'cross_validation': {'n_folds': 3, 'scoring': 'roc_auc', 'n_jobs': 1}},\n",
    "        'evaluation': {'metrics': ['accuracy', 'precision', 'recall', 'f1_score'], 'visualization': {'generate_plots': False}},\n",
    "        'target_word_evaluation': {'enable': False, 'target_codes': ['E119']},\n",
    "        'output': {'embeddings_dir': 'outputs/embeddings', 'models_dir': 'outputs/models', 'metrics_dir': 'outputs/metrics', 'logs_dir': 'outputs/logs'},\n",
    "        'logging': {'level': 'INFO', 'console_level': 'INFO', 'format': '%(asctime)s - %(levelname)s - %(message)s', 'file': 'outputs/logs/pipeline.log'}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        embeddings_config_obj = PipelineConfig(**embeddings_config)\n",
    "        print(f\"   âœ… Embeddings configuration validated\")\n",
    "        \n",
    "        # Test loading embeddings\n",
    "        with open(train_emb_path, 'r') as f:\n",
    "            train_emb_data = json.load(f)\n",
    "        \n",
    "        with open(test_emb_path, 'r') as f:\n",
    "            test_emb_data = json.load(f)\n",
    "        \n",
    "        print(f\"   âœ… Embeddings loaded successfully\")\n",
    "        print(f\"   ğŸ“Š Train embeddings: {len(train_emb_data['embeddings'])} samples\")\n",
    "        print(f\"   ğŸ“Š Embedding dimension: {len(train_emb_data['embeddings'][0])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Configuration or loading failed: {e}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    try:\n",
    "        train_emb_path.unlink()\n",
    "        test_emb_path.unlink()\n",
    "        print(f\"   ğŸ§¹ Cleaned up test files\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Run alternative configuration tests\n",
    "test_separate_files_config()\n",
    "test_embeddings_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Performance Testing with Large Data\n",
    "Test data loading performance with larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance with larger datasets\n",
    "def test_large_data_performance():\n",
    "    \"\"\"Test data loading performance with larger datasets.\"\"\"\n",
    "    \n",
    "    print(f\"\\nâš¡ Testing data loading performance:\")\n",
    "    \n",
    "    dataset_sizes = [100, 500, 1000, 5000]\n",
    "    \n",
    "    for size in dataset_sizes:\n",
    "        print(f\"\\nğŸ“Š Testing with {size} samples:\")\n",
    "        \n",
    "        # Generate large dataset\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        large_data = {\n",
    "            'mcid': [f'CLAIM_{i:06d}' for i in range(size)],\n",
    "            'claims': ['E119 I10 N6320 K9289 |eoc| Z1239 M549 76642'] * size,\n",
    "            'label': [i % 2 for i in range(size)]  # Alternating 0, 1\n",
    "        }\n",
    "        \n",
    "        large_df = pd.DataFrame(large_data)\n",
    "        generation_time = time.time() - start_time\n",
    "        \n",
    "        # Save to file\n",
    "        large_file_path = mgpt_eval_path / \"examples\" / f\"test_large_{size}.csv\"\n",
    "        start_time = time.time()\n",
    "        large_df.to_csv(large_file_path, index=False)\n",
    "        save_time = time.time() - start_time\n",
    "        \n",
    "        # Test loading\n",
    "        start_time = time.time()\n",
    "        loaded_large_df = load_and_validate_data(str(large_file_path))\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        if loaded_large_df is not None:\n",
    "            # Test splitting\n",
    "            start_time = time.time()\n",
    "            train_large, test_large = test_train_test_split(loaded_large_df, 0.8, 42)\n",
    "            split_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"   âœ… Performance results:\")\n",
    "            print(f\"      Generation: {generation_time:.3f}s\")\n",
    "            print(f\"      Save: {save_time:.3f}s ({size/save_time:.0f} samples/sec)\")\n",
    "            print(f\"      Load: {load_time:.3f}s ({size/load_time:.0f} samples/sec)\")\n",
    "            print(f\"      Split: {split_time:.3f}s\")\n",
    "            print(f\"      Memory: ~{large_df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n",
    "        else:\n",
    "            print(f\"   âŒ Failed to load dataset of size {size}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        try:\n",
    "            large_file_path.unlink()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Brief pause between tests\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Run performance tests\n",
    "test_large_data_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Summary and Recommendations\n",
    "Summarize all test results and provide recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize data validation test results\n",
    "print(\"\\nğŸ“‹ Data Loading & Validation Test Summary:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Check what tests passed\n",
    "tests_passed = []\n",
    "tests_failed = []\n",
    "\n",
    "if config:\n",
    "    tests_passed.append(\"Configuration validation\")\nelse:\n",
    "    tests_failed.append(\"Configuration validation\")\n",
    "\n",
    "if loaded_df is not None:\n",
    "    tests_passed.append(\"Basic data loading\")\nelse:\n",
    "    tests_failed.append(\"Basic data loading\")\n",
    "\n",
    "if train_df is not None and test_df is not None:\n",
    "    tests_passed.append(\"Train/test splitting\")\nelse:\n",
    "    tests_failed.append(\"Train/test splitting\")\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nğŸ¯ Test Results:\")\n",
    "for test in tests_passed:\n",
    "    print(f\"   âœ… PASS: {test}\")\n",
    "\n",
    "for test in tests_failed:\n",
    "    print(f\"   âŒ FAIL: {test}\")\n",
    "\n",
    "success_rate = len(tests_passed) / (len(tests_passed) + len(tests_failed)) * 100\n",
    "print(f\"\\nğŸ“Š Overall Success Rate: {success_rate:.0f}%\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nğŸ’¡ Recommendations:\")\n",
    "\n",
    "if success_rate == 100:\n",
    "    print(f\"   ğŸ‰ All data validation tests passed!\")\n",
    "    print(f\"   âœ… Your data format is compatible with the pipeline\")\n",
    "    print(f\"   â¡ï¸  Next: Test API connectivity (if not done yet)\")\n",
    "    print(f\"   â¡ï¸  Then: Test embedding pipeline with this data\")\nelse:\n",
    "    print(f\"   ğŸ”§ Fix the failed tests before proceeding to pipeline testing\")\n",
    "    \n",
    "if loaded_df is not None:\n",
    "    label_dist = loaded_df['label'].value_counts(normalize=True)\n",
    "    if len(label_dist) < 2:\n",
    "        print(f\"   âš ï¸  Warning: Only one class in data - add samples from other class\")\n",
    "    elif label_dist.min() < 0.1:\n",
    "        print(f\"   âš ï¸  Warning: Imbalanced classes ({label_dist.to_dict()}) - consider rebalancing\")\n",
    "    else:\n",
    "        print(f\"   âœ… Good class balance: {label_dist.to_dict()}\")\n",
    "\n",
    "print(f\"\\nğŸ”§ Configuration for embedding pipeline:\")\n",
    "if config:\n",
    "    print(f\"   input:\")\n",
    "    print(f\"     dataset_path: \\\"{sample_data_path}\\\"\")\n",
    "    if hasattr(config.input, 'split_ratio'):\n",
    "        print(f\"     split_ratio: {config.input.split_ratio}\")\n",
    "    \n",
    "    print(f\"   data_processing:\")\n",
    "    print(f\"     random_seed: {config.data_processing.random_seed}\")\n",
    "    print(f\"     max_sequence_length: {config.data_processing.max_sequence_length}\")\n",
    "    print(f\"     output_format: \\\"{config.data_processing.output_format}\\\"\")\n",
    "\n",
    "print(f\"\\nğŸ“š Next steps:\")\n",
    "print(f\"   1. Ensure your actual data follows the same format as the test data\")\n",
    "print(f\"   2. Update the configuration with your actual data path\")\n",
    "print(f\"   3. Run test_03_Embedding_Pipeline_Debug.ipynb\")\n",
    "print(f\"   4. If issues persist, check the original CSV file manually\")\n",
    "\n",
    "# Cleanup sample data\n",
    "try:\n",
    "    sample_data_path.unlink()\n",
    "    print(f\"\\nğŸ§¹ Cleaned up sample test data\")\nexcept:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Debug Cell (Run if needed)\n",
    "Use this cell to test specific data scenarios or debug issues found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug cell - modify as needed for specific testing\n",
    "\n",
    "# Example: Load your actual data file\n",
    "# actual_data_path = \"/path/to/your/actual_data.csv\"\n",
    "# if os.path.exists(actual_data_path):\n",
    "#     print(f\"Testing actual data file: {actual_data_path}\")\n",
    "#     actual_df = load_and_validate_data(actual_data_path)\n",
    "#     if actual_df is not None:\n",
    "#         print(f\"Actual data shape: {actual_df.shape}\")\n",
    "#         print(f\"Actual label distribution: {actual_df['label'].value_counts()}\")\n",
    "\n",
    "# Example: Check specific data issues\n",
    "# if 'loaded_df' in locals() and loaded_df is not None:\n",
    "#     print(\"Checking for specific issues:\")\n",
    "#     print(f\"Claims with |eoc|: {loaded_df['claims'].str.contains('\\|eoc\\|').sum()}\")\n",
    "#     print(f\"Average claim length: {loaded_df['claims'].str.len().mean():.1f} characters\")\n",
    "#     print(f\"Unique codes found: {len(set(' '.join(loaded_df['claims']).split()))}\")\n",
    "\n",
    "# Example: Test custom splitting ratios\n",
    "# if 'loaded_df' in locals() and loaded_df is not None:\n",
    "#     for ratio in [0.6, 0.7, 0.8, 0.9]:\n",
    "#         print(f\"\\nTesting split ratio {ratio}:\")\n",
    "#         test_train_test_split(loaded_df, ratio, 42)\n",
    "\n",
    "print(\"ğŸ’¡ Use this cell to run custom data validation tests and debug specific issues.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}