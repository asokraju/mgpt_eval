{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Word Evaluator Debug\n",
    "\n",
    "This notebook debugs the target word evaluator using the actual TargetWordEvaluator class.\n",
    "Tests target word evaluation methods step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline modules\n",
    "import sys\n",
    "sys.path.append('/home/kosaraju/mgpt-serve/mgpt_eval')\n",
    "\n",
    "from models.config_models import PipelineConfig\n",
    "from evaluation.target_word_evaluator import TargetWordEvaluator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"/home/kosaraju/mgpt-serve/mgpt_eval/configs/examples/config_evaluation_only.yaml\"\n",
    "config = PipelineConfig.from_yaml(config_path)\n",
    "print(f\"Config loaded: {config.job.job_name}\")\n",
    "print(f\"Evaluation config: {config.evaluation}\")\n",
    "if hasattr(config.evaluation, 'target_word'):\n",
    "    print(f\"Target word config: {config.evaluation.target_word}\")\n",
    "    print(f\"Target codes: {config.evaluation.target_word.target_codes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize target word evaluator\n",
    "target_evaluator = TargetWordEvaluator(config)\n",
    "print(f\"Target word evaluator initialized\")\n",
    "print(f\"Evaluator methods: {[m for m in dir(target_evaluator) if not m.startswith('_') and callable(getattr(target_evaluator, m))]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single sample evaluation\n",
    "print(\"Testing single sample evaluation...\")\n",
    "\n",
    "test_claims = [\n",
    "    \"N6320 G0378 |eoc| Z91048 M1710\",\n",
    "    \"E119 A1234 |eoc| B5678 C9012\",\n",
    "    \"Z03818 D3456 |eoc| F7890 G1234\",\n",
    "    \"H5678 I9012 |eoc| J1234 K5678\",\n",
    "    \"L9012 M3456 |eoc| N6320 O7890\"\n",
    "]\n",
    "\n",
    "for i, claims in enumerate(test_claims[:3]):\n",
    "    print(f\"\\nSample {i+1}: {claims}\")\n",
    "    \n",
    "    try:\n",
    "        # Use the actual evaluator method\n",
    "        result = target_evaluator.evaluate_sample(claims)\n",
    "        print(f\"  ✓ Evaluation completed\")\n",
    "        print(f\"    Result type: {type(result)}\")\n",
    "        print(f\"    Result keys: {list(result.keys()) if isinstance(result, dict) else 'Not a dict'}\")\n",
    "        \n",
    "        if isinstance(result, dict):\n",
    "            for key, value in result.items():\n",
    "                print(f\"      {key}: {value}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batch evaluation\n",
    "print(\"Testing batch evaluation...\")\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'mcid': ['TW_001', 'TW_002', 'TW_003', 'TW_004', 'TW_005'],\n",
    "    'claims': test_claims,\n",
    "    'label': [1, 1, 1, 0, 1]\n",
    "})\n",
    "\n",
    "print(f\"Test data: {len(test_data)} samples\")\n",
    "\n",
    "try:\n",
    "    # Check if evaluator has batch evaluation method\n",
    "    if hasattr(target_evaluator, 'evaluate_batch'):\n",
    "        results = target_evaluator.evaluate_batch(test_data)\n",
    "        print(f\"✓ Batch evaluation completed\")\n",
    "        print(f\"  Results type: {type(results)}\")\n",
    "        print(f\"  Results length: {len(results) if hasattr(results, '__len__') else 'No length'}\")\n",
    "        \n",
    "    elif hasattr(target_evaluator, 'evaluate'):\n",
    "        results = target_evaluator.evaluate(test_data)\n",
    "        print(f\"✓ Evaluation completed\")\n",
    "        print(f\"  Results type: {type(results)}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No batch evaluation method found\")\n",
    "        # Try individual evaluations\n",
    "        results = []\n",
    "        for _, row in test_data.iterrows():\n",
    "            result = target_evaluator.evaluate_sample(row['claims'])\n",
    "            results.append(result)\n",
    "        print(f\"✓ Individual evaluations completed: {len(results)} results\")\n",
    "        \nexcept Exception as e:\n",
    "    print(f\"✗ Batch evaluation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test target code detection\n",
    "print(\"Testing target code detection...\")\n",
    "\n",
    "# Check if evaluator has target detection methods\n",
    "detection_methods = ['detect_target_codes', 'check_target_presence', 'find_targets']\n",
    "\n",
    "test_text = \"Generated text: E119 A1234 |eoc| Z03818 B5678 |eoc| C9012 D3456\"\n",
    "print(f\"Test text: {test_text}\")\n",
    "\n",
    "for method_name in detection_methods:\n",
    "    if hasattr(target_evaluator, method_name):\n",
    "        print(f\"Found method: {method_name}\")\n",
    "        try:\n",
    "            method = getattr(target_evaluator, method_name)\n",
    "            result = method(test_text)\n",
    "            print(f\"  ✓ {method_name} completed: {result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ {method_name} failed: {e}\")\n",
    "    else:\n",
    "        print(f\"Method not found: {method_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text generation (if available)\n",
    "print(\"Testing text generation...\")\n",
    "\n",
    "generation_methods = ['generate_text', 'call_api', 'generate']\n",
    "test_prompt = \"N6320 G0378 |eoc| Z91048 M1710\"\n",
    "\n",
    "for method_name in generation_methods:\n",
    "    if hasattr(target_evaluator, method_name):\n",
    "        print(f\"Found method: {method_name}\")\n",
    "        try:\n",
    "            method = getattr(target_evaluator, method_name)\n",
    "            # Try different parameter combinations\n",
    "            if method_name == 'generate_text':\n",
    "                result = method(test_prompt)\n",
    "            elif method_name == 'call_api':\n",
    "                result = method(test_prompt)\n",
    "            else:\n",
    "                result = method(test_prompt)\n",
    "            print(f\"  ✓ {method_name} completed: {result[:100] if isinstance(result, str) else result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ {method_name} failed: {e}\")\n",
    "    else:\n",
    "        print(f\"Method not found: {method_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug evaluator internals\n",
    "print(\"=== Target Word Evaluator Debug ===\")\n",
    "\n",
    "# Check evaluator attributes\n",
    "print(f\"Evaluator config: {hasattr(target_evaluator, 'config')}\")\n",
    "print(f\"Evaluator logger: {hasattr(target_evaluator, 'logger')}\")\n",
    "print(f\"Evaluator target codes: {hasattr(target_evaluator, 'target_codes')}\")\n",
    "print(f\"Evaluator API client: {hasattr(target_evaluator, 'api_client')}\")\n",
    "\n",
    "# Check target codes\n",
    "if hasattr(target_evaluator, 'target_codes'):\n",
    "    print(f\"\\nTarget codes: {target_evaluator.target_codes}\")\nelif hasattr(target_evaluator, 'config') and hasattr(target_evaluator.config, 'evaluation'):\n",
    "    if hasattr(target_evaluator.config.evaluation, 'target_word'):\n",
    "        print(f\"\\nTarget codes from config: {target_evaluator.config.evaluation.target_word.target_codes}\")\n",
    "\n",
    "# Check configuration parameters\n",
    "if hasattr(target_evaluator, 'config'):\n",
    "    config = target_evaluator.config\n",
    "    if hasattr(config, 'evaluation') and hasattr(config.evaluation, 'target_word'):\n",
    "        tw_config = config.evaluation.target_word\n",
    "        print(f\"\\nTarget word configuration:\")\n",
    "        print(f\"  Max tokens: {getattr(tw_config, 'max_tokens', 'Not set')}\")\n",
    "        print(f\"  Generation attempts: {getattr(tw_config, 'generation_attempts', 'Not set')}\")\n",
    "\n",
    "# Check all methods and attributes\n",
    "all_attrs = [attr for attr in dir(target_evaluator) if not attr.startswith('_')]\n",
    "methods = [attr for attr in all_attrs if callable(getattr(target_evaluator, attr))]\n",
    "properties = [attr for attr in all_attrs if not callable(getattr(target_evaluator, attr))]\n",
    "\n",
    "print(f\"\\nMethods: {methods}\")\n",
    "print(f\"Properties: {properties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation metrics calculation\n",
    "print(\"Testing evaluation metrics calculation...\")\n",
    "\n",
    "# Create test predictions and labels\n",
    "test_predictions = [1, 0, 1, 0, 1]\n",
    "test_labels = [1, 1, 1, 0, 1]\n",
    "\n",
    "metrics_methods = ['calculate_metrics', 'compute_metrics', 'get_metrics']\n",
    "\n",
    "for method_name in metrics_methods:\n",
    "    if hasattr(target_evaluator, method_name):\n",
    "        print(f\"Found method: {method_name}\")\n",
    "        try:\n",
    "            method = getattr(target_evaluator, method_name)\n",
    "            result = method(test_predictions, test_labels)\n",
    "            print(f\"  ✓ {method_name} completed: {result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ {method_name} failed: {e}\")\n",
    "    else:\n",
    "        print(f\"Method not found: {method_name}\")\n",
    "\n",
    "# Test if evaluator can process evaluation results\n",
    "if hasattr(target_evaluator, 'process_results'):\n",
    "    try:\n",
    "        mock_results = {\n",
    "            'predictions': test_predictions,\n",
    "            'labels': test_labels,\n",
    "            'mcids': ['TW_001', 'TW_002', 'TW_003', 'TW_004', 'TW_005']\n",
    "        }\n",
    "        processed = target_evaluator.process_results(mock_results)\n",
    "        print(f\"✓ Results processing completed: {processed}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Results processing failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelnel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}