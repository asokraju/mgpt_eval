{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Issues Debug\n",
    "\n",
    "This notebook debugs configuration issues using the actual PipelineConfig class.\n",
    "Tests configuration loading, validation, and usage in pipeline components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration modules\n",
    "import sys\n",
    "sys.path.append('/home/kosaraju/mgpt-serve/mgpt_eval')\n",
    "\n",
    "from models.config_models import PipelineConfig\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration file discovery\n",
    "print(\"Discovering configuration files...\")\n",
    "\n",
    "config_paths = [\n",
    "    \"/home/kosaraju/mgpt-serve/mgpt_eval/configs/pipeline_config.yaml\",\n",
    "    \"/home/kosaraju/mgpt-serve/mgpt_eval/configs/examples/config_embeddings_only.yaml\",\n",
    "    \"/home/kosaraju/mgpt-serve/mgpt_eval/configs/examples/config_evaluation_only.yaml\",\n",
    "    \"/home/kosaraju/mgpt-serve/mgpt_eval/configs/examples/config_training_from_embeddings.yaml\",\n",
    "    \"/home/kosaraju/mgpt-serve/mgpt_eval/configs/examples/custom_medical_config.yaml\"\n",
    "]\n",
    "\n",
    "existing_configs = []\n",
    "for config_path in config_paths:\n",
    "    if os.path.exists(config_path):\n",
    "        existing_configs.append(config_path)\n",
    "        print(f\"✓ Found: {os.path.basename(config_path)}\")\n",
    "    else:\n",
    "        print(f\"✗ Missing: {os.path.basename(config_path)}\")\n",
    "\n",
    "print(f\"\\nTotal configs found: {len(existing_configs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test individual configuration loading\n",
    "print(\"Testing individual configuration loading...\")\n",
    "\n",
    "for config_path in existing_configs:\n",
    "    config_name = os.path.basename(config_path)\n",
    "    print(f\"\\n--- Testing {config_name} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Test YAML loading first\n",
    "        with open(config_path, 'r') as f:\n",
    "            yaml_content = yaml.safe_load(f)\n",
    "        print(f\"  ✓ YAML parsing successful\")\n",
    "        print(f\"    Top-level keys: {list(yaml_content.keys())}\")\n",
    "        \n",
    "        # Test PipelineConfig loading\n",
    "        config = PipelineConfig.from_yaml(config_path)\n",
    "        print(f\"  ✓ PipelineConfig loading successful\")\n",
    "        print(f\"    Job name: {config.job.job_name}\")\n",
    "        \n",
    "        # Check configuration components\n",
    "        components = []\n",
    "        if hasattr(config, 'job') and config.job:\n",
    "            components.append('job')\n",
    "        if hasattr(config, 'input') and config.input:\n",
    "            components.append('input')\n",
    "        if hasattr(config, 'api') and config.api:\n",
    "            components.append('api')\n",
    "        if hasattr(config, 'embedding') and config.embedding:\n",
    "            components.append('embedding')\n",
    "        if hasattr(config, 'classification') and config.classification:\n",
    "            components.append('classification')\n",
    "        if hasattr(config, 'evaluation') and config.evaluation:\n",
    "            components.append('evaluation')\n",
    "        \n",
    "        print(f\"    Components: {components}\")\n",
    "        \n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"  ✗ YAML parsing failed: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ PipelineConfig loading failed: {e}\")\n",
    "        # Try to get more details\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration validation issues\n",
    "print(\"Testing configuration validation...\")\n",
    "\n",
    "# Test with main pipeline config\n",
    "main_config_path = \"/home/kosaraju/mgpt-serve/mgpt_eval/configs/pipeline_config.yaml\"\n",
    "\n",
    "if os.path.exists(main_config_path):\n",
    "    try:\n",
    "        config = PipelineConfig.from_yaml(main_config_path)\n",
    "        print(f\"✓ Main config loaded successfully\")\n",
    "        \n",
    "        # Validate specific fields\n",
    "        print(f\"\\nValidation checks:\")\n",
    "        \n",
    "        # Job configuration\n",
    "        if hasattr(config, 'job') and config.job:\n",
    "            print(f\"  ✓ Job config present\")\n",
    "            print(f\"    Job name: {getattr(config.job, 'job_name', 'Missing')}\")\n",
    "            print(f\"    Output dir: {getattr(config.job, 'output_base_dir', 'Missing')}\")\n",
    "            print(f\"    Split ratio: {getattr(config.job, 'split_ratio', 'Missing')}\")\n",
    "        else:\n",
    "            print(f\"  ✗ Job config missing\")\n",
    "        \n",
    "        # Input configuration\n",
    "        if hasattr(config, 'input') and config.input:\n",
    "            print(f\"  ✓ Input config present\")\n",
    "            \n",
    "            # Check input data specification\n",
    "            dataset_path = getattr(config.input, 'dataset_path', None)\n",
    "            train_path = getattr(config.input, 'train_dataset_path', None)\n",
    "            test_path = getattr(config.input, 'test_dataset_path', None)\n",
    "            \n",
    "            print(f\"    Dataset path: {dataset_path}\")\n",
    "            print(f\"    Train path: {train_path}\")\n",
    "            print(f\"    Test path: {test_path}\")\n",
    "            \n",
    "            # Check for conflicting specifications\n",
    "            has_single = bool(dataset_path)\n",
    "            has_split = bool(train_path and test_path)\n",
    "            \n",
    "            if has_single and has_split:\n",
    "                print(f\"    ⚠️  Both single and split datasets specified\")\n",
    "            elif not (has_single or has_split):\n",
    "                print(f\"    ✗ No valid dataset specification\")\n",
    "            else:\n",
    "                print(f\"    ✓ Valid dataset specification\")\n",
    "        else:\n",
    "            print(f\"  ✗ Input config missing\")\n",
    "        \n",
    "        # API configuration\n",
    "        if hasattr(config, 'api') and config.api:\n",
    "            print(f\"  ✓ API config present\")\n",
    "            print(f\"    Host: {getattr(config.api, 'host', 'Missing')}\")\n",
    "            print(f\"    Port: {getattr(config.api, 'port', 'Missing')}\")\n",
    "        else:\n",
    "            print(f\"  ✗ API config missing\")\n",
    "        \n",
    "        # Evaluation configuration\n",
    "        if hasattr(config, 'evaluation') and config.evaluation:\n",
    "            print(f\"  ✓ Evaluation config present\")\n",
    "            \n",
    "            if hasattr(config.evaluation, 'target_word') and config.evaluation.target_word:\n",
    "                target_codes = getattr(config.evaluation.target_word, 'target_codes', [])\n",
    "                print(f\"    Target codes: {target_codes}\")\n",
    "                \n",
    "                if not target_codes:\n",
    "                    print(f\"    ⚠️  Target codes list is empty\")\n",
    "                elif len(target_codes) == 0:\n",
    "                    print(f\"    ⚠️  Target codes list has no elements\")\n",
    "                else:\n",
    "                    print(f\"    ✓ Target codes properly configured ({len(target_codes)} codes)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Main config validation failed: {e}\")\nelse:\n",
    "    print(f\"Main config file not found: {main_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration field access patterns\n",
    "print(\"Testing configuration field access patterns...\")\n",
    "\n",
    "if existing_configs:\n",
    "    test_config_path = existing_configs[0]\n",
    "    \n",
    "    try:\n",
    "        config = PipelineConfig.from_yaml(test_config_path)\n",
    "        print(f\"Testing with: {os.path.basename(test_config_path)}\")\n",
    "        \n",
    "        # Test different access patterns\n",
    "        print(f\"\\nAccess pattern tests:\")\n",
    "        \n",
    "        # Direct attribute access\n",
    "        try:\n",
    "            job_name = config.job.job_name\n",
    "            print(f\"  ✓ Direct access: config.job.job_name = {job_name}\")\n",
    "        except AttributeError as e:\n",
    "            print(f\"  ✗ Direct access failed: {e}\")\n",
    "        \n",
    "        # Getattr with default\n",
    "        try:\n",
    "            split_ratio = getattr(config.job, 'split_ratio', 'Not found')\n",
    "            print(f\"  ✓ Getattr access: split_ratio = {split_ratio}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Getattr access failed: {e}\")\n",
    "        \n",
    "        # Hasattr check\n",
    "        try:\n",
    "            has_embedding = hasattr(config, 'embedding')\n",
    "            print(f\"  ✓ Hasattr check: has embedding = {has_embedding}\")\n",
    "            \n",
    "            if has_embedding:\n",
    "                embedding_config = config.embedding\n",
    "                print(f\"    Embedding config: {embedding_config}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Hasattr check failed: {e}\")\n",
    "        \n",
    "        # Test nested access\n",
    "        try:\n",
    "            if hasattr(config, 'evaluation') and config.evaluation:\n",
    "                if hasattr(config.evaluation, 'target_word') and config.evaluation.target_word:\n",
    "                    target_codes = config.evaluation.target_word.target_codes\n",
    "                    print(f\"  ✓ Nested access: target_codes = {target_codes}\")\n",
    "                else:\n",
    "                    print(f\"  - No target_word config\")\n",
    "            else:\n",
    "                print(f\"  - No evaluation config\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Nested access failed: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Configuration loading failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration consistency across files\n",
    "print(\"Testing configuration consistency...\")\n",
    "\n",
    "configs = {}\n",
    "for config_path in existing_configs:\n",
    "    try:\n",
    "        config = PipelineConfig.from_yaml(config_path)\n",
    "        config_name = os.path.basename(config_path)\n",
    "        configs[config_name] = config\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {os.path.basename(config_path)}: {e}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(configs)} configurations for comparison\")\n",
    "\n",
    "if len(configs) > 1:\n",
    "    # Compare API configurations\n",
    "    api_hosts = {}\n",
    "    api_ports = {}\n",
    "    \n",
    "    for name, config in configs.items():\n",
    "        if hasattr(config, 'api') and config.api:\n",
    "            api_hosts[name] = getattr(config.api, 'host', None)\n",
    "            api_ports[name] = getattr(config.api, 'port', None)\n",
    "    \n",
    "    print(f\"\\nAPI Configuration Comparison:\")\n",
    "    print(f\"  Hosts: {api_hosts}\")\n",
    "    print(f\"  Ports: {api_ports}\")\n",
    "    \n",
    "    # Check for inconsistencies\n",
    "    unique_hosts = set(api_hosts.values())\n",
    "    unique_ports = set(api_ports.values())\n",
    "    \n",
    "    if len(unique_hosts) > 1:\n",
    "        print(f\"  ⚠️  Inconsistent API hosts: {unique_hosts}\")\n",
    "    else:\n",
    "        print(f\"  ✓ Consistent API hosts\")\n",
    "    \n",
    "    if len(unique_ports) > 1:\n",
    "        print(f\"  ⚠️  Inconsistent API ports: {unique_ports}\")\n",
    "    else:\n",
    "        print(f\"  ✓ Consistent API ports\")\n",
    "    \n",
    "    # Compare target codes (if available)\n",
    "    target_codes_configs = {}\n",
    "    for name, config in configs.items():\n",
    "        if (hasattr(config, 'evaluation') and config.evaluation and \n",
    "            hasattr(config.evaluation, 'target_word') and config.evaluation.target_word):\n",
    "            target_codes_configs[name] = config.evaluation.target_word.target_codes\n",
    "    \n",
    "    if target_codes_configs:\n",
    "        print(f\"\\nTarget Codes Comparison:\")\n",
    "        for name, codes in target_codes_configs.items():\n",
    "            print(f\"  {name}: {codes}\")\nelse:\n",
    "    print(\"Not enough configurations to compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration with pipeline components\n",
    "print(\"Testing configuration with pipeline components...\")\n",
    "\n",
    "if configs:\n",
    "    # Test with first available config\n",
    "    config_name, config = next(iter(configs.items()))\n",
    "    print(f\"Testing with: {config_name}\")\n",
    "    \n",
    "    # Test embedding pipeline initialization\n",
    "    try:\n",
    "        from pipelines.embedding_pipeline import EmbeddingPipeline\n",
    "        embedding_pipeline = EmbeddingPipeline(config)\n",
    "        print(f\"  ✓ EmbeddingPipeline accepts config\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ EmbeddingPipeline config error: {e}\")\n",
    "    \n",
    "    # Test classification pipeline initialization\n",
    "    try:\n",
    "        from pipelines.classification_pipeline import ClassificationPipeline\n",
    "        classification_pipeline = ClassificationPipeline(config)\n",
    "        print(f\"  ✓ ClassificationPipeline accepts config\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ ClassificationPipeline config error: {e}\")\n",
    "    \n",
    "    # Test target word evaluator initialization\n",
    "    try:\n",
    "        from evaluation.target_word_evaluator import TargetWordEvaluator\n",
    "        target_evaluator = TargetWordEvaluator(config)\n",
    "        print(f\"  ✓ TargetWordEvaluator accepts config\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ TargetWordEvaluator config error: {e}\")\n",
    "    \n",
    "    # Test evaluation pipeline initialization\n",
    "    try:\n",
    "        from pipelines.evaluation_pipeline import EvaluationPipeline\n",
    "        evaluation_pipeline = EvaluationPipeline(config)\n",
    "        print(f\"  ✓ EvaluationPipeline accepts config\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ EvaluationPipeline config error: {e}\")\n",
    "    \n",
    "    # Test end-to-end pipeline initialization\n",
    "    try:\n",
    "        from pipelines.end_to_end_pipeline import EndToEndPipeline\n",
    "        e2e_pipeline = EndToEndPipeline(config)\n",
    "        print(f\"  ✓ EndToEndPipeline accepts config\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ EndToEndPipeline config error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug split ratio issue mentioned in the problem\n",
    "print(\"=== Split Ratio Issue Debug ===\")\n",
    "\n",
    "print(\"Checking split_ratio placement in configurations...\")\n",
    "\n",
    "for config_name, config in configs.items():\n",
    "    print(f\"\\n{config_name}:\")\n",
    "    \n",
    "    # Check if split_ratio is in job config\n",
    "    if hasattr(config, 'job') and config.job:\n",
    "        if hasattr(config.job, 'split_ratio'):\n",
    "            split_ratio = config.job.split_ratio\n",
    "            print(f\"  ✓ split_ratio in job config: {split_ratio}\")\n",
    "        else:\n",
    "            print(f\"  ✗ split_ratio NOT in job config\")\n",
    "    \n",
    "    # Check if split_ratio is in input config\n",
    "    if hasattr(config, 'input') and config.input:\n",
    "        if hasattr(config.input, 'split_ratio'):\n",
    "            split_ratio = config.input.split_ratio\n",
    "            print(f\"  ! split_ratio found in input config: {split_ratio}\")\n",
    "            print(f\"    This might be the issue - should it be in job config instead?\")\n",
    "        else:\n",
    "            print(f\"  - split_ratio not in input config\")\n",
    "    \n",
    "    # Check input data specification\n",
    "    if hasattr(config, 'input') and config.input:\n",
    "        dataset_path = getattr(config.input, 'dataset_path', None)\n",
    "        train_path = getattr(config.input, 'train_dataset_path', None)\n",
    "        test_path = getattr(config.input, 'test_dataset_path', None)\n",
    "        \n",
    "        print(f\"  Input data specification:\")\n",
    "        print(f\"    Single dataset: {bool(dataset_path)}\")\n",
    "        print(f\"    Split datasets: {bool(train_path and test_path)}\")\n",
    "        \n",
    "        # Logic check\n",
    "        if dataset_path and (train_path or test_path):\n",
    "            print(f\"    ⚠️  CONFLICT: Both single and split datasets specified\")\n",
    "        elif dataset_path:\n",
    "            print(f\"    ✓ Single dataset mode - split_ratio should be used\")\n",
    "            if not (hasattr(config.job, 'split_ratio') or hasattr(config.input, 'split_ratio')):\n",
    "                print(f\"    ✗ ERROR: Single dataset mode but no split_ratio found\")\n",
    "        elif train_path and test_path:\n",
    "            print(f\"    ✓ Split dataset mode - split_ratio should NOT be used\")\n",
    "            if hasattr(config.job, 'split_ratio') or hasattr(config.input, 'split_ratio'):\n",
    "                print(f\"    ⚠️  WARNING: Split dataset mode but split_ratio is specified\")\n",
    "        else:\n",
    "            print(f\"    ✗ ERROR: No valid dataset specification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}