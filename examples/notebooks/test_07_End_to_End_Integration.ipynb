{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Integration Testing\n",
    "\n",
    "This notebook tests the complete MGPT-eval pipeline from start to finish.\n",
    "Use this to debug full workflow integration and identify pipeline issues.\n",
    "\n",
    "## Overview\n",
    "- Test complete pipeline workflows\n",
    "- Debug integration between components\n",
    "- Validate data flow through entire system\n",
    "- Test different pipeline configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('/home/kosaraju/mgpt-serve/mgpt_eval')\n",
    "\n",
    "# Import all pipeline modules\n",
    "from pipelines.end_to_end_pipeline import EndToEndPipeline\n",
    "from pipelines.embedding_pipeline import EmbeddingPipeline\n",
    "from pipelines.classification_pipeline import ClassificationPipeline\n",
    "from pipelines.evaluation_pipeline import EvaluationPipeline\n",
    "from evaluation.target_word_evaluator import TargetWordEvaluator\n",
    "from models.config_models import PipelineConfig\n",
    "from models.data_models import DataSample, DataBatch\n",
    "from utils.logging_utils import setup_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Testing\n",
    "Test different pipeline configurations for integration workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple configuration scenarios\n",
    "def test_pipeline_configurations():\n",
    "    \"\"\"Test different pipeline configuration scenarios\"\"\"\n",
    "    print(\"Testing pipeline configurations...\")\n",
    "    \n",
    "    config_scenarios = [\n",
    "        {\n",
    "            'name': 'Embeddings Only',\n",
    "            'path': '/home/kosaraju/mgpt-serve/mgpt_eval/configs/examples/config_embeddings_only.yaml',\n",
    "            'expected_components': ['embedding']\n",
    "        },\n",
    "        {\n",
    "            'name': 'Evaluation Only',\n",
    "            'path': '/home/kosaraju/mgpt-serve/mgpt_eval/configs/examples/config_evaluation_only.yaml',\n",
    "            'expected_components': ['evaluation']\n",
    "        },\n",
    "        {\n",
    "            'name': 'Training from Embeddings',\n",
    "            'path': '/home/kosaraju/mgpt-serve/mgpt_eval/configs/examples/config_training_from_embeddings.yaml',\n",
    "            'expected_components': ['classification']\n",
    "        },\n",
    "        {\n",
    "            'name': 'Full Pipeline',\n",
    "            'path': '/home/kosaraju/mgpt-serve/mgpt_eval/configs/pipeline_config.yaml',\n",
    "            'expected_components': ['embedding', 'classification', 'evaluation']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    valid_configs = {}\n",
    "    \n",
    "    for scenario in config_scenarios:\n",
    "        print(f\"\\n--- {scenario['name']} ---\")\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(scenario['path']):\n",
    "                config = PipelineConfig.from_yaml(scenario['path'])\n",
    "                \n",
    "                # Validate configuration structure\n",
    "                validation_results = validate_config_structure(config, scenario['expected_components'])\n",
    "                \n",
    "                if validation_results['valid']:\n",
    "                    valid_configs[scenario['name']] = {\n",
    "                        'config': config,\n",
    "                        'path': scenario['path'],\n",
    "                        'components': validation_results['found_components']\n",
    "                    }\n",
    "                    print(f\"✓ Configuration valid\")\n",
    "                    print(f\"  Components: {validation_results['found_components']}\")\n",
    "                else:\n",
    "                    print(f\"✗ Configuration invalid: {validation_results['issues']}\")\n",
    "            else:\n",
    "                print(f\"✗ Configuration file not found: {scenario['path']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to load configuration: {e}\")\n",
    "    \n",
    "    return valid_configs\n",
    "\n",
    "def validate_config_structure(config: PipelineConfig, expected_components: List[str]) -> Dict:\n",
    "    \"\"\"Validate configuration structure for expected components\"\"\"\n",
    "    found_components = []\n",
    "    issues = []\n",
    "    \n",
    "    # Check basic structure\n",
    "    if not hasattr(config, 'job') or not config.job:\n",
    "        issues.append('Missing job configuration')\n",
    "    \n",
    "    if not hasattr(config, 'input') or not config.input:\n",
    "        issues.append('Missing input configuration')\n",
    "    \n",
    "    if not hasattr(config, 'api') or not config.api:\n",
    "        issues.append('Missing API configuration')\n",
    "    \n",
    "    # Check component-specific configurations\n",
    "    if 'embedding' in expected_components:\n",
    "        if hasattr(config, 'embedding') and config.embedding:\n",
    "            found_components.append('embedding')\n",
    "        else:\n",
    "            issues.append('Expected embedding configuration not found')\n",
    "    \n",
    "    if 'classification' in expected_components:\n",
    "        if hasattr(config, 'classification') and config.classification:\n",
    "            found_components.append('classification')\n",
    "        else:\n",
    "            issues.append('Expected classification configuration not found')\n",
    "    \n",
    "    if 'evaluation' in expected_components:\n",
    "        if hasattr(config, 'evaluation') and config.evaluation:\n",
    "            found_components.append('evaluation')\n",
    "        else:\n",
    "            issues.append('Expected evaluation configuration not found')\n",
    "    \n",
    "    return {\n",
    "        'valid': len(issues) == 0,\n",
    "        'found_components': found_components,\n",
    "        'issues': issues\n",
    "    }\n",
    "\n",
    "# Test configurations\n",
    "pipeline_configs = test_pipeline_configurations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create minimal test configuration for integration testing\n",
    "def create_integration_test_config() -> PipelineConfig:\n",
    "    \"\"\"Create a minimal configuration for integration testing\"\"\"\n",
    "    print(\"Creating integration test configuration...\")\n",
    "    \n",
    "    from models.config_models import (\n",
    "        JobConfig, InputConfig, APIConfig, EmbeddingConfig,\n",
    "        ClassificationConfig, EvaluationConfig, TargetWordConfig\n",
    "    )\n",
    "    \n",
    "    # Create test configuration with all components\n",
    "    job_config = JobConfig(\n",
    "        job_name=\"integration_test\",\n",
    "        output_base_dir=\"outputs/integration_test\",\n",
    "        split_ratio=0.8,\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    input_config = InputConfig(\n",
    "        dataset_path=\"test_integration_data.csv\"\n",
    "    )\n",
    "    \n",
    "    api_config = APIConfig(\n",
    "        host=\"localhost\",\n",
    "        port=8000\n",
    "    )\n",
    "    \n",
    "    embedding_config = EmbeddingConfig(\n",
    "        batch_size=32,\n",
    "        save_embeddings=True,\n",
    "        embeddings_file=\"test_embeddings.pkl\"\n",
    "    )\n",
    "    \n",
    "    classification_config = ClassificationConfig(\n",
    "        models=[\"logistic_regression\", \"svm\"],\n",
    "        cross_validation_folds=3,\n",
    "        hyperparameter_search=True\n",
    "    )\n",
    "    \n",
    "    target_word_config = TargetWordConfig(\n",
    "        target_codes=[\"E119\", \"Z03818\", \"N6320\"],\n",
    "        max_tokens=50,\n",
    "        generation_attempts=3\n",
    "    )\n",
    "    \n",
    "    evaluation_config = EvaluationConfig(\n",
    "        target_word=target_word_config,\n",
    "        classification=classification_config\n",
    "    )\n",
    "    \n",
    "    config = PipelineConfig(\n",
    "        job=job_config,\n",
    "        input=input_config,\n",
    "        api=api_config,\n",
    "        embedding=embedding_config,\n",
    "        classification=classification_config,\n",
    "        evaluation=evaluation_config\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Integration test configuration created\")\n",
    "    return config\n",
    "\n",
    "# Create test configuration\n",
    "if not pipeline_configs or len(pipeline_configs) == 0:\n",
    "    test_config = create_integration_test_config()\n",
    "else:\n",
    "    # Use first available configuration\n",
    "    config_name = list(pipeline_configs.keys())[0]\n",
    "    test_config = pipeline_configs[config_name]['config']\n",
    "    print(f\"Using configuration: {config_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Data Generation\n",
    "Create comprehensive test dataset for end-to-end testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive integration test dataset\n",
    "def create_integration_test_dataset(size: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"Create comprehensive test dataset for integration testing\"\"\"\n",
    "    print(f\"Creating integration test dataset ({size} samples)...\")\n",
    "    \n",
    "    # Define code pools\n",
    "    target_codes = [\"E119\", \"Z03818\", \"N6320\", \"M1710\"]\n",
    "    common_codes = [\n",
    "        \"G0378\", \"Z91048\", \"O0903\", \"K9289\", \"N6322\", \"76642\", \n",
    "        \"Z09\", \"Z1239\", \"O9989\", \"U0003\", \"A1234\", \"B5678\",\n",
    "        \"C9012\", \"D3456\", \"F7890\", \"H5678\", \"I9012\", \"J1234\"\n",
    "    ]\n",
    "    \n",
    "    data = {\n",
    "        'mcid': [],\n",
    "        'claims': [],\n",
    "        'label': []\n",
    "    }\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for i in range(size):\n",
    "        mcid = f\"INT_TEST_{i:04d}\"\n",
    "        \n",
    "        # 60% positive samples (contain target codes)\n",
    "        has_target = np.random.random() < 0.6\n",
    "        \n",
    "        codes = []\n",
    "        \n",
    "        if has_target:\n",
    "            # Add 1-2 target codes\n",
    "            num_targets = np.random.choice([1, 2], p=[0.8, 0.2])\n",
    "            selected_targets = np.random.choice(target_codes, num_targets, replace=False)\n",
    "            codes.extend(selected_targets)\n",
    "        \n",
    "        # Add 4-8 common codes\n",
    "        num_common = np.random.randint(4, 9)\n",
    "        selected_common = np.random.choice(common_codes, num_common, replace=True)\n",
    "        codes.extend(selected_common)\n",
    "        \n",
    "        # Shuffle codes and create claims string\n",
    "        np.random.shuffle(codes)\n",
    "        \n",
    "        # Split into segments with |eoc| separator\n",
    "        split_point = len(codes) // 2\n",
    "        segment1 = ' '.join(codes[:split_point])\n",
    "        segment2 = ' '.join(codes[split_point:])\n",
    "        claims = f\"{segment1} |eoc| {segment2}\"\n",
    "        \n",
    "        data['mcid'].append(mcid)\n",
    "        data['claims'].append(claims)\n",
    "        data['label'].append(1 if has_target else 0)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(f\"✓ Created dataset with {len(df)} samples\")\n",
    "    print(f\"  Positive samples: {sum(df['label'])} ({sum(df['label'])/len(df):.1%})\")\n",
    "    print(f\"  Negative samples: {len(df) - sum(df['label'])} ({(len(df) - sum(df['label']))/len(df):.1%})\")\n",
    "    \n",
    "    # Save to temporary file\n",
    "    temp_file = \"/tmp/integration_test_data.csv\"\n",
    "    df.to_csv(temp_file, index=False)\n",
    "    print(f\"  Saved to: {temp_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create test dataset\n",
    "integration_dataset = create_integration_test_dataset(size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Full Pipeline Integration Test\n",
    "Test complete end-to-end pipeline workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete end-to-end pipeline\n",
    "def test_end_to_end_pipeline(config: PipelineConfig, test_data: pd.DataFrame, use_mock: bool = True):\n",
    "    \"\"\"Test complete end-to-end pipeline integration\"\"\"\n",
    "    print(\"Testing end-to-end pipeline integration...\")\n",
    "    \n",
    "    pipeline_results = {\n",
    "        'stages': {},\n",
    "        'timing': {},\n",
    "        'success': False,\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: Data Loading and Splitting\n",
    "        print(\"\\n=== Stage 1: Data Loading and Splitting ===\")\n",
    "        stage1_start = time.time()\n",
    "        \n",
    "        # Split data into train/test\n",
    "        split_ratio = config.job.split_ratio if hasattr(config.job, 'split_ratio') else 0.8\n",
    "        split_idx = int(len(test_data) * split_ratio)\n",
    "        \n",
    "        train_data = test_data.iloc[:split_idx].copy()\n",
    "        test_data_split = test_data.iloc[split_idx:].copy()\n",
    "        \n",
    "        print(f\"✓ Data split: {len(train_data)} train, {len(test_data_split)} test\")\n",
    "        \n",
    "        pipeline_results['stages']['data_loading'] = {\n",
    "            'train_samples': len(train_data),\n",
    "            'test_samples': len(test_data_split),\n",
    "            'split_ratio': split_ratio\n",
    "        }\n",
    "        pipeline_results['timing']['data_loading'] = time.time() - stage1_start\n",
    "        \n",
    "        # Stage 2: Embedding Generation\n",
    "        print(\"\\n=== Stage 2: Embedding Generation ===\")\n",
    "        stage2_start = time.time()\n",
    "        \n",
    "        if use_mock:\n",
    "            print(\"Using mock embedding generation...\")\n",
    "            \n",
    "            # Generate mock embeddings\n",
    "            embedding_dim = 768\n",
    "            train_embeddings = np.random.randn(len(train_data), embedding_dim)\n",
    "            test_embeddings = np.random.randn(len(test_data_split), embedding_dim)\n",
    "            \n",
    "            print(f\"✓ Generated embeddings: {train_embeddings.shape}, {test_embeddings.shape}\")\n",
    "            \n",
    "            # Mock saving embeddings\n",
    "            embeddings_file = f\"outputs/mock_embeddings_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "            os.makedirs(os.path.dirname(embeddings_file), exist_ok=True)\n",
    "            \n",
    "            with open(embeddings_file, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'train_embeddings': train_embeddings,\n",
    "                    'test_embeddings': test_embeddings,\n",
    "                    'train_labels': train_data['label'].values,\n",
    "                    'test_labels': test_data_split['label'].values\n",
    "                }, f)\n",
    "            \n",
    "            print(f\"✓ Embeddings saved to: {embeddings_file}\")\n",
    "        else:\n",
    "            # Use actual embedding pipeline\n",
    "            embedding_pipeline = EmbeddingPipeline(config)\n",
    "            train_embeddings = embedding_pipeline.generate_embeddings(train_data)\n",
    "            test_embeddings = embedding_pipeline.generate_embeddings(test_data_split)\n",
    "        \n",
    "        pipeline_results['stages']['embedding'] = {\n",
    "            'train_embedding_shape': train_embeddings.shape,\n",
    "            'test_embedding_shape': test_embeddings.shape,\n",
    "            'embedding_file': embeddings_file if use_mock else 'actual_pipeline'\n",
    "        }\n",
    "        pipeline_results['timing']['embedding'] = time.time() - stage2_start\n",
    "        \n",
    "        # Stage 3: Classification Training\n",
    "        print(\"\\n=== Stage 3: Classification Training ===\")\n",
    "        stage3_start = time.time()\n",
    "        \n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "        \n",
    "        # Train multiple classifiers\n",
    "        classifiers = {\n",
    "            'logistic_regression': LogisticRegression(random_state=42),\n",
    "            'svm': SVC(random_state=42, probability=True),\n",
    "            'random_forest': RandomForestClassifier(random_state=42, n_estimators=50)\n",
    "        }\n",
    "        \n",
    "        trained_models = {}\n",
    "        classification_results = {}\n",
    "        \n",
    "        for name, classifier in classifiers.items():\n",
    "            print(f\"  Training {name}...\")\n",
    "            \n",
    "            # Train classifier\n",
    "            classifier.fit(train_embeddings, train_data['label'])\n",
    "            trained_models[name] = classifier\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            predictions = classifier.predict(test_embeddings)\n",
    "            probabilities = classifier.predict_proba(test_embeddings)[:, 1]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(test_data_split['label'], predictions)\n",
    "            precision = precision_score(test_data_split['label'], predictions, zero_division=0)\n",
    "            recall = recall_score(test_data_split['label'], predictions, zero_division=0)\n",
    "            f1 = f1_score(test_data_split['label'], predictions, zero_division=0)\n",
    "            \n",
    "            classification_results[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'predictions': predictions.tolist(),\n",
    "                'probabilities': probabilities.tolist()\n",
    "            }\n",
    "            \n",
    "            print(f\"    Accuracy: {accuracy:.3f}, F1: {f1:.3f}\")\n",
    "        \n",
    "        pipeline_results['stages']['classification'] = classification_results\n",
    "        pipeline_results['timing']['classification'] = time.time() - stage3_start\n",
    "        \n",
    "        # Stage 4: Target Word Evaluation\n",
    "        print(\"\\n=== Stage 4: Target Word Evaluation ===\")\n",
    "        stage4_start = time.time()\n",
    "        \n",
    "        target_word_results = []\n",
    "        target_codes = config.evaluation.target_word.target_codes if hasattr(config, 'evaluation') and config.evaluation and hasattr(config.evaluation, 'target_word') else [\"E119\", \"Z03818\", \"N6320\"]\n",
    "        \n",
    "        # Test subset for target word evaluation (expensive operation)\n",
    "        test_subset = test_data_split.head(10)\n",
    "        \n",
    "        for idx, row in test_subset.iterrows():\n",
    "            # Mock target word evaluation\n",
    "            claims_tokens = row['claims'].split()\n",
    "            found_targets = []\n",
    "            \n",
    "            for token in claims_tokens:\n",
    "                clean_token = token.strip('|eoc|').upper()\n",
    "                if clean_token in [code.upper() for code in target_codes]:\n",
    "                    found_targets.append(clean_token)\n",
    "            \n",
    "            # Simulate generation attempts\n",
    "            mock_generations = [\n",
    "                f\"GEN1_{idx} GEN2_{idx} |eoc| GEN3_{idx}\",\n",
    "                f\"E119 GEN4_{idx} |eoc| GEN5_{idx}\",\n",
    "                f\"GEN6_{idx} Z03818 |eoc| GEN7_{idx}\"\n",
    "            ]\n",
    "            \n",
    "            generated_targets = []\n",
    "            for i, gen in enumerate(mock_generations):\n",
    "                gen_tokens = gen.split()\n",
    "                for token in gen_tokens:\n",
    "                    clean_token = token.strip('|eoc|').upper()\n",
    "                    if clean_token in [code.upper() for code in target_codes]:\n",
    "                        generated_targets.append({\n",
    "                            'attempt': i + 1,\n",
    "                            'code': clean_token,\n",
    "                            'generation': gen\n",
    "                        })\n",
    "            \n",
    "            prediction = 1 if generated_targets else 0\n",
    "            \n",
    "            target_word_results.append({\n",
    "                'mcid': row['mcid'],\n",
    "                'true_label': row['label'],\n",
    "                'prediction': prediction,\n",
    "                'found_in_claims': found_targets,\n",
    "                'generated_targets': generated_targets,\n",
    "                'correct': prediction == row['label']\n",
    "            })\n",
    "        \n",
    "        # Calculate target word metrics\n",
    "        target_predictions = [r['prediction'] for r in target_word_results]\n",
    "        target_true_labels = [r['true_label'] for r in target_word_results]\n",
    "        \n",
    "        target_accuracy = accuracy_score(target_true_labels, target_predictions)\n",
    "        target_precision = precision_score(target_true_labels, target_predictions, zero_division=0)\n",
    "        target_recall = recall_score(target_true_labels, target_predictions, zero_division=0)\n",
    "        target_f1 = f1_score(target_true_labels, target_predictions, zero_division=0)\n",
    "        \n",
    "        print(f\"✓ Target word evaluation completed\")\n",
    "        print(f\"  Samples evaluated: {len(target_word_results)}\")\n",
    "        print(f\"  Accuracy: {target_accuracy:.3f}, F1: {target_f1:.3f}\")\n",
    "        \n",
    "        pipeline_results['stages']['target_word'] = {\n",
    "            'accuracy': target_accuracy,\n",
    "            'precision': target_precision,\n",
    "            'recall': target_recall,\n",
    "            'f1_score': target_f1,\n",
    "            'samples_evaluated': len(target_word_results),\n",
    "            'results': target_word_results\n",
    "        }\n",
    "        pipeline_results['timing']['target_word'] = time.time() - stage4_start\n",
    "        \n",
    "        # Stage 5: Results Comparison and Integration\n",
    "        print(\"\\n=== Stage 5: Results Integration ===\")\n",
    "        stage5_start = time.time()\n",
    "        \n",
    "        # Compare classification and target word results\n",
    "        best_classifier = max(classification_results.items(), key=lambda x: x[1]['f1_score'])\n",
    "        best_classifier_name = best_classifier[0]\n",
    "        best_classifier_metrics = best_classifier[1]\n",
    "        \n",
    "        print(f\"✓ Best classifier: {best_classifier_name} (F1: {best_classifier_metrics['f1_score']:.3f})\")\n",
    "        print(f\"✓ Target word F1: {target_f1:.3f}\")\n",
    "        \n",
    "        # Create final integration results\n",
    "        integration_summary = {\n",
    "            'dataset_size': len(test_data),\n",
    "            'train_size': len(train_data),\n",
    "            'test_size': len(test_data_split),\n",
    "            'best_classification_method': best_classifier_name,\n",
    "            'classification_f1': best_classifier_metrics['f1_score'],\n",
    "            'target_word_f1': target_f1,\n",
    "            'total_stages': 5,\n",
    "            'total_time': sum(pipeline_results['timing'].values())\n",
    "        }\n",
    "        \n",
    "        pipeline_results['stages']['integration'] = integration_summary\n",
    "        pipeline_results['timing']['integration'] = time.time() - stage5_start\n",
    "        pipeline_results['success'] = True\n",
    "        \n",
    "        print(f\"\\n=== End-to-End Pipeline Summary ===\")\n",
    "        print(f\"Total time: {integration_summary['total_time']:.2f}s\")\n",
    "        print(f\"Data: {integration_summary['train_size']} train + {integration_summary['test_size']} test\")\n",
    "        print(f\"Best classification F1: {integration_summary['classification_f1']:.3f} ({best_classifier_name})\")\n",
    "        print(f\"Target word F1: {integration_summary['target_word_f1']:.3f}\")\n",
    "        print(f\"All stages completed successfully ✓\")\n",
    "        \n",
    "        return pipeline_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Pipeline integration failed: {e}\"\n",
    "        print(f\"✗ {error_msg}\")\n",
    "        pipeline_results['errors'].append(error_msg)\n",
    "        return pipeline_results\n",
    "\n",
    "# Run end-to-end integration test\n",
    "integration_results = test_end_to_end_pipeline(test_config, integration_dataset, use_mock=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Flow Validation\n",
    "Test data flow between pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data flow between components\n",
    "def test_data_flow_validation(integration_results: Dict):\n",
    "    \"\"\"Validate data flow between pipeline components\"\"\"\n",
    "    print(\"Testing data flow validation...\")\n",
    "    \n",
    "    if not integration_results['success']:\n",
    "        print(\"✗ Cannot validate data flow - integration test failed\")\n",
    "        return None\n",
    "    \n",
    "    validation_results = {\n",
    "        'data_consistency': {},\n",
    "        'shape_validation': {},\n",
    "        'type_validation': {},\n",
    "        'flow_integrity': True,\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        stages = integration_results['stages']\n",
    "        \n",
    "        # Validate data loading stage\n",
    "        if 'data_loading' in stages:\n",
    "            data_stage = stages['data_loading']\n",
    "            total_samples = data_stage['train_samples'] + data_stage['test_samples']\n",
    "            \n",
    "            print(f\"✓ Data loading validation:\")\n",
    "            print(f\"  Total samples: {total_samples}\")\n",
    "            print(f\"  Split ratio: {data_stage['split_ratio']}\")\n",
    "            \n",
    "            validation_results['data_consistency']['total_samples'] = total_samples\n",
    "        \n",
    "        # Validate embedding stage\n",
    "        if 'embedding' in stages:\n",
    "            embedding_stage = stages['embedding']\n",
    "            train_shape = embedding_stage['train_embedding_shape']\n",
    "            test_shape = embedding_stage['test_embedding_shape']\n",
    "            \n",
    "            print(f\"✓ Embedding validation:\")\n",
    "            print(f\"  Train embeddings: {train_shape}\")\n",
    "            print(f\"  Test embeddings: {test_shape}\")\n",
    "            \n",
    "            # Check consistency\n",
    "            if train_shape[1] != test_shape[1]:\n",
    "                validation_results['issues'].append(\"Embedding dimension mismatch between train/test\")\n",
    "                validation_results['flow_integrity'] = False\n",
    "            \n",
    "            validation_results['shape_validation']['embedding_dim'] = train_shape[1]\n",
    "        \n",
    "        # Validate classification stage\n",
    "        if 'classification' in stages:\n",
    "            classification_stage = stages['classification']\n",
    "            \n",
    "            print(f\"✓ Classification validation:\")\n",
    "            for model_name, metrics in classification_stage.items():\n",
    "                predictions = metrics['predictions']\n",
    "                probabilities = metrics['probabilities']\n",
    "                \n",
    "                print(f\"  {model_name}: {len(predictions)} predictions\")\n",
    "                \n",
    "                # Validate prediction ranges\n",
    "                if not all(p in [0, 1] for p in predictions):\n",
    "                    validation_results['issues'].append(f\"{model_name}: Invalid prediction values\")\n",
    "                    validation_results['flow_integrity'] = False\n",
    "                \n",
    "                if not all(0 <= p <= 1 for p in probabilities):\n",
    "                    validation_results['issues'].append(f\"{model_name}: Invalid probability values\")\n",
    "                    validation_results['flow_integrity'] = False\n",
    "            \n",
    "            validation_results['type_validation']['classification_models'] = len(classification_stage)\n",
    "        \n",
    "        # Validate target word stage\n",
    "        if 'target_word' in stages:\n",
    "            target_word_stage = stages['target_word']\n",
    "            results = target_word_stage['results']\n",
    "            \n",
    "            print(f\"✓ Target word validation:\")\n",
    "            print(f\"  Evaluated samples: {len(results)}\")\n",
    "            print(f\"  Accuracy: {target_word_stage['accuracy']:.3f}\")\n",
    "            \n",
    "            # Validate result structure\n",
    "            required_keys = ['mcid', 'true_label', 'prediction', 'correct']\n",
    "            for result in results[:3]:  # Check first 3 results\n",
    "                missing_keys = [key for key in required_keys if key not in result]\n",
    "                if missing_keys:\n",
    "                    validation_results['issues'].append(f\"Missing keys in target word results: {missing_keys}\")\n",
    "                    validation_results['flow_integrity'] = False\n",
    "            \n",
    "            validation_results['data_consistency']['target_word_samples'] = len(results)\n",
    "        \n",
    "        # Validate timing consistency\n",
    "        timing = integration_results['timing']\n",
    "        total_time = sum(timing.values())\n",
    "        \n",
    "        print(f\"\\n✓ Timing validation:\")\n",
    "        for stage, time_taken in timing.items():\n",
    "            percentage = (time_taken / total_time) * 100\n",
    "            print(f\"  {stage}: {time_taken:.2f}s ({percentage:.1f}%)\")\n",
    "        \n",
    "        validation_results['data_consistency']['total_time'] = total_time\n",
    "        \n",
    "        # Final validation summary\n",
    "        if validation_results['flow_integrity']:\n",
    "            print(f\"\\n✓ Data flow validation PASSED\")\n",
    "            print(f\"  All stages have consistent data flow\")\n",
    "        else:\n",
    "            print(f\"\\n✗ Data flow validation FAILED\")\n",
    "            print(f\"  Issues found: {len(validation_results['issues'])}\")\n",
    "            for issue in validation_results['issues']:\n",
    "                print(f\"    - {issue}\")\n",
    "        \n",
    "        return validation_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Data flow validation failed: {e}\"\n",
    "        print(f\"✗ {error_msg}\")\n",
    "        validation_results['issues'].append(error_msg)\n",
    "        validation_results['flow_integrity'] = False\n",
    "        return validation_results\n",
    "\n",
    "# Run data flow validation\n",
    "flow_validation = test_data_flow_validation(integration_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Handling and Recovery Testing\n",
    "Test error scenarios and recovery mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test error handling scenarios\n",
    "def test_error_handling_scenarios():\n",
    "    \"\"\"Test various error conditions and recovery mechanisms\"\"\"\n",
    "    print(\"Testing error handling scenarios...\")\n",
    "    \n",
    "    error_scenarios = [\n",
    "        {\n",
    "            'name': 'Empty dataset',\n",
    "            'data': pd.DataFrame(columns=['mcid', 'claims', 'label']),\n",
    "            'expected': 'Handle gracefully with appropriate error message'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Single sample dataset',\n",
    "            'data': pd.DataFrame({\n",
    "                'mcid': ['SINGLE_001'],\n",
    "                'claims': ['E119 A1234 |eoc| B5678'],\n",
    "                'label': [1]\n",
    "            }),\n",
    "            'expected': 'Handle edge case with minimal data'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Missing columns',\n",
    "            'data': pd.DataFrame({\n",
    "                'mcid': ['MISSING_001', 'MISSING_002'],\n",
    "                'claims': ['E119 A1234', 'B5678 C9012']\n",
    "                # Missing 'label' column\n",
    "            }),\n",
    "            'expected': 'Detect missing required columns'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Invalid data types',\n",
    "            'data': pd.DataFrame({\n",
    "                'mcid': [123, 456],  # Should be strings\n",
    "                'claims': ['E119 A1234', 'B5678 C9012'],\n",
    "                'label': ['yes', 'no']  # Should be integers\n",
    "            }),\n",
    "            'expected': 'Handle type conversion or validation errors'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Extremely long claims',\n",
    "            'data': pd.DataFrame({\n",
    "                'mcid': ['LONG_001'],\n",
    "                'claims': [' '.join(['CODE123'] * 1000)],  # Very long claim\n",
    "                'label': [1]\n",
    "            }),\n",
    "            'expected': 'Handle memory and processing issues with long text'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    error_test_results = []\n",
    "    \n",
    "    for scenario in error_scenarios:\n",
    "        print(f\"\\n--- {scenario['name']} ---\")\n",
    "        print(f\"Expected: {scenario['expected']}\")\n",
    "        \n",
    "        try:\n",
    "            # Test basic data validation\n",
    "            data = scenario['data']\n",
    "            print(f\"Data shape: {data.shape}\")\n",
    "            print(f\"Columns: {list(data.columns)}\")\n",
    "            \n",
    "            # Test required column validation\n",
    "            required_columns = ['mcid', 'claims', 'label']\n",
    "            missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "            \n",
    "            if missing_columns:\n",
    "                print(f\"✓ Detected missing columns: {missing_columns}\")\n",
    "                error_test_results.append({\n",
    "                    'scenario': scenario['name'],\n",
    "                    'status': 'handled',\n",
    "                    'issue': f\"Missing columns: {missing_columns}\"\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Test data type validation\n",
    "            if len(data) > 0:\n",
    "                # Check label column type\n",
    "                if 'label' in data.columns:\n",
    "                    if not pd.api.types.is_numeric_dtype(data['label']):\n",
    "                        print(f\"✓ Detected non-numeric labels: {data['label'].dtype}\")\n",
    "                        error_test_results.append({\n",
    "                            'scenario': scenario['name'],\n",
    "                            'status': 'handled',\n",
    "                            'issue': f\"Invalid label type: {data['label'].dtype}\"\n",
    "                        })\n",
    "                        continue\n",
    "                \n",
    "                # Check for extremely long claims\n",
    "                max_claim_length = data['claims'].str.len().max()\n",
    "                if max_claim_length > 10000:  # Arbitrary threshold\n",
    "                    print(f\"✓ Detected very long claim: {max_claim_length} characters\")\n",
    "                    error_test_results.append({\n",
    "                        'scenario': scenario['name'],\n",
    "                        'status': 'handled',\n",
    "                        'issue': f\"Extremely long claim: {max_claim_length} chars\"\n",
    "                    })\n",
    "                    continue\n",
    "            \n",
    "            # Test edge cases\n",
    "            if len(data) == 0:\n",
    "                print(f\"✓ Detected empty dataset\")\n",
    "                error_test_results.append({\n",
    "                    'scenario': scenario['name'],\n",
    "                    'status': 'handled',\n",
    "                    'issue': 'Empty dataset'\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            if len(data) == 1:\n",
    "                print(f\"✓ Detected single-sample dataset\")\n",
    "                error_test_results.append({\n",
    "                    'scenario': scenario['name'],\n",
    "                    'status': 'handled',\n",
    "                    'issue': 'Single sample - cannot split train/test'\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # If we reach here, the scenario might be valid\n",
    "            print(f\"✓ Scenario appears valid - no immediate issues detected\")\n",
    "            error_test_results.append({\n",
    "                'scenario': scenario['name'],\n",
    "                'status': 'passed',\n",
    "                'issue': None\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✓ Exception caught and handled: {e}\")\n",
    "            error_test_results.append({\n",
    "                'scenario': scenario['name'],\n",
    "                'status': 'exception_handled',\n",
    "                'issue': str(e)\n",
    "            })\n",
    "    \n",
    "    # Summary of error handling tests\n",
    "    print(f\"\\n=== Error Handling Summary ===\")\n",
    "    handled_count = sum(1 for r in error_test_results if r['status'] in ['handled', 'exception_handled'])\n",
    "    print(f\"Scenarios tested: {len(error_scenarios)}\")\n",
    "    print(f\"Errors handled: {handled_count}\")\n",
    "    print(f\"Passed scenarios: {len(error_test_results) - handled_count}\")\n",
    "    \n",
    "    for result in error_test_results:\n",
    "        if result['issue']:\n",
    "            print(f\"  {result['scenario']}: {result['status']} - {result['issue']}\")\n",
    "    \n",
    "    return error_test_results\n",
    "\n",
    "# Run error handling tests\n",
    "error_handling_results = test_error_handling_scenarios()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Benchmarking\n",
    "Test end-to-end performance characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test end-to-end performance benchmarking\n",
    "def test_performance_benchmarking(config: PipelineConfig):\n",
    "    \"\"\"Test end-to-end performance with different dataset sizes\"\"\"\n",
    "    print(\"Testing end-to-end performance benchmarking...\")\n",
    "    \n",
    "    benchmark_sizes = [10, 25, 50, 100]  # Different dataset sizes\n",
    "    benchmark_results = []\n",
    "    \n",
    "    for size in benchmark_sizes:\n",
    "        print(f\"\\n--- Benchmarking with {size} samples ---\")\n",
    "        \n",
    "        try:\n",
    "            # Create test dataset\n",
    "            test_data = create_integration_test_dataset(size)\n",
    "            \n",
    "            # Run simplified pipeline\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Data splitting\n",
    "            split_time_start = time.time()\n",
    "            split_idx = int(len(test_data) * 0.8)\n",
    "            train_data = test_data.iloc[:split_idx]\n",
    "            test_data_split = test_data.iloc[split_idx:]\n",
    "            split_time = time.time() - split_time_start\n",
    "            \n",
    "            # Mock embedding generation\n",
    "            embedding_time_start = time.time()\n",
    "            train_embeddings = np.random.randn(len(train_data), 768)\n",
    "            test_embeddings = np.random.randn(len(test_data_split), 768)\n",
    "            embedding_time = time.time() - embedding_time_start\n",
    "            \n",
    "            # Classification training and evaluation\n",
    "            classification_time_start = time.time()\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            classifier = LogisticRegression(random_state=42)\n",
    "            classifier.fit(train_embeddings, train_data['label'])\n",
    "            predictions = classifier.predict(test_embeddings)\n",
    "            classification_time = time.time() - classification_time_start\n",
    "            \n",
    "            # Target word evaluation (subset)\n",
    "            target_word_time_start = time.time()\n",
    "            target_subset = test_data_split.head(min(5, len(test_data_split)))\n",
    "            # Mock target word processing\n",
    "            time.sleep(0.01 * len(target_subset))  # Simulate processing time\n",
    "            target_word_time = time.time() - target_word_time_start\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate metrics\n",
    "            from sklearn.metrics import accuracy_score, f1_score\n",
    "            accuracy = accuracy_score(test_data_split['label'], predictions)\n",
    "            f1 = f1_score(test_data_split['label'], predictions, zero_division=0)\n",
    "            \n",
    "            benchmark_result = {\n",
    "                'dataset_size': size,\n",
    "                'train_size': len(train_data),\n",
    "                'test_size': len(test_data_split),\n",
    "                'total_time': total_time,\n",
    "                'split_time': split_time,\n",
    "                'embedding_time': embedding_time,\n",
    "                'classification_time': classification_time,\n",
    "                'target_word_time': target_word_time,\n",
    "                'samples_per_second': size / total_time,\n",
    "                'accuracy': accuracy,\n",
    "                'f1_score': f1,\n",
    "                'memory_efficiency': embedding_time / size  # Time per sample for embeddings\n",
    "            }\n",
    "            \n",
    "            benchmark_results.append(benchmark_result)\n",
    "            \n",
    "            print(f\"  Total time: {total_time:.2f}s\")\n",
    "            print(f\"  Samples/sec: {benchmark_result['samples_per_second']:.2f}\")\n",
    "            print(f\"  F1 score: {f1:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Benchmark failed for size {size}: {e}\")\n",
    "    \n",
    "    # Analyze performance trends\n",
    "    if benchmark_results:\n",
    "        print(f\"\\n=== Performance Benchmark Summary ===\")\n",
    "        print(f\"{'Size':<6} {'Time(s)':<8} {'Rate':<8} {'F1':<6} {'Memory Eff':<12}\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        for result in benchmark_results:\n",
    "            print(f\"{result['dataset_size']:<6} {result['total_time']:<8.2f} {result['samples_per_second']:<8.2f} {result['f1_score']:<6.3f} {result['memory_efficiency']:<12.4f}\")\n",
    "        \n",
    "        # Performance analysis\n",
    "        print(f\"\\nPerformance Analysis:\")\n",
    "        \n",
    "        # Scalability\n",
    "        if len(benchmark_results) >= 2:\n",
    "            small_rate = benchmark_results[0]['samples_per_second']\n",
    "            large_rate = benchmark_results[-1]['samples_per_second']\n",
    "            scalability_ratio = large_rate / small_rate\n",
    "            print(f\"  Scalability: {scalability_ratio:.2f}x (large vs small dataset rate)\")\n",
    "        \n",
    "        # Time breakdown analysis\n",
    "        avg_embedding_pct = np.mean([r['embedding_time']/r['total_time']*100 for r in benchmark_results])\n",
    "        avg_classification_pct = np.mean([r['classification_time']/r['total_time']*100 for r in benchmark_results])\n",
    "        \n",
    "        print(f\"  Avg embedding time: {avg_embedding_pct:.1f}% of total\")\n",
    "        print(f\"  Avg classification time: {avg_classification_pct:.1f}% of total\")\n",
    "        \n",
    "        # Performance bottlenecks\n",
    "        bottleneck_stages = []\n",
    "        for result in benchmark_results:\n",
    "            stage_times = {\n",
    "                'embedding': result['embedding_time'],\n",
    "                'classification': result['classification_time'],\n",
    "                'target_word': result['target_word_time']\n",
    "            }\n",
    "            slowest_stage = max(stage_times.items(), key=lambda x: x[1])\n",
    "            bottleneck_stages.append(slowest_stage[0])\n",
    "        \n",
    "        from collections import Counter\n",
    "        bottleneck_counts = Counter(bottleneck_stages)\n",
    "        most_common_bottleneck = bottleneck_counts.most_common(1)[0]\n",
    "        print(f\"  Most common bottleneck: {most_common_bottleneck[0]} ({most_common_bottleneck[1]}/{len(benchmark_results)} tests)\")\n",
    "    \n",
    "    return benchmark_results\n",
    "\n",
    "# Run performance benchmarking\n",
    "performance_benchmarks = test_performance_benchmarking(test_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integration Results Summary\n",
    "Comprehensive summary of all integration tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive integration test summary\n",
    "def generate_integration_summary():\n",
    "    \"\"\"Generate comprehensive summary of all integration tests\"\"\"\n",
    "    print(\"Generating integration test summary...\")\n",
    "    \n",
    "    summary = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'test_categories': {\n",
    "            'configuration': None,\n",
    "            'end_to_end_pipeline': None,\n",
    "            'data_flow_validation': None,\n",
    "            'error_handling': None,\n",
    "            'performance_benchmarking': None\n",
    "        },\n",
    "        'overall_status': 'unknown',\n",
    "        'recommendations': [],\n",
    "        'critical_issues': []\n",
    "    }\n",
    "    \n",
    "    # Configuration testing summary\n",
    "    if 'pipeline_configs' in globals() and pipeline_configs:\n",
    "        summary['test_categories']['configuration'] = {\n",
    "            'status': 'passed',\n",
    "            'configs_tested': len(pipeline_configs),\n",
    "            'valid_configs': len(pipeline_configs),\n",
    "            'details': f\"Successfully loaded {len(pipeline_configs)} configurations\"\n",
    "        }\n",
    "    else:\n",
    "        summary['test_categories']['configuration'] = {\n",
    "            'status': 'partial',\n",
    "            'details': 'Created test configuration, but existing configs had issues'\n",
    "        }\n",
    "        summary['recommendations'].append('Review and fix configuration file issues')\n",
    "    \n",
    "    # End-to-end pipeline summary\n",
    "    if 'integration_results' in globals() and integration_results:\n",
    "        if integration_results['success']:\n",
    "            summary['test_categories']['end_to_end_pipeline'] = {\n",
    "                'status': 'passed',\n",
    "                'stages_completed': len(integration_results['stages']),\n",
    "                'total_time': sum(integration_results['timing'].values()),\n",
    "                'details': 'All pipeline stages completed successfully'\n",
    "            }\n",
    "        else:\n",
    "            summary['test_categories']['end_to_end_pipeline'] = {\n",
    "                'status': 'failed',\n",
    "                'errors': integration_results['errors'],\n",
    "                'details': f\"Pipeline failed with {len(integration_results['errors'])} errors\"\n",
    "            }\n",
    "            summary['critical_issues'].extend(integration_results['errors'])\n",
    "    \n",
    "    # Data flow validation summary\n",
    "    if 'flow_validation' in globals() and flow_validation:\n",
    "        if flow_validation['flow_integrity']:\n",
    "            summary['test_categories']['data_flow_validation'] = {\n",
    "                'status': 'passed',\n",
    "                'details': 'Data flow integrity validated across all stages'\n",
    "            }\n",
    "        else:\n",
    "            summary['test_categories']['data_flow_validation'] = {\n",
    "                'status': 'failed',\n",
    "                'issues': flow_validation['issues'],\n",
    "                'details': f\"Data flow issues detected: {len(flow_validation['issues'])}\"\n",
    "            }\n",
    "            summary['critical_issues'].extend(flow_validation['issues'])\n",
    "    \n",
    "    # Error handling summary\n",
    "    if 'error_handling_results' in globals() and error_handling_results:\n",
    "        handled_count = sum(1 for r in error_handling_results if r['status'] in ['handled', 'exception_handled'])\n",
    "        total_scenarios = len(error_handling_results)\n",
    "        \n",
    "        summary['test_categories']['error_handling'] = {\n",
    "            'status': 'passed' if handled_count == total_scenarios else 'partial',\n",
    "            'scenarios_tested': total_scenarios,\n",
    "            'scenarios_handled': handled_count,\n",
    "            'details': f\"Error handling: {handled_count}/{total_scenarios} scenarios handled properly\"\n",
    "        }\n",
    "        \n",
    "        if handled_count < total_scenarios:\n",
    "            summary['recommendations'].append('Improve error handling for unhandled scenarios')\n",
    "    \n",
    "    # Performance benchmarking summary\n",
    "    if 'performance_benchmarks' in globals() and performance_benchmarks:\n",
    "        avg_rate = np.mean([r['samples_per_second'] for r in performance_benchmarks])\n",
    "        avg_f1 = np.mean([r['f1_score'] for r in performance_benchmarks])\n",
    "        \n",
    "        summary['test_categories']['performance_benchmarking'] = {\n",
    "            'status': 'passed',\n",
    "            'dataset_sizes_tested': [r['dataset_size'] for r in performance_benchmarks],\n",
    "            'avg_samples_per_second': avg_rate,\n",
    "            'avg_f1_score': avg_f1,\n",
    "            'details': f\"Performance tested on {len(performance_benchmarks)} dataset sizes\"\n",
    "        }\n",
    "        \n",
    "        if avg_rate < 1.0:\n",
    "            summary['recommendations'].append('Performance optimization needed - processing rate below 1 sample/sec')\n",
    "    \n",
    "    # Determine overall status\n",
    "    passed_tests = sum(1 for cat in summary['test_categories'].values() \n",
    "                      if cat and cat['status'] == 'passed')\n",
    "    total_tests = sum(1 for cat in summary['test_categories'].values() if cat)\n",
    "    failed_tests = sum(1 for cat in summary['test_categories'].values() \n",
    "                      if cat and cat['status'] == 'failed')\n",
    "    \n",
    "    if failed_tests > 0:\n",
    "        summary['overall_status'] = 'failed'\n",
    "    elif passed_tests == total_tests:\n",
    "        summary['overall_status'] = 'passed'\n",
    "    else:\n",
    "        summary['overall_status'] = 'partial'\n",
    "    \n",
    "    # General recommendations\n",
    "    if not summary['recommendations']:\n",
    "        summary['recommendations'].append('All tests passed - system ready for production use')\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"INTEGRATION TEST SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Generated: {summary['timestamp']}\")\n",
    "    print(f\"Overall Status: {summary['overall_status'].upper()}\")\n",
    "    print(f\"\\nTest Results: {passed_tests}/{total_tests} passed\")\n",
    "    \n",
    "    for category, result in summary['test_categories'].items():\n",
    "        if result:\n",
    "            status_symbol = \"✓\" if result['status'] == 'passed' else \"✗\" if result['status'] == 'failed' else \"~\"\n",
    "            print(f\"  {status_symbol} {category.replace('_', ' ').title()}: {result['status']}\")\n",
    "            print(f\"    {result['details']}\")\n",
    "    \n",
    "    if summary['critical_issues']:\n",
    "        print(f\"\\nCritical Issues ({len(summary['critical_issues'])}):\")\n",
    "        for issue in summary['critical_issues']:\n",
    "            print(f\"  ✗ {issue}\")\n",
    "    \n",
    "    print(f\"\\nRecommendations:\")\n",
    "    for rec in summary['recommendations']:\n",
    "        print(f\"  • {rec}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate final integration summary\n",
    "final_summary = generate_integration_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Integration Testing\n",
    "Use this section for custom integration scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom integration testing - modify as needed\n",
    "print(\"=== Custom Integration Testing ===\")\n",
    "print(\"Use this cell for your own integration testing scenarios\")\n",
    "\n",
    "# Example: Test specific configuration combinations\n",
    "custom_scenarios = [\n",
    "    {\n",
    "        'name': 'High-volume processing',\n",
    "        'dataset_size': 200,\n",
    "        'target_codes': ['E119', 'Z03818', 'N6320', 'M1710', 'G0378'],\n",
    "        'test_focus': 'Performance under load'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Minimal target codes',\n",
    "        'dataset_size': 50,\n",
    "        'target_codes': ['E119'],\n",
    "        'test_focus': 'Single target code evaluation'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Edge case data',\n",
    "        'dataset_size': 20,\n",
    "        'target_codes': ['E119', 'Z03818'],\n",
    "        'test_focus': 'Unusual data patterns'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\nCustom scenarios available for testing:\")\n",
    "for i, scenario in enumerate(custom_scenarios, 1):\n",
    "    print(f\"  {i}. {scenario['name']} - {scenario['test_focus']}\")\n",
    "    print(f\"     Dataset: {scenario['dataset_size']} samples, Targets: {scenario['target_codes']}\")\n",
    "\n",
    "# Add your custom integration testing logic here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom debugging for specific integration issues\n",
    "print(\"=== Custom Integration Debugging ===\")\n",
    "\n",
    "# Add your integration debugging code here\n",
    "# Examples:\n",
    "# - Test specific component interactions\n",
    "# - Debug memory usage during pipeline execution\n",
    "# - Test different API response scenarios\n",
    "# - Validate specific configuration edge cases\n",
    "# - Test recovery from partial failures\n",
    "\n",
    "print(\"Add your custom integration debugging code in this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides comprehensive end-to-end integration testing for the MGPT-eval pipeline:\n",
    "\n",
    "1. **Configuration Testing**: Validates multiple pipeline configurations\n",
    "2. **Test Data Generation**: Creates comprehensive datasets for testing\n",
    "3. **Full Pipeline Integration**: Tests complete end-to-end workflows\n",
    "4. **Data Flow Validation**: Validates data consistency between components\n",
    "5. **Error Handling**: Tests error scenarios and recovery mechanisms\n",
    "6. **Performance Benchmarking**: Measures performance across different scales\n",
    "7. **Integration Summary**: Provides comprehensive test results overview\n",
    "8. **Custom Testing**: Space for your own integration scenarios\n",
    "\n",
    "Use this notebook to:\n",
    "- Validate complete pipeline integration\n",
    "- Debug component interactions\n",
    "- Test system performance and scalability\n",
    "- Verify error handling and recovery\n",
    "- Ensure data flow integrity\n",
    "\n",
    "The integration test results provide a comprehensive view of system readiness and highlight any issues that need attention before production deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.5"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}