{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Word Evaluation Pipeline Testing\n",
    "\n",
    "This notebook tests the target word evaluation pipeline step by step.\n",
    "Use this to debug target word detection and evaluation issues.\n",
    "\n",
    "## Overview\n",
    "- Test text generation and target code detection\n",
    "- Debug tokenization and decoding processes\n",
    "- Validate target word evaluation metrics\n",
    "- Test batch processing and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('/home/kosaraju/mgpt-serve/mgpt_eval')\n",
    "\n",
    "# Import actual pipeline modules\n",
    "from evaluation.target_word_evaluator import TargetWordEvaluator\n",
    "from models.config_models import PipelineConfig, EvaluationConfig, TargetWordConfig\n",
    "from models.data_models import DataSample, DataBatch\n",
    "from pipelines.evaluation_pipeline import EvaluationPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup\n",
    "Load and validate target word evaluation configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration for target word evaluation\n",
    "config_path = \"/home/kosaraju/mgpt-serve/mgpt_eval/configs/examples/config_evaluation_only.yaml\"\n",
    "\n",
    "try:\n",
    "    config = PipelineConfig.from_yaml(config_path)\n",
    "    print(\"✓ Configuration loaded successfully\")\n",
    "    print(f\"  - Target codes: {config.evaluation.target_word.target_codes}\")\n",
    "    print(f\"  - Max tokens: {config.evaluation.target_word.max_tokens}\")\n",
    "    print(f\"  - Generation attempts: {config.evaluation.target_word.generation_attempts}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Configuration loading failed: {e}\")\n",
    "    config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test target codes validation\n",
    "def test_target_codes_validation():\n",
    "    \"\"\"Test target codes format and validation\"\"\"\n",
    "    print(\"Testing target codes validation...\")\n",
    "    \n",
    "    # Test valid target codes\n",
    "    valid_codes = [\"E119\", \"Z03818\", \"N6320\", \"M1710\"]\n",
    "    print(f\"Valid codes: {valid_codes}\")\n",
    "    \n",
    "    # Test invalid formats\n",
    "    invalid_codes = [\"\", \"invalid\", \"123\", \"toolong123\"]\n",
    "    for code in invalid_codes:\n",
    "        print(f\"Testing invalid code: '{code}'\")\n",
    "    \n",
    "    # Test case sensitivity\n",
    "    case_codes = [\"e119\", \"E119\", \"z03818\", \"Z03818\"]\n",
    "    print(f\"Case sensitivity test: {case_codes}\")\n",
    "\n",
    "test_target_codes_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. API Connectivity Test\n",
    "Test generation endpoint for target word evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation API endpoint\n",
    "def test_generation_api(api_url: str, test_prompt: str, max_tokens: int = 50):\n",
    "    \"\"\"Test text generation API for target word evaluation\"\"\"\n",
    "    print(f\"Testing generation API: {api_url}\")\n",
    "    print(f\"Input prompt: '{test_prompt}'\")\n",
    "    \n",
    "    try:\n",
    "        payload = {\n",
    "            \"prompt\": test_prompt,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = requests.post(f\"{api_url}/generate\", json=payload, timeout=30)\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            generated_text = result.get('generated_text', '')\n",
    "            print(f\"✓ Generation successful ({response_time:.2f}s)\")\n",
    "            print(f\"  Generated: '{generated_text}'\")\n",
    "            return generated_text\n",
    "        else:\n",
    "            print(f\"✗ Generation failed: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ API error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with sample medical codes\n",
    "if config and config.api:\n",
    "    api_url = f\"http://{config.api.host}:{config.api.port}\"\n",
    "    test_prompt = \"N6320 G0378 |eoc| Z91048 M1710\"\n",
    "    generated = test_generation_api(api_url, test_prompt, 100)\nelse:\n",
    "    print(\"Using mock generation for testing\")\n",
    "    generated = \"O0903 K9289 |eoc| N6322 76642 Z09 76642 |eoc| Z1239 O9989\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Word Detection Logic\n",
    "Test the core target word detection functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test target word detection function\n",
    "def test_target_detection(text: str, target_codes: List[str]) -> Dict:\n",
    "    \"\"\"Test target code detection in generated text\"\"\"\n",
    "    print(f\"Testing target detection in: '{text}'\")\n",
    "    print(f\"Target codes: {target_codes}\")\n",
    "    \n",
    "    # Split text into tokens (codes)\n",
    "    tokens = text.split()\n",
    "    print(f\"Tokens found: {tokens}\")\n",
    "    \n",
    "    # Check for target codes\n",
    "    found_targets = []\n",
    "    target_set = set(code.upper() for code in target_codes)\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        # Clean token (remove special characters)\n",
    "        clean_token = token.strip('|eoc|').upper()\n",
    "        if clean_token in target_set:\n",
    "            found_targets.append({\n",
    "                'code': clean_token,\n",
    "                'position': i,\n",
    "                'original_token': token\n",
    "            })\n",
    "    \n",
    "    result = {\n",
    "        'text': text,\n",
    "        'tokens': tokens,\n",
    "        'target_codes': target_codes,\n",
    "        'found_targets': found_targets,\n",
    "        'has_target': len(found_targets) > 0,\n",
    "        'num_targets_found': len(found_targets)\n",
    "    }\n",
    "    \n",
    "    print(f\"Found targets: {found_targets}\")\n",
    "    print(f\"Has target: {result['has_target']}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with sample data\n",
    "sample_text = \"O0903 K9289 |eoc| N6322 76642 Z09 76642 |eoc| Z1239 O9989 |eoc| Z03818 U0003\"\n",
    "target_codes = [\"E119\", \"Z03818\", \"N6320\", \"M1710\"]\n",
    "\n",
    "detection_result = test_target_detection(sample_text, target_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test edge cases for target detection\n",
    "def test_detection_edge_cases():\n",
    "    \"\"\"Test edge cases in target detection\"\"\"\n",
    "    print(\"Testing edge cases for target detection...\")\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            'name': 'Empty text',\n",
    "            'text': '',\n",
    "            'targets': ['E119']\n",
    "        },\n",
    "        {\n",
    "            'name': 'No targets in text',\n",
    "            'text': 'A1234 B5678 C9012',\n",
    "            'targets': ['E119', 'Z03818']\n",
    "        },\n",
    "        {\n",
    "            'name': 'Multiple same targets',\n",
    "            'text': 'E119 A1234 E119 B5678 E119',\n",
    "            'targets': ['E119']\n",
    "        },\n",
    "        {\n",
    "            'name': 'Case sensitivity',\n",
    "            'text': 'e119 E119 z03818 Z03818',\n",
    "            'targets': ['E119', 'Z03818']\n",
    "        },\n",
    "        {\n",
    "            'name': 'With |eoc| separators',\n",
    "            'text': 'E119 |eoc| Z03818 |eoc| A1234',\n",
    "            'targets': ['E119', 'Z03818']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for case in test_cases:\n",
    "        print(f\"\\n--- {case['name']} ---\")\n",
    "        result = test_target_detection(case['text'], case['targets'])\n",
    "        print(f\"Result: {result['has_target']} (found {result['num_targets_found']} targets)\")\n",
    "\n",
    "test_detection_edge_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Target Word Evaluator Testing\n",
    "Test the actual TargetWordEvaluator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TargetWordEvaluator\n",
    "def test_target_word_evaluator():\n",
    "    \"\"\"Test TargetWordEvaluator initialization and basic functionality\"\"\"\n",
    "    print(\"Testing TargetWordEvaluator...\")\n",
    "    \n",
    "    try:\n",
    "        # Create evaluator with test configuration\n",
    "        if config:\n",
    "            evaluator = TargetWordEvaluator(config)\n",
    "        else:\n",
    "            # Create mock config\n",
    "            from models.config_models import TargetWordConfig, EvaluationConfig, APIConfig\n",
    "            \n",
    "            target_config = TargetWordConfig(\n",
    "                target_codes=[\"E119\", \"Z03818\", \"N6320\"],\n",
    "                max_tokens=200,\n",
    "                generation_attempts=5\n",
    "            )\n",
    "            eval_config = EvaluationConfig(target_word=target_config)\n",
    "            api_config = APIConfig(host=\"localhost\", port=8000)\n",
    "            \n",
    "            mock_config = type('Config', (), {\n",
    "                'evaluation': eval_config,\n",
    "                'api': api_config\n",
    "            })()\n",
    "            \n",
    "            evaluator = TargetWordEvaluator(mock_config)\n",
    "        \n",
    "        print(\"✓ TargetWordEvaluator initialized successfully\")\n",
    "        print(f\"  Target codes: {evaluator.target_codes}\")\n",
    "        print(f\"  Max tokens: {evaluator.max_tokens}\")\n",
    "        print(f\"  Generation attempts: {evaluator.generation_attempts}\")\n",
    "        \n",
    "        return evaluator\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ TargetWordEvaluator initialization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "evaluator = test_target_word_evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single sample evaluation\n",
    "def test_single_evaluation(evaluator, sample_prompt: str, use_mock: bool = True):\n",
    "    \"\"\"Test evaluation of a single sample\"\"\"\n",
    "    print(f\"Testing single sample evaluation: '{sample_prompt}'\")\n",
    "    \n",
    "    if not evaluator:\n",
    "        print(\"✗ No evaluator available\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if use_mock:\n",
    "            # Mock generation responses for testing\n",
    "            mock_responses = [\n",
    "                \"O0903 K9289 |eoc| N6322 76642\",\n",
    "                \"Z09 76642 |eoc| Z1239 O9989\",\n",
    "                \"Z03818 U0003 |eoc| E119 A1234\",\n",
    "                \"B5678 C9012 |eoc| N6320 D3456\",\n",
    "                \"F7890 G1234 |eoc| H5678 I9012\"\n",
    "            ]\n",
    "            \n",
    "            # Simulate evaluation process\n",
    "            found_targets = []\n",
    "            for i, response in enumerate(mock_responses):\n",
    "                print(f\"  Attempt {i+1}: '{response}'\")\n",
    "                \n",
    "                # Check for targets\n",
    "                tokens = response.split()\n",
    "                for token in tokens:\n",
    "                    clean_token = token.strip('|eoc|').upper()\n",
    "                    if clean_token in [code.upper() for code in evaluator.target_codes]:\n",
    "                        found_targets.append({\n",
    "                            'attempt': i+1,\n",
    "                            'code': clean_token,\n",
    "                            'response': response\n",
    "                        })\n",
    "                        print(f\"    → Found target: {clean_token}\")\n",
    "            \n",
    "            result = {\n",
    "                'prompt': sample_prompt,\n",
    "                'target_codes': evaluator.target_codes,\n",
    "                'total_attempts': len(mock_responses),\n",
    "                'found_targets': found_targets,\n",
    "                'has_target': len(found_targets) > 0,\n",
    "                'prediction': 1 if found_targets else 0\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            # Use actual API (if available)\n",
    "            result = evaluator.evaluate_sample(sample_prompt)\n",
    "        \n",
    "        print(f\"\\nEvaluation result:\")\n",
    "        print(f\"  Has target: {result['has_target']}\")\n",
    "        print(f\"  Prediction: {result['prediction']}\")\n",
    "        print(f\"  Found targets: {len(result['found_targets'])}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Single evaluation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with sample\n",
    "if evaluator:\n",
    "    sample = \"N6320 G0378 |eoc| Z91048 M1710\"\n",
    "    eval_result = test_single_evaluation(evaluator, sample, use_mock=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Evaluation Testing\n",
    "Test batch processing for multiple samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset for batch evaluation\n",
    "def create_test_dataset() -> pd.DataFrame:\n",
    "    \"\"\"Create a test dataset for batch evaluation\"\"\"\n",
    "    print(\"Creating test dataset...\")\n",
    "    \n",
    "    test_data = {\n",
    "        'mcid': ['123456', '123457', '123458', '123459', '123460'],\n",
    "        'claims': [\n",
    "            'N6320 G0378 |eoc| Z91048 M1710',\n",
    "            'E119 A1234 |eoc| B5678 C9012',\n",
    "            'Z03818 D3456 |eoc| F7890 G1234',\n",
    "            'H5678 I9012 |eoc| J1234 K5678',\n",
    "            'L9012 M3456 |eoc| N6320 O7890'\n",
    "        ],\n",
    "        'label': [1, 1, 1, 0, 1]  # Ground truth labels\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(test_data)\n",
    "    print(f\"Created dataset with {len(df)} samples\")\n",
    "    print(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "test_df = create_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batch evaluation\n",
    "def test_batch_evaluation(evaluator, test_df: pd.DataFrame, use_mock: bool = True):\n",
    "    \"\"\"Test batch evaluation functionality\"\"\"\n",
    "    print(\"Testing batch evaluation...\")\n",
    "    \n",
    "    if not evaluator:\n",
    "        print(\"✗ No evaluator available\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        results = []\n",
    "        \n",
    "        for idx, row in test_df.iterrows():\n",
    "            print(f\"\\nEvaluating sample {idx + 1}/{len(test_df)}: {row['mcid']}\")\n",
    "            print(f\"  Claims: '{row['claims']}'\")\n",
    "            print(f\"  True label: {row['label']}\")\n",
    "            \n",
    "            # Get evaluation result\n",
    "            result = test_single_evaluation(evaluator, row['claims'], use_mock=use_mock)\n",
    "            \n",
    "            if result:\n",
    "                result['mcid'] = row['mcid']\n",
    "                result['true_label'] = row['label']\n",
    "                result['correct'] = (result['prediction'] == row['label'])\n",
    "                results.append(result)\n",
    "                \n",
    "                print(f\"  Predicted: {result['prediction']} | Correct: {result['correct']}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if results:\n",
    "            predictions = [r['prediction'] for r in results]\n",
    "            true_labels = [r['true_label'] for r in results]\n",
    "            correct = [r['correct'] for r in results]\n",
    "            \n",
    "            accuracy = sum(correct) / len(correct)\n",
    "            precision = sum(p == 1 and t == 1 for p, t in zip(predictions, true_labels)) / max(sum(predictions), 1)\n",
    "            recall = sum(p == 1 and t == 1 for p, t in zip(predictions, true_labels)) / max(sum(true_labels), 1)\n",
    "            f1 = 2 * precision * recall / max(precision + recall, 1e-10)\n",
    "            \n",
    "            metrics = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'total_samples': len(results),\n",
    "                'correct_predictions': sum(correct)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n=== Batch Evaluation Metrics ===\")\n",
    "            print(f\"Accuracy:  {accuracy:.3f}\")\n",
    "            print(f\"Precision: {precision:.3f}\")\n",
    "            print(f\"Recall:    {recall:.3f}\")\n",
    "            print(f\"F1-Score:  {f1:.3f}\")\n",
    "            print(f\"Samples:   {len(results)}\")\n",
    "            \n",
    "            return {\n",
    "                'results': results,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Batch evaluation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run batch evaluation\n",
    "if evaluator and test_df is not None:\n",
    "    batch_results = test_batch_evaluation(evaluator, test_df, use_mock=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance and Memory Testing\n",
    "Test performance characteristics and memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance metrics\n",
    "def test_performance_metrics(evaluator, num_samples: int = 10):\n",
    "    \"\"\"Test performance and timing metrics\"\"\"\n",
    "    print(f\"Testing performance with {num_samples} samples...\")\n",
    "    \n",
    "    if not evaluator:\n",
    "        print(\"✗ No evaluator available\")\n",
    "        return None\n",
    "    \n",
    "    # Generate test prompts\n",
    "    test_prompts = [\n",
    "        f\"TEST{i:03d} CODE{i:03d} |eoc| SAMPLE{i:03d} DATA{i:03d}\"\n",
    "        for i in range(num_samples)\n",
    "    ]\n",
    "    \n",
    "    times = []\n",
    "    memory_usage = []\n",
    "    \n",
    "    try:\n",
    "        import psutil\n",
    "        import gc\n",
    "        \n",
    "        for i, prompt in enumerate(test_prompts):\n",
    "            print(f\"  Processing sample {i+1}/{num_samples}...\")\n",
    "            \n",
    "            # Measure memory before\n",
    "            gc.collect()\n",
    "            process = psutil.Process()\n",
    "            memory_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "            \n",
    "            # Time the evaluation\n",
    "            start_time = time.time()\n",
    "            result = test_single_evaluation(evaluator, prompt, use_mock=True)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Measure memory after\n",
    "            memory_after = process.memory_info().rss / 1024 / 1024  # MB\n",
    "            \n",
    "            execution_time = end_time - start_time\n",
    "            memory_diff = memory_after - memory_before\n",
    "            \n",
    "            times.append(execution_time)\n",
    "            memory_usage.append(memory_diff)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        avg_time = np.mean(times)\n",
    "        std_time = np.std(times)\n",
    "        avg_memory = np.mean(memory_usage)\n",
    "        max_memory = np.max(memory_usage)\n",
    "        \n",
    "        performance_stats = {\n",
    "            'num_samples': num_samples,\n",
    "            'avg_time_per_sample': avg_time,\n",
    "            'std_time': std_time,\n",
    "            'total_time': sum(times),\n",
    "            'avg_memory_per_sample': avg_memory,\n",
    "            'max_memory_usage': max_memory,\n",
    "            'samples_per_second': num_samples / sum(times)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n=== Performance Statistics ===\")\n",
    "        print(f\"Average time per sample: {avg_time:.3f}s (±{std_time:.3f}s)\")\n",
    "        print(f\"Total time: {sum(times):.3f}s\")\n",
    "        print(f\"Samples per second: {performance_stats['samples_per_second']:.2f}\")\n",
    "        print(f\"Average memory per sample: {avg_memory:.2f} MB\")\n",
    "        print(f\"Max memory usage: {max_memory:.2f} MB\")\n",
    "        \n",
    "        return performance_stats\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"psutil not available for memory monitoring\")\n",
    "        # Basic timing without memory monitoring\n",
    "        for i, prompt in enumerate(test_prompts):\n",
    "            start_time = time.time()\n",
    "            result = test_single_evaluation(evaluator, prompt, use_mock=True)\n",
    "            times.append(time.time() - start_time)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        print(f\"Average time per sample: {avg_time:.3f}s\")\n",
    "        print(f\"Samples per second: {len(test_prompts) / sum(times):.2f}\")\n",
    "        \n",
    "        return {'avg_time_per_sample': avg_time, 'total_time': sum(times)}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Performance testing failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run performance test\n",
    "if evaluator:\n",
    "    perf_stats = test_performance_metrics(evaluator, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Handling and Edge Cases\n",
    "Test error conditions and edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test error handling scenarios\n",
    "def test_error_scenarios(evaluator):\n",
    "    \"\"\"Test various error conditions and edge cases\"\"\"\n",
    "    print(\"Testing error scenarios and edge cases...\")\n",
    "    \n",
    "    if not evaluator:\n",
    "        print(\"✗ No evaluator available\")\n",
    "        return\n",
    "    \n",
    "    error_cases = [\n",
    "        {\n",
    "            'name': 'Empty prompt',\n",
    "            'prompt': '',\n",
    "            'expected': 'Handle gracefully'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Very long prompt',\n",
    "            'prompt': ' '.join(['LONGCODE123'] * 1000),\n",
    "            'expected': 'Truncate or handle gracefully'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Special characters',\n",
    "            'prompt': 'N6320@#$ G0378!@# |eoc| Z91048%^&',\n",
    "            'expected': 'Clean and process'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Unicode characters',\n",
    "            'prompt': 'N6320 G0378 |eoc| Z91048 测试',\n",
    "            'expected': 'Handle encoding'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Only separators',\n",
    "            'prompt': '|eoc| |eoc| |eoc|',\n",
    "            'expected': 'No codes found'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for case in error_cases:\n",
    "        print(f\"\\n--- {case['name']} ---\")\n",
    "        print(f\"Input: '{case['prompt'][:100]}{'...' if len(case['prompt']) > 100 else ''}'\")\n",
    "        print(f\"Expected: {case['expected']}\")\n",
    "        \n",
    "        try:\n",
    "            result = test_single_evaluation(evaluator, case['prompt'], use_mock=True)\n",
    "            if result:\n",
    "                print(f\"✓ Handled successfully: prediction={result['prediction']}\")\n",
    "            else:\n",
    "                print(f\"✗ Failed to handle case\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Exception occurred: {e}\")\n",
    "\n",
    "# Run error scenario tests\n",
    "if evaluator:\n",
    "    test_error_scenarios(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integration with Classification Results\n",
    "Test integration with classification pipeline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test comparison with classification results\n",
    "def test_classification_comparison(target_results, classification_results=None):\n",
    "    \"\"\"Compare target word evaluation with classification results\"\"\"\n",
    "    print(\"Testing comparison with classification results...\")\n",
    "    \n",
    "    if not target_results or 'results' not in target_results:\n",
    "        print(\"✗ No target word results available\")\n",
    "        return\n",
    "    \n",
    "    # Create mock classification results if not provided\n",
    "    if classification_results is None:\n",
    "        print(\"Creating mock classification results...\")\n",
    "        classification_results = {\n",
    "            'predictions': [1, 0, 1, 0, 1],  # Mock predictions\n",
    "            'probabilities': [0.8, 0.3, 0.9, 0.2, 0.7],\n",
    "            'model_type': 'LogisticRegression'\n",
    "        }\n",
    "    \n",
    "    target_predictions = [r['prediction'] for r in target_results['results']]\n",
    "    class_predictions = classification_results['predictions']\n",
    "    \n",
    "    print(f\"Target word predictions: {target_predictions}\")\n",
    "    print(f\"Classification predictions: {class_predictions}\")\n",
    "    \n",
    "    # Calculate agreement\n",
    "    agreement = sum(t == c for t, c in zip(target_predictions, class_predictions))\n",
    "    agreement_rate = agreement / len(target_predictions)\n",
    "    \n",
    "    print(f\"\\n=== Method Comparison ===\")\n",
    "    print(f\"Agreement: {agreement}/{len(target_predictions)} ({agreement_rate:.3f})\")\n",
    "    \n",
    "    # Analyze disagreements\n",
    "    disagreements = []\n",
    "    for i, (t, c) in enumerate(zip(target_predictions, class_predictions)):\n",
    "        if t != c:\n",
    "            disagreements.append({\n",
    "                'sample_index': i,\n",
    "                'target_prediction': t,\n",
    "                'classification_prediction': c,\n",
    "                'mcid': target_results['results'][i]['mcid']\n",
    "            })\n",
    "    \n",
    "    if disagreements:\n",
    "        print(f\"\\nDisagreements ({len(disagreements)} samples):\")\n",
    "        for d in disagreements:\n",
    "            print(f\"  Sample {d['sample_index']} ({d['mcid']}): Target={d['target_prediction']}, Class={d['classification_prediction']}\")\n",
    "    else:\n",
    "        print(\"\\nNo disagreements - methods agree on all samples\")\n",
    "    \n",
    "    return {\n",
    "        'agreement_rate': agreement_rate,\n",
    "        'total_samples': len(target_predictions),\n",
    "        'agreements': agreement,\n",
    "        'disagreements': disagreements\n",
    "    }\n",
    "\n",
    "# Run comparison if batch results are available\n",
    "if 'batch_results' in locals() and batch_results:\n",
    "    comparison = test_classification_comparison(batch_results)\nelse:\n",
    "    print(\"No batch results available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Custom Debugging Section\n",
    "Use this section for custom debugging and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom debugging - modify as needed\n",
    "print(\"=== Custom Debugging Section ===\")\n",
    "print(\"Use this cell for your own debugging and testing\")\n",
    "\n",
    "# Example: Test specific target codes\n",
    "custom_target_codes = [\"E119\", \"Z03818\"]  # Modify as needed\n",
    "custom_prompt = \"E119 A1234 |eoc| B5678 Z03818\"  # Modify as needed\n",
    "\n",
    "print(f\"\\nTesting custom scenario:\")\n",
    "print(f\"Prompt: '{custom_prompt}'\")\n",
    "print(f\"Target codes: {custom_target_codes}\")\n",
    "\n",
    "custom_result = test_target_detection(custom_prompt, custom_target_codes)\n",
    "print(f\"Result: {custom_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug specific issues here\n",
    "print(\"=== Debug Specific Issues ===\")\n",
    "\n",
    "# Add your debugging code here\n",
    "# Examples:\n",
    "# - Test specific API responses\n",
    "# - Debug tokenization issues\n",
    "# - Test memory leaks\n",
    "# - Validate configuration edge cases\n",
    "\n",
    "print(\"Add your custom debugging code in this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides comprehensive testing for the target word evaluation pipeline:\n",
    "\n",
    "1. **Configuration Testing**: Validates target word configuration and codes\n",
    "2. **API Testing**: Tests text generation endpoints\n",
    "3. **Detection Logic**: Tests core target code detection functionality\n",
    "4. **Evaluator Testing**: Tests TargetWordEvaluator class functionality\n",
    "5. **Batch Processing**: Tests batch evaluation capabilities\n",
    "6. **Performance**: Measures timing and memory usage\n",
    "7. **Error Handling**: Tests edge cases and error conditions\n",
    "8. **Integration**: Compares with classification results\n",
    "9. **Custom Debugging**: Space for your own testing\n",
    "\n",
    "Use this notebook to:\n",
    "- Debug target word detection issues\n",
    "- Test API connectivity\n",
    "- Validate evaluation metrics\n",
    "- Compare evaluation methods\n",
    "- Performance optimization\n",
    "\n",
    "Modify the test cases and parameters as needed for your specific debugging requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}