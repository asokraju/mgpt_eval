{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Pipeline Testing\n",
    "\n",
    "This notebook tests the overall evaluation pipeline step by step.\n",
    "Use this to debug end-to-end evaluation workflows and integration issues.\n",
    "\n",
    "## Overview\n",
    "- Test evaluation pipeline initialization and configuration\n",
    "- Debug model evaluation workflows\n",
    "- Test metrics calculation and reporting\n",
    "- Validate evaluation results and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('/home/kosaraju/mgpt-serve/mgpt_eval')\n",
    "\n",
    "# Import actual pipeline modules\n",
    "from pipelines.evaluation_pipeline import EvaluationPipeline\n",
    "from pipelines.embedding_pipeline import EmbeddingPipeline\n",
    "from pipelines.classification_pipeline import ClassificationPipeline\n",
    "from evaluation.target_word_evaluator import TargetWordEvaluator\n",
    "from models.config_models import PipelineConfig\n",
    "from models.data_models import DataSample, DataBatch\n",
    "from utils.logging_utils import setup_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup\n",
    "Load evaluation configuration and validate settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation configuration\n",
    "def test_evaluation_config_loading():\n",
    "    \"\"\"Test loading and validation of evaluation configurations\"\"\"\n",
    "    print(\"Testing evaluation configuration loading...\")\n",
    "    \n",
    "    config_files = [\n",
    "        \"/home/kosaraju/mgpt-serve/mgpt_eval/configs/examples/config_evaluation_only.yaml\",\n",
    "        \"/home/kosaraju/mgpt-serve/mgpt_eval/configs/pipeline_config.yaml\"\n",
    "    ]\n",
    "    \n",
    "    configs = {}\n",
    "    \n",
    "    for config_path in config_files:\n",
    "        try:\n",
    "            if os.path.exists(config_path):\n",
    "                config = PipelineConfig.from_yaml(config_path)\n",
    "                config_name = os.path.basename(config_path)\n",
    "                configs[config_name] = config\n",
    "                \n",
    "                print(f\"✓ {config_name} loaded successfully\")\n",
    "                print(f\"  - Evaluation enabled: {hasattr(config, 'evaluation') and config.evaluation is not None}\")\n",
    "                \n",
    "                if hasattr(config, 'evaluation') and config.evaluation:\n",
    "                    if hasattr(config.evaluation, 'target_word') and config.evaluation.target_word:\n",
    "                        print(f\"  - Target word evaluation: {config.evaluation.target_word.target_codes}\")\n",
    "                    if hasattr(config.evaluation, 'classification') and config.evaluation.classification:\n",
    "                        print(f\"  - Classification evaluation: {config.evaluation.classification.models}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"✗ Config file not found: {config_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to load {config_path}: {e}\")\n",
    "    \n",
    "    return configs\n",
    "\n",
    "configs = test_evaluation_config_loading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select configuration for testing\n",
    "def select_test_config(configs: Dict) -> PipelineConfig:\n",
    "    \"\"\"Select or create a test configuration\"\"\"\n",
    "    print(\"Selecting test configuration...\")\n",
    "    \n",
    "    if configs:\n",
    "        # Use the first available config\n",
    "        config_name = list(configs.keys())[0]\n",
    "        config = configs[config_name]\n",
    "        print(f\"Using configuration: {config_name}\")\n",
    "        return config\n",
    "    else:\n",
    "        # Create a minimal test configuration\n",
    "        print(\"Creating minimal test configuration...\")\n",
    "        \n",
    "        from models.config_models import (\n",
    "            JobConfig, InputConfig, APIConfig, EvaluationConfig,\n",
    "            TargetWordConfig, ClassificationConfig\n",
    "        )\n",
    "        \n",
    "        job_config = JobConfig(\n",
    "            job_name=\"test_evaluation\",\n",
    "            output_base_dir=\"outputs\",\n",
    "            split_ratio=0.8,\n",
    "            random_seed=42\n",
    "        )\n",
    "        \n",
    "        input_config = InputConfig(\n",
    "            dataset_path=\"test_data.csv\"\n",
    "        )\n",
    "        \n",
    "        api_config = APIConfig(\n",
    "            host=\"localhost\",\n",
    "            port=8000\n",
    "        )\n",
    "        \n",
    "        target_word_config = TargetWordConfig(\n",
    "            target_codes=[\"E119\", \"Z03818\", \"N6320\"],\n",
    "            max_tokens=200,\n",
    "            generation_attempts=5\n",
    "        )\n",
    "        \n",
    "        classification_config = ClassificationConfig(\n",
    "            models=[\"logistic_regression\", \"svm\"],\n",
    "            cross_validation_folds=5,\n",
    "            hyperparameter_search=True\n",
    "        )\n",
    "        \n",
    "        evaluation_config = EvaluationConfig(\n",
    "            target_word=target_word_config,\n",
    "            classification=classification_config\n",
    "        )\n",
    "        \n",
    "        config = PipelineConfig(\n",
    "            job=job_config,\n",
    "            input=input_config,\n",
    "            api=api_config,\n",
    "            evaluation=evaluation_config\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Test configuration created\")\n",
    "        return config\n",
    "\n",
    "test_config = select_test_config(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Data Preparation\n",
    "Create test datasets for evaluation pipeline testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive test dataset\n",
    "def create_evaluation_test_dataset() -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Create test datasets for evaluation pipeline testing\"\"\"\n",
    "    print(\"Creating evaluation test datasets...\")\n",
    "    \n",
    "    # Training dataset\n",
    "    train_data = {\n",
    "        'mcid': [f'TRAIN_{i:04d}' for i in range(1, 101)],  # 100 samples\n",
    "        'claims': [],\n",
    "        'label': []\n",
    "    }\n",
    "    \n",
    "    # Test dataset\n",
    "    test_data = {\n",
    "        'mcid': [f'TEST_{i:04d}' for i in range(1, 31)],  # 30 samples\n",
    "        'claims': [],\n",
    "        'label': []\n",
    "    }\n",
    "    \n",
    "    # Target codes for generation\n",
    "    target_codes = [\"E119\", \"Z03818\", \"N6320\", \"M1710\"]\n",
    "    other_codes = [\"G0378\", \"Z91048\", \"O0903\", \"K9289\", \"N6322\", \"76642\", \"Z09\", \"Z1239\", \"O9989\"]\n",
    "    \n",
    "    # Generate training data\n",
    "    np.random.seed(42)\n",
    "    for i in range(100):\n",
    "        # 60% positive, 40% negative\n",
    "        has_target = np.random.random() < 0.6\n",
    "        \n",
    "        codes = []\n",
    "        if has_target:\n",
    "            # Add 1-2 target codes\n",
    "            num_targets = np.random.choice([1, 2], p=[0.7, 0.3])\n",
    "            codes.extend(np.random.choice(target_codes, num_targets, replace=False))\n",
    "        \n",
    "        # Add 3-6 other codes\n",
    "        num_others = np.random.randint(3, 7)\n",
    "        codes.extend(np.random.choice(other_codes, num_others, replace=True))\n",
    "        \n",
    "        # Shuffle and add separators\n",
    "        np.random.shuffle(codes)\n",
    "        claims = ' '.join(codes[:3]) + ' |eoc| ' + ' '.join(codes[3:])\n",
    "        \n",
    "        train_data['claims'].append(claims)\n",
    "        train_data['label'].append(1 if has_target else 0)\n",
    "    \n",
    "    # Generate test data\n",
    "    for i in range(30):\n",
    "        # 50% positive, 50% negative for balanced test\n",
    "        has_target = i < 15\n",
    "        \n",
    "        codes = []\n",
    "        if has_target:\n",
    "            # Add 1 target code\n",
    "            codes.append(np.random.choice(target_codes))\n",
    "        \n",
    "        # Add 3-5 other codes\n",
    "        num_others = np.random.randint(3, 6)\n",
    "        codes.extend(np.random.choice(other_codes, num_others, replace=True))\n",
    "        \n",
    "        # Shuffle and add separators\n",
    "        np.random.shuffle(codes)\n",
    "        claims = ' '.join(codes[:2]) + ' |eoc| ' + ' '.join(codes[2:])\n",
    "        \n",
    "        test_data['claims'].append(claims)\n",
    "        test_data['label'].append(1 if has_target else 0)\n",
    "    \n",
    "    train_df = pd.DataFrame(train_data)\n",
    "    test_df = pd.DataFrame(test_data)\n",
    "    \n",
    "    print(f\"✓ Created training dataset: {len(train_df)} samples\")\n",
    "    print(f\"  - Positive samples: {sum(train_df['label'])} ({sum(train_df['label'])/len(train_df):.1%})\")\n",
    "    print(f\"✓ Created test dataset: {len(test_df)} samples\")\n",
    "    print(f\"  - Positive samples: {sum(test_df['label'])} ({sum(test_df['label'])/len(test_df):.1%})\")\n",
    "    \n",
    "    return {\n",
    "        'train': train_df,\n",
    "        'test': test_df\n",
    "    }\n",
    "\n",
    "test_datasets = create_evaluation_test_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Pipeline Initialization\n",
    "Test pipeline initialization and component validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation pipeline initialization\n",
    "def test_evaluation_pipeline_init(config: PipelineConfig):\n",
    "    \"\"\"Test evaluation pipeline initialization\"\"\"\n",
    "    print(\"Testing evaluation pipeline initialization...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize evaluation pipeline\n",
    "        eval_pipeline = EvaluationPipeline(config)\n",
    "        \n",
    "        print(\"✓ EvaluationPipeline initialized successfully\")\n",
    "        \n",
    "        # Check component initialization\n",
    "        components = []\n",
    "        \n",
    "        if hasattr(eval_pipeline, 'target_word_evaluator'):\n",
    "            components.append(\"TargetWordEvaluator\")\n",
    "        \n",
    "        if hasattr(eval_pipeline, 'classification_pipeline'):\n",
    "            components.append(\"ClassificationPipeline\")\n",
    "        \n",
    "        print(f\"  - Initialized components: {components}\")\n",
    "        \n",
    "        # Test configuration access\n",
    "        print(f\"  - Output directory: {eval_pipeline.output_dir if hasattr(eval_pipeline, 'output_dir') else 'Not set'}\")\n",
    "        print(f\"  - Logger configured: {hasattr(eval_pipeline, 'logger')}\")\n",
    "        \n",
    "        return eval_pipeline\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ EvaluationPipeline initialization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "evaluation_pipeline = test_evaluation_pipeline_init(test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test individual evaluator initialization\n",
    "def test_individual_evaluators(config: PipelineConfig):\n",
    "    \"\"\"Test individual evaluator components\"\"\"\n",
    "    print(\"Testing individual evaluator components...\")\n",
    "    \n",
    "    evaluators = {}\n",
    "    \n",
    "    # Test TargetWordEvaluator\n",
    "    if hasattr(config, 'evaluation') and config.evaluation and hasattr(config.evaluation, 'target_word'):\n",
    "        try:\n",
    "            target_evaluator = TargetWordEvaluator(config)\n",
    "            evaluators['target_word'] = target_evaluator\n",
    "            print(\"✓ TargetWordEvaluator initialized\")\n",
    "            print(f\"  - Target codes: {target_evaluator.target_codes}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ TargetWordEvaluator failed: {e}\")\n",
    "    \n",
    "    # Test ClassificationPipeline (for evaluation)\n",
    "    try:\n",
    "        classification_pipeline = ClassificationPipeline(config)\n",
    "        evaluators['classification'] = classification_pipeline\n",
    "        print(\"✓ ClassificationPipeline initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ ClassificationPipeline failed: {e}\")\n",
    "    \n",
    "    # Test EmbeddingPipeline (may be needed for evaluation)\n",
    "    try:\n",
    "        embedding_pipeline = EmbeddingPipeline(config)\n",
    "        evaluators['embedding'] = embedding_pipeline\n",
    "        print(\"✓ EmbeddingPipeline initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ EmbeddingPipeline failed: {e}\")\n",
    "    \n",
    "    return evaluators\n",
    "\n",
    "individual_evaluators = test_individual_evaluators(test_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Target Word Evaluation Testing\n",
    "Test target word evaluation process step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test target word evaluation process\n",
    "def test_target_word_evaluation_process(evaluator, test_data: pd.DataFrame, use_mock: bool = True):\n",
    "    \"\"\"Test target word evaluation on test dataset\"\"\"\n",
    "    print(\"Testing target word evaluation process...\")\n",
    "    \n",
    "    if not evaluator or 'target_word' not in evaluator:\n",
    "        print(\"✗ No target word evaluator available\")\n",
    "        return None\n",
    "    \n",
    "    target_evaluator = evaluator['target_word']\n",
    "    \n",
    "    try:\n",
    "        results = []\n",
    "        sample_size = min(10, len(test_data))  # Test with first 10 samples\n",
    "        \n",
    "        print(f\"Testing with {sample_size} samples...\")\n",
    "        \n",
    "        for idx in range(sample_size):\n",
    "            row = test_data.iloc[idx]\n",
    "            print(f\"\\n--- Sample {idx + 1}/{sample_size} ---\")\n",
    "            print(f\"MCID: {row['mcid']}\")\n",
    "            print(f\"Claims: {row['claims']}\")\n",
    "            print(f\"True label: {row['label']}\")\n",
    "            \n",
    "            # Mock evaluation process\n",
    "            if use_mock:\n",
    "                # Simulate target word evaluation\n",
    "                target_codes = target_evaluator.target_codes\n",
    "                claims_tokens = row['claims'].split()\n",
    "                \n",
    "                # Check if any target codes are in claims\n",
    "                found_targets = []\n",
    "                for token in claims_tokens:\n",
    "                    clean_token = token.strip('|eoc|').upper()\n",
    "                    if clean_token in [code.upper() for code in target_codes]:\n",
    "                        found_targets.append(clean_token)\n",
    "                \n",
    "                # Simulate generation attempts\n",
    "                generation_attempts = 3\n",
    "                mock_generations = [\n",
    "                    f\"GEN{idx}A GEN{idx}B |eoc| GEN{idx}C\",\n",
    "                    f\"GEN{idx}D E119 |eoc| GEN{idx}E\",  # Contains target\n",
    "                    f\"GEN{idx}F GEN{idx}G |eoc| Z03818\"  # Contains target\n",
    "                ]\n",
    "                \n",
    "                generated_targets = []\n",
    "                for i, gen in enumerate(mock_generations[:generation_attempts]):\n",
    "                    gen_tokens = gen.split()\n",
    "                    for token in gen_tokens:\n",
    "                        clean_token = token.strip('|eoc|').upper()\n",
    "                        if clean_token in [code.upper() for code in target_codes]:\n",
    "                            generated_targets.append({\n",
    "                                'attempt': i + 1,\n",
    "                                'code': clean_token,\n",
    "                                'generation': gen\n",
    "                            })\n",
    "                \n",
    "                prediction = 1 if generated_targets else 0\n",
    "                \n",
    "                result = {\n",
    "                    'mcid': row['mcid'],\n",
    "                    'claims': row['claims'],\n",
    "                    'true_label': row['label'],\n",
    "                    'prediction': prediction,\n",
    "                    'found_in_claims': found_targets,\n",
    "                    'generated_targets': generated_targets,\n",
    "                    'correct': prediction == row['label']\n",
    "                }\n",
    "                \n",
    "            else:\n",
    "                # Use actual evaluator (if API is available)\n",
    "                result = target_evaluator.evaluate_sample(row['claims'])\n",
    "                result['mcid'] = row['mcid']\n",
    "                result['true_label'] = row['label']\n",
    "                result['correct'] = result['prediction'] == row['label']\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"Prediction: {result['prediction']} | Correct: {result['correct']}\")\n",
    "            if result['generated_targets']:\n",
    "                print(f\"Generated targets: {[t['code'] for t in result['generated_targets']]}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        predictions = [r['prediction'] for r in results]\n",
    "        true_labels = [r['true_label'] for r in results]\n",
    "        correct = [r['correct'] for r in results]\n",
    "        \n",
    "        accuracy = sum(correct) / len(correct)\n",
    "        \n",
    "        # Calculate precision, recall, F1\n",
    "        tp = sum(p == 1 and t == 1 for p, t in zip(predictions, true_labels))\n",
    "        fp = sum(p == 1 and t == 0 for p, t in zip(predictions, true_labels))\n",
    "        fn = sum(p == 0 and t == 1 for p, t in zip(predictions, true_labels))\n",
    "        \n",
    "        precision = tp / max(tp + fp, 1)\n",
    "        recall = tp / max(tp + fn, 1)\n",
    "        f1 = 2 * precision * recall / max(precision + recall, 1e-10)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'total_samples': len(results),\n",
    "            'correct_predictions': sum(correct)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n=== Target Word Evaluation Metrics ===\")\n",
    "        print(f\"Accuracy:  {accuracy:.3f}\")\n",
    "        print(f\"Precision: {precision:.3f}\")\n",
    "        print(f\"Recall:    {recall:.3f}\")\n",
    "        print(f\"F1-Score:  {f1:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'method': 'target_word',\n",
    "            'results': results,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Target word evaluation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run target word evaluation\n",
    "if individual_evaluators and test_datasets:\n",
    "    target_word_results = test_target_word_evaluation_process(\n",
    "        individual_evaluators, \n",
    "        test_datasets['test'], \n",
    "        use_mock=True\n",
    "    )\nelse:\n",
    "    print(\"Skipping target word evaluation - components not available\")\n",
    "    target_word_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification-Based Evaluation Testing\n",
    "Test classification-based evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test classification-based evaluation\n",
    "def test_classification_evaluation_process(evaluators, train_data: pd.DataFrame, test_data: pd.DataFrame, use_mock: bool = True):\n",
    "    \"\"\"Test classification-based evaluation process\"\"\"\n",
    "    print(\"Testing classification-based evaluation process...\")\n",
    "    \n",
    "    if not evaluators or 'classification' not in evaluators:\n",
    "        print(\"✗ No classification pipeline available\")\n",
    "        return None\n",
    "    \n",
    "    classification_pipeline = evaluators['classification']\n",
    "    \n",
    "    try:\n",
    "        if use_mock:\n",
    "            print(\"Using mock classification process...\")\n",
    "            \n",
    "            # Mock the embeddings step\n",
    "            print(\"\\n1. Mock embedding generation...\")\n",
    "            train_embeddings = np.random.randn(len(train_data), 768)  # Mock 768-dim embeddings\n",
    "            test_embeddings = np.random.randn(len(test_data), 768)\n",
    "            print(f\"   Generated {train_embeddings.shape[0]} training embeddings\")\n",
    "            print(f\"   Generated {test_embeddings.shape[0]} test embeddings\")\n",
    "            \n",
    "            # Mock the training step\n",
    "            print(\"\\n2. Mock classifier training...\")\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "            \n",
    "            # Train classifier on mock embeddings\n",
    "            classifier = LogisticRegression(random_state=42)\n",
    "            classifier.fit(train_embeddings, train_data['label'])\n",
    "            print(\"   Classifier trained successfully\")\n",
    "            \n",
    "            # Mock the evaluation step\n",
    "            print(\"\\n3. Mock evaluation...\")\n",
    "            predictions = classifier.predict(test_embeddings)\n",
    "            probabilities = classifier.predict_proba(test_embeddings)[:, 1]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(test_data['label'], predictions)\n",
    "            precision = precision_score(test_data['label'], predictions, zero_division=0)\n",
    "            recall = recall_score(test_data['label'], predictions, zero_division=0)\n",
    "            f1 = f1_score(test_data['label'], predictions, zero_division=0)\n",
    "            \n",
    "            # Create detailed results\n",
    "            results = []\n",
    "            for idx, (pred, prob, true_label) in enumerate(zip(predictions, probabilities, test_data['label'])):\n",
    "                results.append({\n",
    "                    'mcid': test_data.iloc[idx]['mcid'],\n",
    "                    'claims': test_data.iloc[idx]['claims'],\n",
    "                    'true_label': true_label,\n",
    "                    'prediction': pred,\n",
    "                    'probability': prob,\n",
    "                    'correct': pred == true_label\n",
    "                })\n",
    "            \n",
    "            metrics = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'total_samples': len(test_data),\n",
    "                'correct_predictions': sum(predictions == test_data['label'])\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            # Use actual classification pipeline\n",
    "            print(\"Using actual classification pipeline...\")\n",
    "            \n",
    "            # This would involve:\n",
    "            # 1. Generating embeddings for train and test data\n",
    "            # 2. Training classifiers\n",
    "            # 3. Evaluating on test data\n",
    "            \n",
    "            results = classification_pipeline.evaluate(train_data, test_data)\n",
    "            metrics = results['metrics']\n",
    "        \n",
    "        print(f\"\\n=== Classification Evaluation Metrics ===\")\n",
    "        print(f\"Accuracy:  {metrics['accuracy']:.3f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "        print(f\"Recall:    {metrics['recall']:.3f}\")\n",
    "        print(f\"F1-Score:  {metrics['f1_score']:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'method': 'classification',\n",
    "            'results': results,\n",
    "            'metrics': metrics,\n",
    "            'model_type': 'LogisticRegression'\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Classification evaluation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run classification evaluation\n",
    "if individual_evaluators and test_datasets:\n",
    "    classification_results = test_classification_evaluation_process(\n",
    "        individual_evaluators,\n",
    "        test_datasets['train'],\n",
    "        test_datasets['test'],\n",
    "        use_mock=True\n",
    "    )\nelse:\n",
    "    print(\"Skipping classification evaluation - components not available\")\n",
    "    classification_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Results Comparison\n",
    "Compare results from different evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare evaluation methods\n",
    "def compare_evaluation_methods(target_results, classification_results):\n",
    "    \"\"\"Compare results from different evaluation methods\"\"\"\n",
    "    print(\"Comparing evaluation methods...\")\n",
    "    \n",
    "    if not target_results or not classification_results:\n",
    "        print(\"✗ Missing evaluation results for comparison\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Extract predictions\n",
    "        target_predictions = [r['prediction'] for r in target_results['results']]\n",
    "        classification_predictions = [r['prediction'] for r in classification_results['results']]\n",
    "        true_labels = [r['true_label'] for r in target_results['results']]\n",
    "        \n",
    "        # Ensure same number of samples\n",
    "        min_samples = min(len(target_predictions), len(classification_predictions))\n",
    "        target_predictions = target_predictions[:min_samples]\n",
    "        classification_predictions = classification_predictions[:min_samples]\n",
    "        true_labels = true_labels[:min_samples]\n",
    "        \n",
    "        print(f\"\\n=== Method Comparison ({min_samples} samples) ===\")\n",
    "        \n",
    "        # Method agreement\n",
    "        agreements = sum(t == c for t, c in zip(target_predictions, classification_predictions))\n",
    "        agreement_rate = agreements / min_samples\n",
    "        \n",
    "        print(f\"Agreement between methods: {agreements}/{min_samples} ({agreement_rate:.3f})\")\n",
    "        \n",
    "        # Performance comparison\n",
    "        target_metrics = target_results['metrics']\n",
    "        classification_metrics = classification_results['metrics']\n",
    "        \n",
    "        print(f\"\\nMetrics Comparison:\")\n",
    "        print(f\"                Target Word   Classification\")\n",
    "        print(f\"Accuracy:       {target_metrics['accuracy']:.3f}         {classification_metrics['accuracy']:.3f}\")\n",
    "        print(f\"Precision:      {target_metrics['precision']:.3f}         {classification_metrics['precision']:.3f}\")\n",
    "        print(f\"Recall:         {target_metrics['recall']:.3f}         {classification_metrics['recall']:.3f}\")\n",
    "        print(f\"F1-Score:       {target_metrics['f1_score']:.3f}         {classification_metrics['f1_score']:.3f}\")\n",
    "        \n",
    "        # Analyze disagreements\n",
    "        disagreements = []\n",
    "        for i, (t, c, true_label) in enumerate(zip(target_predictions, classification_predictions, true_labels)):\n",
    "            if t != c:\n",
    "                disagreements.append({\n",
    "                    'sample_index': i,\n",
    "                    'mcid': target_results['results'][i]['mcid'],\n",
    "                    'true_label': true_label,\n",
    "                    'target_prediction': t,\n",
    "                    'classification_prediction': c,\n",
    "                    'target_correct': t == true_label,\n",
    "                    'classification_correct': c == true_label\n",
    "                })\n",
    "        \n",
    "        if disagreements:\n",
    "            print(f\"\\nDisagreements ({len(disagreements)} samples):\")\n",
    "            for d in disagreements[:5]:  # Show first 5\n",
    "                print(f\"  {d['mcid']}: True={d['true_label']}, Target={d['target_prediction']}, Class={d['classification_prediction']}\")\n",
    "            if len(disagreements) > 5:\n",
    "                print(f\"  ... and {len(disagreements) - 5} more\")\n",
    "        else:\n",
    "            print(\"\\nNo disagreements - methods agree on all samples\")\n",
    "        \n",
    "        # Performance by agreement\n",
    "        agree_correct = sum(1 for t, c, true in zip(target_predictions, classification_predictions, true_labels)\n",
    "                          if t == c and t == true)\n",
    "        disagree_target_correct = sum(1 for d in disagreements if d['target_correct'])\n",
    "        disagree_class_correct = sum(1 for d in disagreements if d['classification_correct'])\n",
    "        \n",
    "        print(f\"\\nPerformance Analysis:\")\n",
    "        print(f\"Both methods agree and correct: {agree_correct}/{agreements} ({agree_correct/max(agreements,1):.3f})\")\n",
    "        if disagreements:\n",
    "            print(f\"Target method correct in disagreements: {disagree_target_correct}/{len(disagreements)} ({disagree_target_correct/len(disagreements):.3f})\")\n",
    "            print(f\"Classification correct in disagreements: {disagree_class_correct}/{len(disagreements)} ({disagree_class_correct/len(disagreements):.3f})\")\n",
    "        \n",
    "        comparison_results = {\n",
    "            'agreement_rate': agreement_rate,\n",
    "            'total_samples': min_samples,\n",
    "            'agreements': agreements,\n",
    "            'disagreements': disagreements,\n",
    "            'target_metrics': target_metrics,\n",
    "            'classification_metrics': classification_metrics\n",
    "        }\n",
    "        \n",
    "        return comparison_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Method comparison failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run comparison\n",
    "if target_word_results and classification_results:\n",
    "    method_comparison = compare_evaluation_methods(target_word_results, classification_results)\nelse:\n",
    "    print(\"Skipping method comparison - results not available\")\n",
    "    method_comparison = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Saving and Reporting\n",
    "Test results saving and report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test results saving functionality\n",
    "def test_results_saving(evaluation_results: Dict, output_dir: str = \"outputs/test_evaluation\"):\n",
    "    \"\"\"Test saving evaluation results to files\"\"\"\n",
    "    print(\"Testing results saving functionality...\")\n",
    "    \n",
    "    try:\n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"✓ Output directory created: {output_dir}\")\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        saved_files = []\n",
    "        \n",
    "        # Save target word results\n",
    "        if 'target_word' in evaluation_results and evaluation_results['target_word']:\n",
    "            target_results = evaluation_results['target_word']\n",
    "            \n",
    "            # Save detailed results\n",
    "            details_file = os.path.join(output_dir, f\"target_word_details_{timestamp}.json\")\n",
    "            with open(details_file, 'w') as f:\n",
    "                json.dump(target_results, f, indent=2, default=str)\n",
    "            saved_files.append(details_file)\n",
    "            \n",
    "            # Save summary metrics\n",
    "            summary_file = os.path.join(output_dir, f\"target_word_summary_{timestamp}.json\")\n",
    "            with open(summary_file, 'w') as f:\n",
    "                json.dump({\n",
    "                    'method': 'target_word',\n",
    "                    'timestamp': timestamp,\n",
    "                    'metrics': target_results['metrics'],\n",
    "                    'total_samples': len(target_results['results'])\n",
    "                }, f, indent=2)\n",
    "            saved_files.append(summary_file)\n",
    "            \n",
    "            # Save CSV for analysis\n",
    "            csv_file = os.path.join(output_dir, f\"target_word_predictions_{timestamp}.csv\")\n",
    "            results_df = pd.DataFrame(target_results['results'])\n",
    "            results_df.to_csv(csv_file, index=False)\n",
    "            saved_files.append(csv_file)\n",
    "            \n",
    "            print(f\"✓ Target word results saved ({len(target_results['results'])} samples)\")\n",
    "        \n",
    "        # Save classification results\n",
    "        if 'classification' in evaluation_results and evaluation_results['classification']:\n",
    "            class_results = evaluation_results['classification']\n",
    "            \n",
    "            details_file = os.path.join(output_dir, f\"classification_details_{timestamp}.json\")\n",
    "            with open(details_file, 'w') as f:\n",
    "                json.dump(class_results, f, indent=2, default=str)\n",
    "            saved_files.append(details_file)\n",
    "            \n",
    "            summary_file = os.path.join(output_dir, f\"classification_summary_{timestamp}.json\")\n",
    "            with open(summary_file, 'w') as f:\n",
    "                json.dump({\n",
    "                    'method': 'classification',\n",
    "                    'timestamp': timestamp,\n",
    "                    'metrics': class_results['metrics'],\n",
    "                    'model_type': class_results.get('model_type', 'unknown'),\n",
    "                    'total_samples': len(class_results['results'])\n",
    "                }, f, indent=2)\n",
    "            saved_files.append(summary_file)\n",
    "            \n",
    "            csv_file = os.path.join(output_dir, f\"classification_predictions_{timestamp}.csv\")\n",
    "            results_df = pd.DataFrame(class_results['results'])\n",
    "            results_df.to_csv(csv_file, index=False)\n",
    "            saved_files.append(csv_file)\n",
    "            \n",
    "            print(f\"✓ Classification results saved ({len(class_results['results'])} samples)\")\n",
    "        \n",
    "        # Save comparison results\n",
    "        if 'comparison' in evaluation_results and evaluation_results['comparison']:\n",
    "            comparison_file = os.path.join(output_dir, f\"method_comparison_{timestamp}.json\")\n",
    "            with open(comparison_file, 'w') as f:\n",
    "                json.dump(evaluation_results['comparison'], f, indent=2, default=str)\n",
    "            saved_files.append(comparison_file)\n",
    "            print(f\"✓ Method comparison saved\")\n",
    "        \n",
    "        # Create evaluation report\n",
    "        report_file = os.path.join(output_dir, f\"evaluation_report_{timestamp}.md\")\n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(f\"# Evaluation Report\\n\\n\")\n",
    "            f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            if 'target_word' in evaluation_results:\n",
    "                metrics = evaluation_results['target_word']['metrics']\n",
    "                f.write(f\"## Target Word Evaluation\\n\\n\")\n",
    "                f.write(f\"- **Accuracy:** {metrics['accuracy']:.3f}\\n\")\n",
    "                f.write(f\"- **Precision:** {metrics['precision']:.3f}\\n\")\n",
    "                f.write(f\"- **Recall:** {metrics['recall']:.3f}\\n\")\n",
    "                f.write(f\"- **F1-Score:** {metrics['f1_score']:.3f}\\n\")\n",
    "                f.write(f\"- **Total Samples:** {metrics['total_samples']}\\n\\n\")\n",
    "            \n",
    "            if 'classification' in evaluation_results:\n",
    "                metrics = evaluation_results['classification']['metrics']\n",
    "                f.write(f\"## Classification Evaluation\\n\\n\")\n",
    "                f.write(f\"- **Accuracy:** {metrics['accuracy']:.3f}\\n\")\n",
    "                f.write(f\"- **Precision:** {metrics['precision']:.3f}\\n\")\n",
    "                f.write(f\"- **Recall:** {metrics['recall']:.3f}\\n\")\n",
    "                f.write(f\"- **F1-Score:** {metrics['f1_score']:.3f}\\n\")\n",
    "                f.write(f\"- **Total Samples:** {metrics['total_samples']}\\n\\n\")\n",
    "            \n",
    "            if 'comparison' in evaluation_results:\n",
    "                comp = evaluation_results['comparison']\n",
    "                f.write(f\"## Method Comparison\\n\\n\")\n",
    "                f.write(f\"- **Agreement Rate:** {comp['agreement_rate']:.3f}\\n\")\n",
    "                f.write(f\"- **Agreements:** {comp['agreements']}/{comp['total_samples']}\\n\")\n",
    "                f.write(f\"- **Disagreements:** {len(comp['disagreements'])}\\n\\n\")\n",
    "        \n",
    "        saved_files.append(report_file)\n",
    "        print(f\"✓ Evaluation report generated\")\n",
    "        \n",
    "        print(f\"\\n=== Files Saved ({len(saved_files)} files) ===\")\n",
    "        for file_path in saved_files:\n",
    "            file_size = os.path.getsize(file_path) / 1024  # KB\n",
    "            print(f\"  {os.path.basename(file_path)} ({file_size:.1f} KB)\")\n",
    "        \n",
    "        return {\n",
    "            'output_dir': output_dir,\n",
    "            'saved_files': saved_files,\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Results saving failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Collect all evaluation results\n",
    "all_evaluation_results = {}\n",
    "if target_word_results:\n",
    "    all_evaluation_results['target_word'] = target_word_results\n",
    "if classification_results:\n",
    "    all_evaluation_results['classification'] = classification_results\n",
    "if method_comparison:\n",
    "    all_evaluation_results['comparison'] = method_comparison\n",
    "\n",
    "# Save results\n",
    "if all_evaluation_results:\n",
    "    save_results = test_results_saving(all_evaluation_results)\nelse:\n",
    "    print(\"No evaluation results to save\")\n",
    "    save_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance and Memory Analysis\n",
    "Test evaluation pipeline performance characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation pipeline performance\n",
    "def test_evaluation_performance(evaluators, sample_size: int = 20):\n",
    "    \"\"\"Test evaluation pipeline performance and memory usage\"\"\"\n",
    "    print(f\"Testing evaluation pipeline performance ({sample_size} samples)...\")\n",
    "    \n",
    "    if not evaluators:\n",
    "        print(\"✗ No evaluators available\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Generate test samples\n",
    "        test_samples = []\n",
    "        for i in range(sample_size):\n",
    "            codes = [f\"CODE{i:03d}\", f\"TEST{i:03d}\", \"E119\", \"Z03818\"]\n",
    "            np.random.shuffle(codes)\n",
    "            claims = ' '.join(codes[:2]) + ' |eoc| ' + ' '.join(codes[2:])\n",
    "            test_samples.append({\n",
    "                'mcid': f'PERF_{i:04d}',\n",
    "                'claims': claims,\n",
    "                'label': np.random.choice([0, 1])\n",
    "            })\n",
    "        \n",
    "        performance_results = {}\n",
    "        \n",
    "        # Test target word evaluation performance\n",
    "        if 'target_word' in evaluators:\n",
    "            print(\"\\nTesting target word evaluation performance...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for sample in test_samples[:min(10, sample_size)]:\n",
    "                # Mock evaluation (faster than real API calls)\n",
    "                time.sleep(0.01)  # Simulate processing time\n",
    "            \n",
    "            target_time = time.time() - start_time\n",
    "            target_rate = min(10, sample_size) / target_time\n",
    "            \n",
    "            performance_results['target_word'] = {\n",
    "                'total_time': target_time,\n",
    "                'samples_per_second': target_rate,\n",
    "                'avg_time_per_sample': target_time / min(10, sample_size)\n",
    "            }\n",
    "            \n",
    "            print(f\"  Time: {target_time:.2f}s\")\n",
    "            print(f\"  Rate: {target_rate:.2f} samples/sec\")\n",
    "        \n",
    "        # Test classification evaluation performance\n",
    "        if 'classification' in evaluators:\n",
    "            print(\"\\nTesting classification evaluation performance...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Mock embedding generation and classification\n",
    "            mock_embeddings = np.random.randn(sample_size, 768)\n",
    "            \n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            classifier = LogisticRegression()\n",
    "            \n",
    "            # Mock training\n",
    "            train_embeddings = np.random.randn(100, 768)\n",
    "            train_labels = np.random.choice([0, 1], 100)\n",
    "            classifier.fit(train_embeddings, train_labels)\n",
    "            \n",
    "            # Mock prediction\n",
    "            predictions = classifier.predict(mock_embeddings)\n",
    "            \n",
    "            class_time = time.time() - start_time\n",
    "            class_rate = sample_size / class_time\n",
    "            \n",
    "            performance_results['classification'] = {\n",
    "                'total_time': class_time,\n",
    "                'samples_per_second': class_rate,\n",
    "                'avg_time_per_sample': class_time / sample_size\n",
    "            }\n",
    "            \n",
    "            print(f\"  Time: {class_time:.2f}s\")\n",
    "            print(f\"  Rate: {class_rate:.2f} samples/sec\")\n",
    "        \n",
    "        # Memory usage analysis\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process()\n",
    "            memory_info = process.memory_info()\n",
    "            \n",
    "            performance_results['memory'] = {\n",
    "                'rss_mb': memory_info.rss / 1024 / 1024,\n",
    "                'vms_mb': memory_info.vms / 1024 / 1024\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nMemory Usage:\")\n",
    "            print(f\"  RSS: {performance_results['memory']['rss_mb']:.1f} MB\")\n",
    "            print(f\"  VMS: {performance_results['memory']['vms_mb']:.1f} MB\")\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"\\npsutil not available for memory monitoring\")\n",
    "        \n",
    "        print(f\"\\n=== Performance Summary ===\")\n",
    "        for method, stats in performance_results.items():\n",
    "            if method != 'memory':\n",
    "                print(f\"{method.title()}: {stats['samples_per_second']:.2f} samples/sec\")\n",
    "        \n",
    "        return performance_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Performance testing failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run performance test\n",
    "if individual_evaluators:\n",
    "    performance_stats = test_evaluation_performance(individual_evaluators, sample_size=20)\nelse:\n",
    "    print(\"Skipping performance test - evaluators not available\")\n",
    "    performance_stats = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Custom Debugging Section\n",
    "Use this section for custom debugging and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom debugging - modify as needed\n",
    "print(\"=== Custom Debugging Section ===\")\n",
    "print(\"Use this cell for your own debugging and testing\")\n",
    "\n",
    "# Example: Debug specific evaluation scenarios\n",
    "debug_scenarios = [\n",
    "    {\n",
    "        'name': 'High confidence positive',\n",
    "        'claims': 'E119 Z03818 |eoc| N6320 M1710',\n",
    "        'expected_label': 1\n",
    "    },\n",
    "    {\n",
    "        'name': 'Clear negative',\n",
    "        'claims': 'A1234 B5678 |eoc| C9012 D3456',\n",
    "        'expected_label': 0\n",
    "    },\n",
    "    {\n",
    "        'name': 'Borderline case',\n",
    "        'claims': 'E119X A1234 |eoc| Z03818Y B5678',  # Similar but not exact\n",
    "        'expected_label': 0\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nTesting debug scenarios:\")\n",
    "for scenario in debug_scenarios:\n",
    "    print(f\"\\n--- {scenario['name']} ---\")\n",
    "    print(f\"Claims: {scenario['claims']}\")\n",
    "    print(f\"Expected: {scenario['expected_label']}\")\n",
    "    \n",
    "    # Add your debugging logic here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug specific configuration issues\n",
    "print(\"=== Configuration Debug ===\")\n",
    "\n",
    "# Add your configuration debugging code here\n",
    "# Examples:\n",
    "# - Test different target code combinations\n",
    "# - Debug output directory creation\n",
    "# - Test API connectivity issues\n",
    "# - Validate model loading\n",
    "\n",
    "print(\"Add your custom configuration debugging code in this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides comprehensive testing for the evaluation pipeline:\n",
    "\n",
    "1. **Configuration Testing**: Validates evaluation pipeline configuration\n",
    "2. **Data Preparation**: Creates comprehensive test datasets\n",
    "3. **Pipeline Initialization**: Tests component initialization and setup\n",
    "4. **Target Word Evaluation**: Tests target word evaluation process\n",
    "5. **Classification Evaluation**: Tests classification-based evaluation\n",
    "6. **Method Comparison**: Compares different evaluation approaches\n",
    "7. **Results Saving**: Tests results persistence and reporting\n",
    "8. **Performance Analysis**: Measures performance and memory usage\n",
    "9. **Custom Debugging**: Space for your own testing\n",
    "\n",
    "Use this notebook to:\n",
    "- Debug end-to-end evaluation workflows\n",
    "- Compare evaluation methods\n",
    "- Validate evaluation metrics\n",
    "- Test results saving and reporting\n",
    "- Performance optimization\n",
    "\n",
    "Modify the test cases and parameters as needed for your specific debugging requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}