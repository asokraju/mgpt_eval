{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Validation Testing\n",
    "\n",
    "This notebook tests configuration validation and management systems.\n",
    "Use this to debug configuration issues and validate config file structures.\n",
    "\n",
    "## Overview\n",
    "- Test configuration file loading and validation\n",
    "- Debug configuration structure and schema issues\n",
    "- Test configuration merging and overrides\n",
    "- Validate configuration compatibility across components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "from datetime import datetime\n",
    "import copy\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('/home/kosaraju/mgpt-serve/mgpt_eval')\n",
    "\n",
    "# Import configuration models\n",
    "from models.config_models import (\n",
    "    PipelineConfig, JobConfig, InputConfig, APIConfig,\n",
    "    EmbeddingConfig, ClassificationConfig, EvaluationConfig,\n",
    "    TargetWordConfig\n",
    ")\n",
    "from models.data_models import DataSample, DataBatch\n",
    "from pydantic import ValidationError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration File Discovery and Loading\n",
    "Test loading and parsing of configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover and test configuration files\n",
    "def discover_configuration_files():\n",
    "    \"\"\"Discover all configuration files in the project\"\"\"\n",
    "    print(\"Discovering configuration files...\")\n",
    "    \n",
    "    config_directories = [\n",
    "        '/home/kosaraju/mgpt-serve/mgpt_eval/configs',\n",
    "        '/home/kosaraju/mgpt-serve/mgpt_eval/configs/examples',\n",
    "        '/home/kosaraju/mgpt-serve/mgpt_eval/configs/templates',\n",
    "        '/home/kosaraju/mgpt-serve/mgpt_eval/examples/configs'\n",
    "    ]\n",
    "    \n",
    "    discovered_configs = []\n",
    "    \n",
    "    for config_dir in config_directories:\n",
    "        if os.path.exists(config_dir):\n",
    "            print(f\"\\nScanning directory: {config_dir}\")\n",
    "            \n",
    "            for file_path in Path(config_dir).glob('*.yaml'):\n",
    "                config_info = {\n",
    "                    'path': str(file_path),\n",
    "                    'name': file_path.name,\n",
    "                    'directory': config_dir,\n",
    "                    'size': file_path.stat().st_size,\n",
    "                    'modified': datetime.fromtimestamp(file_path.stat().st_mtime)\n",
    "                }\n",
    "                discovered_configs.append(config_info)\n",
    "                print(f\"  Found: {file_path.name} ({config_info['size']} bytes)\")\n",
    "        else:\n",
    "            print(f\"  Directory not found: {config_dir}\")\n",
    "    \n",
    "    print(f\"\\n✓ Discovered {len(discovered_configs)} configuration files\")\n",
    "    return discovered_configs\n",
    "\n",
    "# Test configuration file loading\n",
    "def test_config_file_loading(config_files: List[Dict]):\n",
    "    \"\"\"Test loading and basic parsing of configuration files\"\"\"\n",
    "    print(\"\\nTesting configuration file loading...\")\n",
    "    \n",
    "    loading_results = []\n",
    "    \n",
    "    for config_info in config_files:\n",
    "        print(f\"\\n--- Testing {config_info['name']} ---\")\n",
    "        \n",
    "        result = {\n",
    "            'file': config_info['name'],\n",
    "            'path': config_info['path'],\n",
    "            'yaml_valid': False,\n",
    "            'pydantic_valid': False,\n",
    "            'yaml_error': None,\n",
    "            'pydantic_error': None,\n",
    "            'config_object': None,\n",
    "            'structure': {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Test YAML parsing\n",
    "            with open(config_info['path'], 'r') as f:\n",
    "                yaml_content = yaml.safe_load(f)\n",
    "            \n",
    "            result['yaml_valid'] = True\n",
    "            result['structure'] = {\n",
    "                'top_level_keys': list(yaml_content.keys()) if isinstance(yaml_content, dict) else [],\n",
    "                'total_keys': len(yaml_content) if isinstance(yaml_content, dict) else 0,\n",
    "                'content_type': type(yaml_content).__name__\n",
    "            }\n",
    "            \n",
    "            print(f\"  ✓ YAML parsing successful\")\n",
    "            print(f\"    Top-level keys: {result['structure']['top_level_keys']}\")\n",
    "            \n",
    "            # Test Pydantic validation\n",
    "            try:\n",
    "                config_obj = PipelineConfig.from_yaml(config_info['path'])\n",
    "                result['pydantic_valid'] = True\n",
    "                result['config_object'] = config_obj\n",
    "                print(f\"  ✓ Pydantic validation successful\")\n",
    "                \n",
    "                # Analyze configuration components\n",
    "                components = []\n",
    "                if hasattr(config_obj, 'job') and config_obj.job:\n",
    "                    components.append('job')\n",
    "                if hasattr(config_obj, 'input') and config_obj.input:\n",
    "                    components.append('input')\n",
    "                if hasattr(config_obj, 'api') and config_obj.api:\n",
    "                    components.append('api')\n",
    "                if hasattr(config_obj, 'embedding') and config_obj.embedding:\n",
    "                    components.append('embedding')\n",
    "                if hasattr(config_obj, 'classification') and config_obj.classification:\n",
    "                    components.append('classification')\n",
    "                if hasattr(config_obj, 'evaluation') and config_obj.evaluation:\n",
    "                    components.append('evaluation')\n",
    "                \n",
    "                result['structure']['components'] = components\n",
    "                print(f\"    Components: {components}\")\n",
    "                \n",
    "            except ValidationError as e:\n",
    "                result['pydantic_error'] = str(e)\n",
    "                print(f\"  ✗ Pydantic validation failed: {e}\")\n",
    "            except Exception as e:\n",
    "                result['pydantic_error'] = str(e)\n",
    "                print(f\"  ✗ Pydantic error: {e}\")\n",
    "            \n",
    "        except yaml.YAMLError as e:\n",
    "            result['yaml_error'] = str(e)\n",
    "            print(f\"  ✗ YAML parsing failed: {e}\")\n",
    "        except Exception as e:\n",
    "            result['yaml_error'] = str(e)\n",
    "            print(f\"  ✗ File loading error: {e}\")\n",
    "        \n",
    "        loading_results.append(result)\n",
    "    \n",
    "    # Summary\n",
    "    yaml_valid_count = sum(1 for r in loading_results if r['yaml_valid'])\n",
    "    pydantic_valid_count = sum(1 for r in loading_results if r['pydantic_valid'])\n",
    "    \n",
    "    print(f\"\\n=== Configuration Loading Summary ===\")\n",
    "    print(f\"Total files tested: {len(loading_results)}\")\n",
    "    print(f\"YAML valid: {yaml_valid_count}/{len(loading_results)}\")\n",
    "    print(f\"Pydantic valid: {pydantic_valid_count}/{len(loading_results)}\")\n",
    "    \n",
    "    return loading_results\n",
    "\n",
    "# Run configuration discovery and loading tests\n",
    "config_files = discover_configuration_files()\n",
    "loading_results = test_config_file_loading(config_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Schema Validation\n",
    "Test detailed schema validation and field requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration schema validation in detail\n",
    "def test_configuration_schema_validation(loading_results: List[Dict]):\n",
    "    \"\"\"Test detailed schema validation for configurations\"\"\"\n",
    "    print(\"Testing configuration schema validation...\")\n",
    "    \n",
    "    schema_validation_results = []\n",
    "    \n",
    "    for result in loading_results:\n",
    "        if not result['pydantic_valid']:\n",
    "            continue\n",
    "        \n",
    "        config = result['config_object']\n",
    "        print(f\"\\n--- Schema validation for {result['file']} ---\")\n",
    "        \n",
    "        validation_result = {\n",
    "            'file': result['file'],\n",
    "            'required_fields': {},\n",
    "            'optional_fields': {},\n",
    "            'field_types': {},\n",
    "            'validation_issues': [],\n",
    "            'completeness_score': 0\n",
    "        }\n",
    "        \n",
    "        # Test job configuration\n",
    "        if hasattr(config, 'job') and config.job:\n",
    "            job = config.job\n",
    "            print(f\"  ✓ Job configuration present\")\n",
    "            \n",
    "            job_fields = {\n",
    "                'job_name': getattr(job, 'job_name', None),\n",
    "                'output_base_dir': getattr(job, 'output_base_dir', None),\n",
    "                'split_ratio': getattr(job, 'split_ratio', None),\n",
    "                'random_seed': getattr(job, 'random_seed', None)\n",
    "            }\n",
    "            \n",
    "            validation_result['required_fields']['job'] = job_fields\n",
    "            \n",
    "            # Validate specific fields\n",
    "            if not job_fields['job_name']:\n",
    "                validation_result['validation_issues'].append('Job name is missing or empty')\n",
    "            \n",
    "            if job_fields['split_ratio'] and not (0 < job_fields['split_ratio'] < 1):\n",
    "                validation_result['validation_issues'].append(f\"Invalid split ratio: {job_fields['split_ratio']}\")\n",
    "        else:\n",
    "            validation_result['validation_issues'].append('Job configuration missing')\n",
    "        \n",
    "        # Test input configuration\n",
    "        if hasattr(config, 'input') and config.input:\n",
    "            input_config = config.input\n",
    "            print(f\"  ✓ Input configuration present\")\n",
    "            \n",
    "            input_fields = {\n",
    "                'dataset_path': getattr(input_config, 'dataset_path', None),\n",
    "                'train_dataset_path': getattr(input_config, 'train_dataset_path', None),\n",
    "                'test_dataset_path': getattr(input_config, 'test_dataset_path', None)\n",
    "            }\n",
    "            \n",
    "            validation_result['required_fields']['input'] = input_fields\n",
    "            \n",
    "            # Validate input data specification\n",
    "            has_single_dataset = bool(input_fields['dataset_path'])\n",
    "            has_split_datasets = bool(input_fields['train_dataset_path'] and input_fields['test_dataset_path'])\n",
    "            \n",
    "            if not (has_single_dataset or has_split_datasets):\n",
    "                validation_result['validation_issues'].append('No valid input data specification found')\n",
    "            elif has_single_dataset and has_split_datasets:\n",
    "                validation_result['validation_issues'].append('Both single dataset and split datasets specified')\n",
    "        else:\n",
    "            validation_result['validation_issues'].append('Input configuration missing')\n",
    "        \n",
    "        # Test API configuration\n",
    "        if hasattr(config, 'api') and config.api:\n",
    "            api = config.api\n",
    "            print(f\"  ✓ API configuration present\")\n",
    "            \n",
    "            api_fields = {\n",
    "                'host': getattr(api, 'host', None),\n",
    "                'port': getattr(api, 'port', None)\n",
    "            }\n",
    "            \n",
    "            validation_result['required_fields']['api'] = api_fields\n",
    "            \n",
    "            # Validate API fields\n",
    "            if not api_fields['host']:\n",
    "                validation_result['validation_issues'].append('API host is missing')\n",
    "            \n",
    "            if not api_fields['port'] or not isinstance(api_fields['port'], int):\n",
    "                validation_result['validation_issues'].append('API port is missing or invalid')\n",
    "        else:\n",
    "            validation_result['validation_issues'].append('API configuration missing')\n",
    "        \n",
    "        # Test embedding configuration\n",
    "        if hasattr(config, 'embedding') and config.embedding:\n",
    "            embedding = config.embedding\n",
    "            print(f\"  ✓ Embedding configuration present\")\n",
    "            \n",
    "            embedding_fields = {\n",
    "                'batch_size': getattr(embedding, 'batch_size', None),\n",
    "                'save_embeddings': getattr(embedding, 'save_embeddings', None),\n",
    "                'embeddings_file': getattr(embedding, 'embeddings_file', None)\n",
    "            }\n",
    "            \n",
    "            validation_result['optional_fields']['embedding'] = embedding_fields\n",
    "        \n",
    "        # Test classification configuration\n",
    "        if hasattr(config, 'classification') and config.classification:\n",
    "            classification = config.classification\n",
    "            print(f\"  ✓ Classification configuration present\")\n",
    "            \n",
    "            classification_fields = {\n",
    "                'models': getattr(classification, 'models', None),\n",
    "                'cross_validation_folds': getattr(classification, 'cross_validation_folds', None),\n",
    "                'hyperparameter_search': getattr(classification, 'hyperparameter_search', None)\n",
    "            }\n",
    "            \n",
    "            validation_result['optional_fields']['classification'] = classification_fields\n",
    "            \n",
    "            # Validate classification models\n",
    "            if classification_fields['models']:\n",
    "                valid_models = ['logistic_regression', 'svm', 'random_forest', 'gradient_boosting']\n",
    "                for model in classification_fields['models']:\n",
    "                    if model not in valid_models:\n",
    "                        validation_result['validation_issues'].append(f\"Unknown classification model: {model}\")\n",
    "        \n",
    "        # Test evaluation configuration\n",
    "        if hasattr(config, 'evaluation') and config.evaluation:\n",
    "            evaluation = config.evaluation\n",
    "            print(f\"  ✓ Evaluation configuration present\")\n",
    "            \n",
    "            evaluation_fields = {}\n",
    "            \n",
    "            # Test target word evaluation\n",
    "            if hasattr(evaluation, 'target_word') and evaluation.target_word:\n",
    "                target_word = evaluation.target_word\n",
    "                target_word_fields = {\n",
    "                    'target_codes': getattr(target_word, 'target_codes', None),\n",
    "                    'max_tokens': getattr(target_word, 'max_tokens', None),\n",
    "                    'generation_attempts': getattr(target_word, 'generation_attempts', None)\n",
    "                }\n",
    "                \n",
    "                evaluation_fields['target_word'] = target_word_fields\n",
    "                \n",
    "                # Validate target codes\n",
    "                if not target_word_fields['target_codes']:\n",
    "                    validation_result['validation_issues'].append('Target codes are required for target word evaluation')\n",
    "                elif len(target_word_fields['target_codes']) == 0:\n",
    "                    validation_result['validation_issues'].append('Target codes list is empty')\n",
    "            \n",
    "            validation_result['optional_fields']['evaluation'] = evaluation_fields\n",
    "        \n",
    "        # Calculate completeness score\n",
    "        required_components = ['job', 'input', 'api']\n",
    "        present_required = sum(1 for comp in required_components \n",
    "                             if comp in validation_result['required_fields'])\n",
    "        \n",
    "        optional_components = ['embedding', 'classification', 'evaluation']\n",
    "        present_optional = sum(1 for comp in optional_components \n",
    "                             if comp in validation_result['optional_fields'])\n",
    "        \n",
    "        completeness = (present_required / len(required_components)) * 0.7 + \\\n",
    "                      (present_optional / len(optional_components)) * 0.3\n",
    "        validation_result['completeness_score'] = completeness\n",
    "        \n",
    "        print(f\"  Completeness score: {completeness:.2f}\")\n",
    "        if validation_result['validation_issues']:\n",
    "            print(f\"  Issues: {len(validation_result['validation_issues'])}\")\n",
    "            for issue in validation_result['validation_issues'][:3]:  # Show first 3\n",
    "                print(f\"    - {issue}\")\n",
    "        \n",
    "        schema_validation_results.append(validation_result)\n",
    "    \n",
    "    print(f\"\\n=== Schema Validation Summary ===\")\n",
    "    if schema_validation_results:\n",
    "        avg_completeness = sum(r['completeness_score'] for r in schema_validation_results) / len(schema_validation_results)\n",
    "        total_issues = sum(len(r['validation_issues']) for r in schema_validation_results)\n",
    "        \n",
    "        print(f\"Configurations validated: {len(schema_validation_results)}\")\n",
    "        print(f\"Average completeness: {avg_completeness:.2f}\")\n",
    "        print(f\"Total validation issues: {total_issues}\")\n",
    "    else:\n",
    "        print(\"No valid configurations to validate\")\n",
    "    \n",
    "    return schema_validation_results\n",
    "\n",
    "# Run schema validation\n",
    "schema_results = test_configuration_schema_validation(loading_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration Template Testing\n",
    "Test configuration templates and example configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration templates\n",
    "def test_configuration_templates():\n",
    "    \"\"\"Test and validate configuration templates\"\"\"\n",
    "    print(\"Testing configuration templates...\")\n",
    "    \n",
    "    # Define expected template types and their characteristics\n",
    "    expected_templates = {\n",
    "        'embeddings_only': {\n",
    "            'required_components': ['job', 'input', 'api', 'embedding'],\n",
    "            'optional_components': [],\n",
    "            'description': 'Generate embeddings only'\n",
    "        },\n",
    "        'from_embeddings': {\n",
    "            'required_components': ['job', 'input', 'classification'],\n",
    "            'optional_components': ['embedding'],\n",
    "            'description': 'Train classifiers from existing embeddings'\n",
    "        },\n",
    "        'target_words_only': {\n",
    "            'required_components': ['job', 'input', 'api', 'evaluation'],\n",
    "            'optional_components': [],\n",
    "            'description': 'Target word evaluation only'\n",
    "        },\n",
    "        'full_pipeline': {\n",
    "            'required_components': ['job', 'input', 'api', 'embedding', 'classification', 'evaluation'],\n",
    "            'optional_components': [],\n",
    "            'description': 'Complete end-to-end pipeline'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    template_test_results = []\n",
    "    \n",
    "    # Test each template type\n",
    "    for template_name, template_spec in expected_templates.items():\n",
    "        print(f\"\\n--- Testing {template_name} template ---\")\n",
    "        print(f\"Description: {template_spec['description']}\")\n",
    "        \n",
    "        # Find matching configuration files\n",
    "        matching_configs = []\n",
    "        for result in loading_results:\n",
    "            if not result['pydantic_valid']:\n",
    "                continue\n",
    "            \n",
    "            file_name_lower = result['file'].lower()\n",
    "            if any(keyword in file_name_lower for keyword in template_name.split('_')):\n",
    "                matching_configs.append(result)\n",
    "        \n",
    "        if not matching_configs:\n",
    "            print(f\"  ✗ No configuration files found for {template_name}\")\n",
    "            template_test_results.append({\n",
    "                'template': template_name,\n",
    "                'found': False,\n",
    "                'matches': 0,\n",
    "                'validation_score': 0,\n",
    "                'issues': ['Template not found']\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Found {len(matching_configs)} matching configurations\")\n",
    "        \n",
    "        # Test each matching configuration\n",
    "        template_result = {\n",
    "            'template': template_name,\n",
    "            'found': True,\n",
    "            'matches': len(matching_configs),\n",
    "            'validation_score': 0,\n",
    "            'issues': [],\n",
    "            'config_details': []\n",
    "        }\n",
    "        \n",
    "        for config_result in matching_configs:\n",
    "            config = config_result['config_object']\n",
    "            print(f\"    Testing {config_result['file']}...\")\n",
    "            \n",
    "            config_detail = {\n",
    "                'file': config_result['file'],\n",
    "                'components_present': [],\n",
    "                'components_missing': [],\n",
    "                'score': 0\n",
    "            }\n",
    "            \n",
    "            # Check required components\n",
    "            for component in template_spec['required_components']:\n",
    "                if hasattr(config, component) and getattr(config, component):\n",
    "                    config_detail['components_present'].append(component)\n",
    "                else:\n",
    "                    config_detail['components_missing'].append(component)\n",
    "                    template_result['issues'].append(f\"{config_result['file']}: Missing required component '{component}'\")\n",
    "            \n",
    "            # Check optional components\n",
    "            for component in template_spec['optional_components']:\n",
    "                if hasattr(config, component) and getattr(config, component):\n",
    "                    config_detail['components_present'].append(f\"{component} (optional)\")\n",
    "            \n",
    "            # Calculate score\n",
    "            required_present = len([c for c in config_detail['components_present'] \n",
    "                                  if c in template_spec['required_components']])\n",
    "            required_total = len(template_spec['required_components'])\n",
    "            config_detail['score'] = required_present / required_total if required_total > 0 else 1\n",
    "            \n",
    "            print(f\"      Components: {config_detail['components_present']}\")\n",
    "            if config_detail['components_missing']:\n",
    "                print(f\"      Missing: {config_detail['components_missing']}\")\n",
    "            print(f\"      Score: {config_detail['score']:.2f}\")\n",
    "            \n",
    "            template_result['config_details'].append(config_detail)\n",
    "        \n",
    "        # Calculate overall template validation score\n",
    "        if template_result['config_details']:\n",
    "            template_result['validation_score'] = sum(d['score'] for d in template_result['config_details']) / len(template_result['config_details'])\n",
    "        \n",
    "        print(f\"  Template validation score: {template_result['validation_score']:.2f}\")\n",
    "        template_test_results.append(template_result)\n",
    "    \n",
    "    print(f\"\\n=== Template Testing Summary ===\")\n",
    "    found_templates = sum(1 for r in template_test_results if r['found'])\n",
    "    avg_score = sum(r['validation_score'] for r in template_test_results) / len(template_test_results)\n",
    "    \n",
    "    print(f\"Templates tested: {len(expected_templates)}\")\n",
    "    print(f\"Templates found: {found_templates}\")\n",
    "    print(f\"Average validation score: {avg_score:.2f}\")\n",
    "    \n",
    "    return template_test_results\n",
    "\n",
    "# Run template testing\n",
    "template_results = test_configuration_templates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration Creation and Modification\n",
    "Test programmatic configuration creation and modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration creation and modification\n",
    "def test_configuration_creation_and_modification():\n",
    "    \"\"\"Test programmatic configuration creation and modification\"\"\"\n",
    "    print(\"Testing configuration creation and modification...\")\n",
    "    \n",
    "    creation_test_results = []\n",
    "    \n",
    "    # Test 1: Create minimal valid configuration\n",
    "    print(\"\\n--- Test 1: Minimal Valid Configuration ---\")\n",
    "    try:\n",
    "        minimal_config = PipelineConfig(\n",
    "            job=JobConfig(\n",
    "                job_name=\"test_minimal\",\n",
    "                output_base_dir=\"outputs/test\",\n",
    "                split_ratio=0.8,\n",
    "                random_seed=42\n",
    "            ),\n",
    "            input=InputConfig(\n",
    "                dataset_path=\"test_data.csv\"\n",
    "            ),\n",
    "            api=APIConfig(\n",
    "                host=\"localhost\",\n",
    "                port=8000\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(f\"  ✓ Minimal configuration created successfully\")\n",
    "        print(f\"    Job name: {minimal_config.job.job_name}\")\n",
    "        \n",
    "        creation_test_results.append({\n",
    "            'test': 'minimal_config',\n",
    "            'success': True,\n",
    "            'error': None\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Minimal configuration creation failed: {e}\")\n",
    "        creation_test_results.append({\n",
    "            'test': 'minimal_config',\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        })\n",
    "    \n",
    "    # Test 2: Create full configuration\n",
    "    print(\"\\n--- Test 2: Full Configuration ---\")\n",
    "    try:\n",
    "        full_config = PipelineConfig(\n",
    "            job=JobConfig(\n",
    "                job_name=\"test_full\",\n",
    "                output_base_dir=\"outputs/test_full\",\n",
    "                split_ratio=0.8,\n",
    "                random_seed=42\n",
    "            ),\n",
    "            input=InputConfig(\n",
    "                dataset_path=\"test_data.csv\"\n",
    "            ),\n",
    "            api=APIConfig(\n",
    "                host=\"localhost\",\n",
    "                port=8000\n",
    "            ),\n",
    "            embedding=EmbeddingConfig(\n",
    "                batch_size=32,\n",
    "                save_embeddings=True,\n",
    "                embeddings_file=\"test_embeddings.pkl\"\n",
    "            ),\n",
    "            classification=ClassificationConfig(\n",
    "                models=[\"logistic_regression\", \"svm\"],\n",
    "                cross_validation_folds=5,\n",
    "                hyperparameter_search=True\n",
    "            ),\n",
    "            evaluation=EvaluationConfig(\n",
    "                target_word=TargetWordConfig(\n",
    "                    target_codes=[\"E119\", \"Z03818\"],\n",
    "                    max_tokens=200,\n",
    "                    generation_attempts=5\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(f\"  ✓ Full configuration created successfully\")\n",
    "        print(f\"    Components: job, input, api, embedding, classification, evaluation\")\n",
    "        \n",
    "        creation_test_results.append({\n",
    "            'test': 'full_config',\n",
    "            'success': True,\n",
    "            'error': None\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Full configuration creation failed: {e}\")\n",
    "        creation_test_results.append({\n",
    "            'test': 'full_config',\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        })\n",
    "    \n",
    "    # Test 3: Configuration modification\n",
    "    print(\"\\n--- Test 3: Configuration Modification ---\")\n",
    "    try:\n",
    "        if 'minimal_config' in locals():\n",
    "            # Test modifying existing configuration\n",
    "            modified_config = copy.deepcopy(minimal_config)\n",
    "            \n",
    "            # Add embedding configuration\n",
    "            modified_config.embedding = EmbeddingConfig(\n",
    "                batch_size=64,\n",
    "                save_embeddings=True,\n",
    "                embeddings_file=\"modified_embeddings.pkl\"\n",
    "            )\n",
    "            \n",
    "            # Modify job name\n",
    "            modified_config.job.job_name = \"test_modified\"\n",
    "            \n",
    "            print(f\"  ✓ Configuration modification successful\")\n",
    "            print(f\"    New job name: {modified_config.job.job_name}\")\n",
    "            print(f\"    Added embedding config with batch_size: {modified_config.embedding.batch_size}\")\n",
    "            \n",
    "            creation_test_results.append({\n",
    "                'test': 'config_modification',\n",
    "                'success': True,\n",
    "                'error': None\n",
    "            })\n",
    "        else:\n",
    "            raise Exception(\"No base configuration to modify\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Configuration modification failed: {e}\")\n",
    "        creation_test_results.append({\n",
    "            'test': 'config_modification',\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        })\n",
    "    \n",
    "    # Test 4: Invalid configuration creation\n",
    "    print(\"\\n--- Test 4: Invalid Configuration Handling ---\")\n",
    "    invalid_config_tests = [\n",
    "        {\n",
    "            'name': 'Missing required field',\n",
    "            'config_func': lambda: PipelineConfig(\n",
    "                job=JobConfig(\n",
    "                    job_name=\"test\",\n",
    "                    output_base_dir=\"outputs\"\n",
    "                    # Missing split_ratio and random_seed\n",
    "                ),\n",
    "                input=InputConfig(dataset_path=\"test.csv\"),\n",
    "                api=APIConfig(host=\"localhost\", port=8000)\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            'name': 'Invalid field type',\n",
    "            'config_func': lambda: PipelineConfig(\n",
    "                job=JobConfig(\n",
    "                    job_name=\"test\",\n",
    "                    output_base_dir=\"outputs\",\n",
    "                    split_ratio=\"invalid\",  # Should be float\n",
    "                    random_seed=42\n",
    "                ),\n",
    "                input=InputConfig(dataset_path=\"test.csv\"),\n",
    "                api=APIConfig(host=\"localhost\", port=8000)\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            'name': 'Invalid range value',\n",
    "            'config_func': lambda: PipelineConfig(\n",
    "                job=JobConfig(\n",
    "                    job_name=\"test\",\n",
    "                    output_base_dir=\"outputs\",\n",
    "                    split_ratio=1.5,  # Should be between 0 and 1\n",
    "                    random_seed=42\n",
    "                ),\n",
    "                input=InputConfig(dataset_path=\"test.csv\"),\n",
    "                api=APIConfig(host=\"localhost\", port=8000)\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for invalid_test in invalid_config_tests:\n",
    "        try:\n",
    "            invalid_test['config_func']()\n",
    "            print(f\"  ✗ {invalid_test['name']}: Should have failed but didn't\")\n",
    "            creation_test_results.append({\n",
    "                'test': f\"invalid_{invalid_test['name']}\",\n",
    "                'success': False,\n",
    "                'error': 'Should have failed validation'\n",
    "            })\n",
    "        except (ValidationError, ValueError, TypeError) as e:\n",
    "            print(f\"  ✓ {invalid_test['name']}: Correctly caught validation error\")\n",
    "            creation_test_results.append({\n",
    "                'test': f\"invalid_{invalid_test['name']}\",\n",
    "                'success': True,\n",
    "                'error': None\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  ~ {invalid_test['name']}: Unexpected error type: {e}\")\n",
    "            creation_test_results.append({\n",
    "                'test': f\"invalid_{invalid_test['name']}\",\n",
    "                'success': True,  # Still caught an error\n",
    "                'error': f\"Unexpected error: {e}\"\n",
    "            })\n",
    "    \n",
    "    print(f\"\\n=== Configuration Creation Summary ===\")\n",
    "    successful_tests = sum(1 for r in creation_test_results if r['success'])\n",
    "    total_tests = len(creation_test_results)\n",
    "    \n",
    "    print(f\"Tests passed: {successful_tests}/{total_tests}\")\n",
    "    for result in creation_test_results:\n",
    "        status = \"✓\" if result['success'] else \"✗\"\n",
    "        print(f\"  {status} {result['test']}\")\n",
    "        if result['error']:\n",
    "            print(f\"    Error: {result['error']}\")\n",
    "    \n",
    "    return creation_test_results\n",
    "\n",
    "# Run configuration creation and modification tests\n",
    "creation_results = test_configuration_creation_and_modification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration Serialization and Export\n",
    "Test configuration export to different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration serialization and export\n",
    "def test_configuration_serialization():\n",
    "    \"\"\"Test configuration serialization to different formats\"\"\"\n",
    "    print(\"Testing configuration serialization...\")\n",
    "    \n",
    "    # Create a test configuration for serialization\n",
    "    test_config = PipelineConfig(\n",
    "        job=JobConfig(\n",
    "            job_name=\"serialization_test\",\n",
    "            output_base_dir=\"outputs/serialization_test\",\n",
    "            split_ratio=0.8,\n",
    "            random_seed=42\n",
    "        ),\n",
    "        input=InputConfig(\n",
    "            dataset_path=\"test_serialization_data.csv\"\n",
    "        ),\n",
    "        api=APIConfig(\n",
    "            host=\"localhost\",\n",
    "            port=8000\n",
    "        ),\n",
    "        embedding=EmbeddingConfig(\n",
    "            batch_size=32,\n",
    "            save_embeddings=True,\n",
    "            embeddings_file=\"serialization_embeddings.pkl\"\n",
    "        ),\n",
    "        evaluation=EvaluationConfig(\n",
    "            target_word=TargetWordConfig(\n",
    "                target_codes=[\"E119\", \"Z03818\", \"N6320\"],\n",
    "                max_tokens=200,\n",
    "                generation_attempts=5\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    serialization_results = []\n",
    "    \n",
    "    # Test 1: YAML serialization\n",
    "    print(\"\\n--- Test 1: YAML Serialization ---\")\n",
    "    try:\n",
    "        # Convert to dictionary\n",
    "        config_dict = test_config.model_dump()\n",
    "        \n",
    "        # Serialize to YAML\n",
    "        yaml_output = yaml.dump(config_dict, default_flow_style=False, sort_keys=False)\n",
    "        \n",
    "        print(f\"  ✓ YAML serialization successful\")\n",
    "        print(f\"    Output length: {len(yaml_output)} characters\")\n",
    "        print(f\"    First 200 chars: {yaml_output[:200]}...\")\n",
    "        \n",
    "        # Test round-trip: YAML -> dict -> config\n",
    "        parsed_dict = yaml.safe_load(yaml_output)\n",
    "        reconstructed_config = PipelineConfig.model_validate(parsed_dict)\n",
    "        \n",
    "        print(f\"  ✓ YAML round-trip successful\")\n",
    "        print(f\"    Original job name: {test_config.job.job_name}\")\n",
    "        print(f\"    Reconstructed job name: {reconstructed_config.job.job_name}\")\n",
    "        \n",
    "        serialization_results.append({\n",
    "            'format': 'yaml',\n",
    "            'serialization_success': True,\n",
    "            'round_trip_success': True,\n",
    "            'output_size': len(yaml_output),\n",
    "            'error': None\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ YAML serialization failed: {e}\")\n",
    "        serialization_results.append({\n",
    "            'format': 'yaml',\n",
    "            'serialization_success': False,\n",
    "            'round_trip_success': False,\n",
    "            'output_size': 0,\n",
    "            'error': str(e)\n",
    "        })\n",
    "    \n",
    "    # Test 2: JSON serialization\n",
    "    print(\"\\n--- Test 2: JSON Serialization ---\")\n",
    "    try:\n",
    "        # Convert to dictionary\n",
    "        config_dict = test_config.model_dump()\n",
    "        \n",
    "        # Serialize to JSON\n",
    "        json_output = json.dumps(config_dict, indent=2)\n",
    "        \n",
    "        print(f\"  ✓ JSON serialization successful\")\n",
    "        print(f\"    Output length: {len(json_output)} characters\")\n",
    "        \n",
    "        # Test round-trip: JSON -> dict -> config\n",
    "        parsed_dict = json.loads(json_output)\n",
    "        reconstructed_config = PipelineConfig.model_validate(parsed_dict)\n",
    "        \n",
    "        print(f\"  ✓ JSON round-trip successful\")\n",
    "        \n",
    "        serialization_results.append({\n",
    "            'format': 'json',\n",
    "            'serialization_success': True,\n",
    "            'round_trip_success': True,\n",
    "            'output_size': len(json_output),\n",
    "            'error': None\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ JSON serialization failed: {e}\")\n",
    "        serialization_results.append({\n",
    "            'format': 'json',\n",
    "            'serialization_success': False,\n",
    "            'round_trip_success': False,\n",
    "            'output_size': 0,\n",
    "            'error': str(e)\n",
    "        })\n",
    "    \n",
    "    # Test 3: File export\n",
    "    print(\"\\n--- Test 3: File Export ---\")\n",
    "    try:\n",
    "        import tempfile\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Export to YAML file\n",
    "            yaml_file = os.path.join(temp_dir, \"test_config.yaml\")\n",
    "            config_dict = test_config.model_dump()\n",
    "            \n",
    "            with open(yaml_file, 'w') as f:\n",
    "                yaml.dump(config_dict, f, default_flow_style=False, sort_keys=False)\n",
    "            \n",
    "            # Verify file was created\n",
    "            if os.path.exists(yaml_file):\n",
    "                file_size = os.path.getsize(yaml_file)\n",
    "                print(f\"  ✓ YAML file export successful\")\n",
    "                print(f\"    File size: {file_size} bytes\")\n",
    "                \n",
    "                # Test loading from file\n",
    "                loaded_config = PipelineConfig.from_yaml(yaml_file)\n",
    "                print(f\"  ✓ Configuration loaded from file\")\n",
    "                print(f\"    Loaded job name: {loaded_config.job.job_name}\")\n",
    "                \n",
    "                serialization_results.append({\n",
    "                    'format': 'file_export',\n",
    "                    'serialization_success': True,\n",
    "                    'round_trip_success': True,\n",
    "                    'output_size': file_size,\n",
    "                    'error': None\n",
    "                })\n",
    "            else:\n",
    "                raise Exception(\"File was not created\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ File export failed: {e}\")\n",
    "        serialization_results.append({\n",
    "            'format': 'file_export',\n",
    "            'serialization_success': False,\n",
    "            'round_trip_success': False,\n",
    "            'output_size': 0,\n",
    "            'error': str(e)\n",
    "        })\n",
    "    \n",
    "    # Test 4: Configuration comparison\n",
    "    print(\"\\n--- Test 4: Configuration Comparison ---\")\n",
    "    try:\n",
    "        # Create a slightly different configuration\n",
    "        modified_config = copy.deepcopy(test_config)\n",
    "        modified_config.job.job_name = \"modified_serialization_test\"\n",
    "        modified_config.embedding.batch_size = 64\n",
    "        \n",
    "        # Compare configurations\n",
    "        original_dict = test_config.model_dump()\n",
    "        modified_dict = modified_config.model_dump()\n",
    "        \n",
    "        differences = []\n",
    "        def compare_dicts(d1, d2, path=\"\"):\n",
    "            for key in set(d1.keys()) | set(d2.keys()):\n",
    "                current_path = f\"{path}.{key}\" if path else key\n",
    "                \n",
    "                if key not in d1:\n",
    "                    differences.append(f\"Missing in original: {current_path}\")\n",
    "                elif key not in d2:\n",
    "                    differences.append(f\"Missing in modified: {current_path}\")\n",
    "                elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n",
    "                    compare_dicts(d1[key], d2[key], current_path)\n",
    "                elif d1[key] != d2[key]:\n",
    "                    differences.append(f\"Value difference at {current_path}: {d1[key]} != {d2[key]}\")\n",
    "        \n",
    "        compare_dicts(original_dict, modified_dict)\n",
    "        \n",
    "        print(f\"  ✓ Configuration comparison completed\")\n",
    "        print(f\"    Found {len(differences)} differences\")\n",
    "        for diff in differences[:3]:  # Show first 3\n",
    "            print(f\"      - {diff}\")\n",
    "        \n",
    "        serialization_results.append({\n",
    "            'format': 'comparison',\n",
    "            'serialization_success': True,\n",
    "            'round_trip_success': True,\n",
    "            'output_size': len(differences),\n",
    "            'error': None\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Configuration comparison failed: {e}\")\n",
    "        serialization_results.append({\n",
    "            'format': 'comparison',\n",
    "            'serialization_success': False,\n",
    "            'round_trip_success': False,\n",
    "            'output_size': 0,\n",
    "            'error': str(e)\n",
    "        })\n",
    "    \n",
    "    print(f\"\\n=== Serialization Summary ===\")\n",
    "    successful_serialization = sum(1 for r in serialization_results if r['serialization_success'])\n",
    "    successful_round_trips = sum(1 for r in serialization_results if r['round_trip_success'])\n",
    "    \n",
    "    print(f\"Serialization tests: {successful_serialization}/{len(serialization_results)} successful\")\n",
    "    print(f\"Round-trip tests: {successful_round_trips}/{len(serialization_results)} successful\")\n",
    "    \n",
    "    for result in serialization_results:\n",
    "        status = \"✓\" if result['serialization_success'] else \"✗\"\n",
    "        print(f\"  {status} {result['format']}: {result['output_size']} bytes/items\")\n",
    "        if result['error']:\n",
    "            print(f\"    Error: {result['error']}\")\n",
    "    \n",
    "    return serialization_results\n",
    "\n",
    "# Run serialization tests\n",
    "serialization_results = test_configuration_serialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration Compatibility Testing\n",
    "Test configuration compatibility across different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration compatibility\n",
    "def test_configuration_compatibility():\n",
    "    \"\"\"Test configuration compatibility across different use cases\"\"\"\n",
    "    print(\"Testing configuration compatibility...\")\n",
    "    \n",
    "    compatibility_test_scenarios = [\n",
    "        {\n",
    "            'name': 'Embeddings + Classification',\n",
    "            'description': 'Configuration with both embedding and classification',\n",
    "            'config_builder': lambda: PipelineConfig(\n",
    "                job=JobConfig(\n",
    "                    job_name=\"embed_classify_test\",\n",
    "                    output_base_dir=\"outputs/embed_classify\",\n",
    "                    split_ratio=0.8,\n",
    "                    random_seed=42\n",
    "                ),\n",
    "                input=InputConfig(dataset_path=\"test.csv\"),\n",
    "                api=APIConfig(host=\"localhost\", port=8000),\n",
    "                embedding=EmbeddingConfig(\n",
    "                    batch_size=32,\n",
    "                    save_embeddings=True,\n",
    "                    embeddings_file=\"embed_classify.pkl\"\n",
    "                ),\n",
    "                classification=ClassificationConfig(\n",
    "                    models=[\"logistic_regression\"],\n",
    "                    cross_validation_folds=3\n",
    "                )\n",
    "            ),\n",
    "            'expected_issues': []\n",
    "        },\n",
    "        {\n",
    "            'name': 'Split ratio edge case',\n",
    "            'description': 'Configuration with very small test set',\n",
    "            'config_builder': lambda: PipelineConfig(\n",
    "                job=JobConfig(\n",
    "                    job_name=\"edge_split_test\",\n",
    "                    output_base_dir=\"outputs/edge_split\",\n",
    "                    split_ratio=0.95,  # Only 5% for test\n",
    "                    random_seed=42\n",
    "                ),\n",
    "                input=InputConfig(dataset_path=\"test.csv\"),\n",
    "                api=APIConfig(host=\"localhost\", port=8000)\n",
    "            ),\n",
    "            'expected_issues': ['Very small test set may cause evaluation issues']\n",
    "        },\n",
    "        {\n",
    "            'name': 'Multiple input sources',\n",
    "            'description': 'Configuration with both single and split datasets',\n",
    "            'config_builder': lambda: PipelineConfig(\n",
    "                job=JobConfig(\n",
    "                    job_name=\"multi_input_test\",\n",
    "                    output_base_dir=\"outputs/multi_input\",\n",
    "                    split_ratio=0.8,\n",
    "                    random_seed=42\n",
    "                ),\n",
    "                input=InputConfig(\n",
    "                    dataset_path=\"single.csv\",\n",
    "                    train_dataset_path=\"train.csv\",\n",
    "                    test_dataset_path=\"test.csv\"\n",
    "                ),\n",
    "                api=APIConfig(host=\"localhost\", port=8000)\n",
    "            ),\n",
    "            'expected_issues': ['Conflicting input specifications']\n",
    "        },\n",
    "        {\n",
    "            'name': 'Large batch size',\n",
    "            'description': 'Configuration with very large batch size',\n",
    "            'config_builder': lambda: PipelineConfig(\n",
    "                job=JobConfig(\n",
    "                    job_name=\"large_batch_test\",\n",
    "                    output_base_dir=\"outputs/large_batch\",\n",
    "                    split_ratio=0.8,\n",
    "                    random_seed=42\n",
    "                ),\n",
    "                input=InputConfig(dataset_path=\"test.csv\"),\n",
    "                api=APIConfig(host=\"localhost\", port=8000),\n",
    "                embedding=EmbeddingConfig(\n",
    "                    batch_size=1000,  # Very large batch\n",
    "                    save_embeddings=True\n",
    "                )\n",
    "            ),\n",
    "            'expected_issues': ['Large batch size may cause memory issues']\n",
    "        },\n",
    "        {\n",
    "            'name': 'Empty target codes',\n",
    "            'description': 'Configuration with empty target codes list',\n",
    "            'config_builder': lambda: PipelineConfig(\n",
    "                job=JobConfig(\n",
    "                    job_name=\"empty_targets_test\",\n",
    "                    output_base_dir=\"outputs/empty_targets\",\n",
    "                    split_ratio=0.8,\n",
    "                    random_seed=42\n",
    "                ),\n",
    "                input=InputConfig(dataset_path=\"test.csv\"),\n",
    "                api=APIConfig(host=\"localhost\", port=8000),\n",
    "                evaluation=EvaluationConfig(\n",
    "                    target_word=TargetWordConfig(\n",
    "                        target_codes=[],  # Empty list\n",
    "                        max_tokens=200,\n",
    "                        generation_attempts=5\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            'expected_issues': ['Empty target codes list']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    compatibility_results = []\n",
    "    \n",
    "    for scenario in compatibility_test_scenarios:\n",
    "        print(f\"\\n--- {scenario['name']} ---\")\n",
    "        print(f\"Description: {scenario['description']}\")\n",
    "        \n",
    "        result = {\n",
    "            'scenario': scenario['name'],\n",
    "            'creation_success': False,\n",
    "            'detected_issues': [],\n",
    "            'expected_issues': scenario['expected_issues'],\n",
    "            'validation_score': 0,\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Create configuration\n",
    "            config = scenario['config_builder']()\n",
    "            result['creation_success'] = True\n",
    "            print(f\"  ✓ Configuration created successfully\")\n",
    "            \n",
    "            # Analyze configuration for potential issues\n",
    "            detected_issues = []\n",
    "            \n",
    "            # Check split ratio\n",
    "            if hasattr(config.job, 'split_ratio'):\n",
    "                if config.job.split_ratio > 0.9:\n",
    "                    detected_issues.append('Very small test set may cause evaluation issues')\n",
    "                elif config.job.split_ratio < 0.5:\n",
    "                    detected_issues.append('Very small training set may cause model issues')\n",
    "            \n",
    "            # Check input configuration\n",
    "            if hasattr(config, 'input') and config.input:\n",
    "                has_single = bool(getattr(config.input, 'dataset_path', None))\n",
    "                has_split = bool(getattr(config.input, 'train_dataset_path', None) and \n",
    "                               getattr(config.input, 'test_dataset_path', None))\n",
    "                \n",
    "                if has_single and has_split:\n",
    "                    detected_issues.append('Conflicting input specifications')\n",
    "                elif not (has_single or has_split):\n",
    "                    detected_issues.append('No valid input specification')\n",
    "            \n",
    "            # Check embedding configuration\n",
    "            if hasattr(config, 'embedding') and config.embedding:\n",
    "                if hasattr(config.embedding, 'batch_size') and config.embedding.batch_size > 500:\n",
    "                    detected_issues.append('Large batch size may cause memory issues')\n",
    "            \n",
    "            # Check evaluation configuration\n",
    "            if hasattr(config, 'evaluation') and config.evaluation:\n",
    "                if hasattr(config.evaluation, 'target_word') and config.evaluation.target_word:\n",
    "                    target_codes = getattr(config.evaluation.target_word, 'target_codes', [])\n",
    "                    if not target_codes or len(target_codes) == 0:\n",
    "                        detected_issues.append('Empty target codes list')\n",
    "            \n",
    "            result['detected_issues'] = detected_issues\n",
    "            \n",
    "            # Calculate validation score based on issue detection accuracy\n",
    "            expected_set = set(scenario['expected_issues'])\n",
    "            detected_set = set(detected_issues)\n",
    "            \n",
    "            if expected_set == detected_set:\n",
    "                result['validation_score'] = 1.0  # Perfect match\n",
    "            elif len(expected_set) == 0 and len(detected_set) == 0:\n",
    "                result['validation_score'] = 1.0  # Both empty\n",
    "            else:\n",
    "                # Calculate overlap\n",
    "                intersection = expected_set & detected_set\n",
    "                union = expected_set | detected_set\n",
    "                result['validation_score'] = len(intersection) / len(union) if union else 0\n",
    "            \n",
    "            print(f\"  Expected issues: {scenario['expected_issues']}\")\n",
    "            print(f\"  Detected issues: {detected_issues}\")\n",
    "            print(f\"  Validation score: {result['validation_score']:.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            result['error'] = str(e)\n",
    "            print(f\"  ✗ Configuration creation failed: {e}\")\n",
    "        \n",
    "        compatibility_results.append(result)\n",
    "    \n",
    "    print(f\"\\n=== Compatibility Testing Summary ===\")\n",
    "    successful_creations = sum(1 for r in compatibility_results if r['creation_success'])\n",
    "    avg_validation_score = sum(r['validation_score'] for r in compatibility_results) / len(compatibility_results)\n",
    "    \n",
    "    print(f\"Scenarios tested: {len(compatibility_test_scenarios)}\")\n",
    "    print(f\"Successful configurations: {successful_creations}\")\n",
    "    print(f\"Average issue detection score: {avg_validation_score:.2f}\")\n",
    "    \n",
    "    for result in compatibility_results:\n",
    "        status = \"✓\" if result['creation_success'] else \"✗\"\n",
    "        print(f\"  {status} {result['scenario']}: Score {result['validation_score']:.2f}\")\n",
    "        if result['error']:\n",
    "            print(f\"    Error: {result['error']}\")\n",
    "    \n",
    "    return compatibility_results\n",
    "\n",
    "# Run compatibility tests\n",
    "compatibility_results = test_configuration_compatibility()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Configuration Validation Summary\n",
    "Comprehensive summary of all configuration validation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive configuration validation summary\n",
    "def generate_configuration_validation_summary():\n",
    "    \"\"\"Generate comprehensive summary of configuration validation tests\"\"\"\n",
    "    print(\"Generating configuration validation summary...\")\n",
    "    \n",
    "    summary = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'test_categories': {\n",
    "            'file_discovery': None,\n",
    "            'schema_validation': None,\n",
    "            'template_testing': None,\n",
    "            'creation_modification': None,\n",
    "            'serialization': None,\n",
    "            'compatibility': None\n",
    "        },\n",
    "        'overall_health': 'unknown',\n",
    "        'recommendations': [],\n",
    "        'critical_issues': []\n",
    "    }\n",
    "    \n",
    "    # File discovery summary\n",
    "    if 'config_files' in globals() and config_files:\n",
    "        summary['test_categories']['file_discovery'] = {\n",
    "            'status': 'passed',\n",
    "            'files_found': len(config_files),\n",
    "            'details': f\"Found {len(config_files)} configuration files\"\n",
    "        }\n",
    "    else:\n",
    "        summary['test_categories']['file_discovery'] = {\n",
    "            'status': 'failed',\n",
    "            'files_found': 0,\n",
    "            'details': 'No configuration files discovered'\n",
    "        }\n",
    "        summary['critical_issues'].append('No configuration files found')\n",
    "    \n",
    "    # Loading and schema validation summary\n",
    "    if 'loading_results' in globals() and loading_results:\n",
    "        yaml_valid = sum(1 for r in loading_results if r['yaml_valid'])\n",
    "        pydantic_valid = sum(1 for r in loading_results if r['pydantic_valid'])\n",
    "        total_files = len(loading_results)\n",
    "        \n",
    "        if pydantic_valid == total_files:\n",
    "            summary['test_categories']['schema_validation'] = {\n",
    "                'status': 'passed',\n",
    "                'valid_configs': pydantic_valid,\n",
    "                'total_configs': total_files,\n",
    "                'details': f\"All {total_files} configurations are valid\"\n",
    "            }\n",
    "        elif pydantic_valid > 0:\n",
    "            summary['test_categories']['schema_validation'] = {\n",
    "                'status': 'partial',\n",
    "                'valid_configs': pydantic_valid,\n",
    "                'total_configs': total_files,\n",
    "                'details': f\"{pydantic_valid}/{total_files} configurations are valid\"\n",
    "            }\n",
    "            summary['recommendations'].append(f\"Fix {total_files - pydantic_valid} invalid configuration files\")\n",
    "        else:\n",
    "            summary['test_categories']['schema_validation'] = {\n",
    "                'status': 'failed',\n",
    "                'valid_configs': 0,\n",
    "                'total_configs': total_files,\n",
    "                'details': 'No valid configurations found'\n",
    "            }\n",
    "            summary['critical_issues'].append('All configuration files are invalid')\n",
    "    \n",
    "    # Template testing summary\n",
    "    if 'template_results' in globals() and template_results:\n",
    "        found_templates = sum(1 for r in template_results if r['found'])\n",
    "        total_templates = len(template_results)\n",
    "        avg_score = sum(r['validation_score'] for r in template_results) / total_templates\n",
    "        \n",
    "        if found_templates == total_templates and avg_score > 0.8:\n",
    "            summary['test_categories']['template_testing'] = {\n",
    "                'status': 'passed',\n",
    "                'templates_found': found_templates,\n",
    "                'avg_score': avg_score,\n",
    "                'details': f\"All {total_templates} template types found with good validation\"\n",
    "            }\n",
    "        elif found_templates > 0:\n",
    "            summary['test_categories']['template_testing'] = {\n",
    "                'status': 'partial',\n",
    "                'templates_found': found_templates,\n",
    "                'avg_score': avg_score,\n",
    "                'details': f\"{found_templates}/{total_templates} template types found\"\n",
    "            }\n",
    "            if avg_score < 0.8:\n",
    "                summary['recommendations'].append('Improve template validation scores')\n",
    "        else:\n",
    "            summary['test_categories']['template_testing'] = {\n",
    "                'status': 'failed',\n",
    "                'templates_found': 0,\n",
    "                'avg_score': 0,\n",
    "                'details': 'No configuration templates found'\n",
    "            }\n",
    "            summary['recommendations'].append('Create configuration templates for common use cases')\n",
    "    \n",
    "    # Creation and modification summary\n",
    "    if 'creation_results' in globals() and creation_results:\n",
    "        successful_tests = sum(1 for r in creation_results if r['success'])\n",
    "        total_tests = len(creation_results)\n",
    "        \n",
    "        if successful_tests == total_tests:\n",
    "            summary['test_categories']['creation_modification'] = {\n",
    "                'status': 'passed',\n",
    "                'successful_tests': successful_tests,\n",
    "                'total_tests': total_tests,\n",
    "                'details': 'All configuration creation and modification tests passed'\n",
    "            }\n",
    "        elif successful_tests > total_tests * 0.7:\n",
    "            summary['test_categories']['creation_modification'] = {\n",
    "                'status': 'partial',\n",
    "                'successful_tests': successful_tests,\n",
    "                'total_tests': total_tests,\n",
    "                'details': f\"Most tests passed ({successful_tests}/{total_tests})\"\n",
    "            }\n",
    "        else:\n",
    "            summary['test_categories']['creation_modification'] = {\n",
    "                'status': 'failed',\n",
    "                'successful_tests': successful_tests,\n",
    "                'total_tests': total_tests,\n",
    "                'details': f\"Many tests failed ({total_tests - successful_tests}/{total_tests})\"\n",
    "            }\n",
    "            summary['critical_issues'].append('Configuration creation/modification has issues')\n",
    "    \n",
    "    # Serialization summary\n",
    "    if 'serialization_results' in globals() and serialization_results:\n",
    "        successful_serialization = sum(1 for r in serialization_results if r['serialization_success'])\n",
    "        successful_round_trips = sum(1 for r in serialization_results if r['round_trip_success'])\n",
    "        total_tests = len(serialization_results)\n",
    "        \n",
    "        if successful_serialization == total_tests and successful_round_trips == total_tests:\n",
    "            summary['test_categories']['serialization'] = {\n",
    "                'status': 'passed',\n",
    "                'serialization_success': successful_serialization,\n",
    "                'round_trip_success': successful_round_trips,\n",
    "                'details': 'All serialization tests passed'\n",
    "            }\n",
    "        elif successful_serialization > 0:\n",
    "            summary['test_categories']['serialization'] = {\n",
    "                'status': 'partial',\n",
    "                'serialization_success': successful_serialization,\n",
    "                'round_trip_success': successful_round_trips,\n",
    "                'details': f\"Partial serialization success ({successful_serialization}/{total_tests})\"\n",
    "            }\n",
    "        else:\n",
    "            summary['test_categories']['serialization'] = {\n",
    "                'status': 'failed',\n",
    "                'serialization_success': 0,\n",
    "                'round_trip_success': 0,\n",
    "                'details': 'Serialization tests failed'\n",
    "            }\n",
    "            summary['critical_issues'].append('Configuration serialization not working')\n",
    "    \n",
    "    # Compatibility summary\n",
    "    if 'compatibility_results' in globals() and compatibility_results:\n",
    "        successful_creations = sum(1 for r in compatibility_results if r['creation_success'])\n",
    "        avg_validation_score = sum(r['validation_score'] for r in compatibility_results) / len(compatibility_results)\n",
    "        total_scenarios = len(compatibility_results)\n",
    "        \n",
    "        if successful_creations == total_scenarios and avg_validation_score > 0.8:\n",
    "            summary['test_categories']['compatibility'] = {\n",
    "                'status': 'passed',\n",
    "                'successful_scenarios': successful_creations,\n",
    "                'avg_validation_score': avg_validation_score,\n",
    "                'details': 'All compatibility scenarios passed'\n",
    "            }\n",
    "        elif successful_creations > 0:\n",
    "            summary['test_categories']['compatibility'] = {\n",
    "                'status': 'partial',\n",
    "                'successful_scenarios': successful_creations,\n",
    "                'avg_validation_score': avg_validation_score,\n",
    "                'details': f\"Partial compatibility ({successful_creations}/{total_scenarios})\"\n",
    "            }\n",
    "            if avg_validation_score < 0.5:\n",
    "                summary['recommendations'].append('Improve configuration issue detection')\n",
    "        else:\n",
    "            summary['test_categories']['compatibility'] = {\n",
    "                'status': 'failed',\n",
    "                'successful_scenarios': 0,\n",
    "                'avg_validation_score': 0,\n",
    "                'details': 'Compatibility testing failed'\n",
    "            }\n",
    "            summary['critical_issues'].append('Configuration compatibility issues')\n",
    "    \n",
    "    # Determine overall health\n",
    "    passed_categories = sum(1 for cat in summary['test_categories'].values() \n",
    "                          if cat and cat['status'] == 'passed')\n",
    "    total_categories = sum(1 for cat in summary['test_categories'].values() if cat)\n",
    "    failed_categories = sum(1 for cat in summary['test_categories'].values() \n",
    "                          if cat and cat['status'] == 'failed')\n",
    "    \n",
    "    if failed_categories > 0:\n",
    "        summary['overall_health'] = 'poor'\n",
    "    elif passed_categories == total_categories:\n",
    "        summary['overall_health'] = 'excellent'\n",
    "    elif passed_categories >= total_categories * 0.7:\n",
    "        summary['overall_health'] = 'good'\n",
    "    else:\n",
    "        summary['overall_health'] = 'fair'\n",
    "    \n",
    "    # Add general recommendations\n",
    "    if not summary['recommendations']:\n",
    "        summary['recommendations'].append('Configuration system is working well')\n",
    "    \n",
    "    if len(summary['critical_issues']) == 0:\n",
    "        summary['recommendations'].append('No critical configuration issues detected')\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CONFIGURATION VALIDATION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Generated: {summary['timestamp']}\")\n",
    "    print(f\"Overall Health: {summary['overall_health'].upper()}\")\n",
    "    print(f\"\\nTest Results: {passed_categories}/{total_categories} categories passed\")\n",
    "    \n",
    "    for category, result in summary['test_categories'].items():\n",
    "        if result:\n",
    "            status_symbol = \"✓\" if result['status'] == 'passed' else \"✗\" if result['status'] == 'failed' else \"~\"\n",
    "            print(f\"  {status_symbol} {category.replace('_', ' ').title()}: {result['status']}\")\n",
    "            print(f\"    {result['details']}\")\n",
    "    \n",
    "    if summary['critical_issues']:\n",
    "        print(f\"\\nCritical Issues ({len(summary['critical_issues'])}):\")\n",
    "        for issue in summary['critical_issues']:\n",
    "            print(f\"  ✗ {issue}\")\n",
    "    \n",
    "    print(f\"\\nRecommendations:\")\n",
    "    for rec in summary['recommendations']:\n",
    "        print(f\"  • {rec}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate final configuration validation summary\n",
    "final_config_summary = generate_configuration_validation_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Configuration Testing\n",
    "Use this section for custom configuration validation scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom configuration testing - modify as needed\n",
    "print(\"=== Custom Configuration Testing ===\")\n",
    "print(\"Use this cell for your own configuration testing scenarios\")\n",
    "\n",
    "# Example: Test specific configuration modifications\n",
    "custom_config_tests = [\n",
    "    {\n",
    "        'name': 'Production-like configuration',\n",
    "        'description': 'Configuration for production deployment',\n",
    "        'modifications': {\n",
    "            'job_name': 'production_pipeline',\n",
    "            'output_dir': '/data/production/outputs',\n",
    "            'api_port': 9000,\n",
    "            'batch_size': 128\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Development configuration',\n",
    "        'description': 'Configuration for development and testing',\n",
    "        'modifications': {\n",
    "            'job_name': 'dev_pipeline',\n",
    "            'split_ratio': 0.9,  # More data for training in dev\n",
    "            'batch_size': 16,    # Smaller batches for faster iteration\n",
    "            'cv_folds': 3        # Faster cross-validation\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\nCustom configuration test scenarios:\")\n",
    "for i, test in enumerate(custom_config_tests, 1):\n",
    "    print(f\"  {i}. {test['name']} - {test['description']}\")\n",
    "    print(f\"     Modifications: {test['modifications']}\")\n",
    "\n",
    "# Add your custom configuration testing logic here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom debugging for specific configuration issues\n",
    "print(\"=== Custom Configuration Debugging ===\")\n",
    "\n",
    "# Add your configuration debugging code here\n",
    "# Examples:\n",
    "# - Test specific field validation rules\n",
    "# - Debug configuration inheritance or merging\n",
    "# - Test environment-specific configurations\n",
    "# - Validate configuration defaults\n",
    "# - Test configuration migration between versions\n",
    "\n",
    "print(\"Add your custom configuration debugging code in this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides comprehensive configuration validation testing for the MGPT-eval pipeline:\n",
    "\n",
    "1. **File Discovery**: Discovers and catalogs all configuration files\n",
    "2. **Schema Validation**: Tests YAML parsing and Pydantic validation\n",
    "3. **Template Testing**: Validates configuration templates and examples\n",
    "4. **Creation/Modification**: Tests programmatic configuration management\n",
    "5. **Serialization**: Tests configuration export and round-trip conversion\n",
    "6. **Compatibility**: Tests configuration compatibility across scenarios\n",
    "7. **Summary Report**: Provides comprehensive validation status\n",
    "8. **Custom Testing**: Space for your own configuration scenarios\n",
    "\n",
    "Use this notebook to:\n",
    "- Debug configuration file issues\n",
    "- Validate configuration schemas\n",
    "- Test configuration templates\n",
    "- Ensure configuration compatibility\n",
    "- Verify configuration serialization\n",
    "\n",
    "The configuration validation summary provides a complete overview of the system's configuration health and highlights any issues that need attention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.5"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}