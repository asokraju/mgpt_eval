{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Pipeline Debug\n",
    "\n",
    "This notebook debugs the end-to-end pipeline using the actual EndToEndPipeline class.\n",
    "Tests complete pipeline workflow using real pipeline methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline modules\n",
    "import sys\n",
    "sys.path.append('/home/kosaraju/mgpt-serve/mgpt_eval')\n",
    "\n",
    "from models.config_models import PipelineConfig\n",
    "from pipelines.end_to_end_pipeline import EndToEndPipeline\n",
    "from pipelines.embedding_pipeline import EmbeddingPipeline\n",
    "from pipelines.classification_pipeline import ClassificationPipeline\n",
    "from pipelines.evaluation_pipeline import EvaluationPipeline\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"/home/kosaraju/mgpt-serve/mgpt_eval/configs/pipeline_config.yaml\"\n",
    "config = PipelineConfig.from_yaml(config_path)\n",
    "print(f\"Config loaded: {config.job.job_name}\")\n",
    "print(f\"Output directory: {config.job.output_base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize end-to-end pipeline\n",
    "try:\n",
    "    e2e_pipeline = EndToEndPipeline(config)\n",
    "    print(f\"End-to-end pipeline initialized\")\n",
    "    print(f\"Pipeline methods: {[m for m in dir(e2e_pipeline) if not m.startswith('_') and callable(getattr(e2e_pipeline, m))]}\")\nexcept Exception as e:\n",
    "    print(f\"End-to-end pipeline initialization failed: {e}\")\n",
    "    e2e_pipeline = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "test_data = pd.DataFrame({\n",
    "    'mcid': [f'E2E_{i:03d}' for i in range(30)],\n",
    "    'claims': [\n",
    "        'N6320 G0378 |eoc| Z91048 M1710',\n",
    "        'E119 A1234 |eoc| B5678 C9012',\n",
    "        'Z03818 D3456 |eoc| F7890 G1234',\n",
    "        'H5678 I9012 |eoc| J1234 K5678',\n",
    "        'L9012 M3456 |eoc| N6320 O7890',\n",
    "        'P1234 Q5678 |eoc| E119 R9012',\n",
    "        'S3456 T7890 |eoc| U1234 V5678',\n",
    "        'W9012 X3456 |eoc| Z03818 Y7890',\n",
    "        'Z1234 A5678 |eoc| B9012 C3456',\n",
    "        'D7890 E1234 |eoc| N6320 F5678'\n",
    "    ] * 3,\n",
    "    'label': [1, 1, 1, 0, 1, 1, 0, 1, 0, 1] * 3\n",
    "})\n",
    "\n",
    "print(f\"Test data created: {len(test_data)} samples\")\n",
    "print(f\"Labels distribution: {test_data['label'].value_counts().to_dict()}\")\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test end-to-end pipeline run\n",
    "if e2e_pipeline:\n",
    "    print(\"Testing end-to-end pipeline run...\")\n",
    "    \n",
    "    try:\n",
    "        # Use the main run method\n",
    "        results = e2e_pipeline.run(test_data)\n",
    "        print(f\"✓ End-to-end pipeline completed\")\n",
    "        print(f\"  Results type: {type(results)}\")\n",
    "        print(f\"  Results keys: {list(results.keys()) if isinstance(results, dict) else 'Not a dict'}\")\n",
    "        \n",
    "        if isinstance(results, dict):\n",
    "            for key, value in results.items():\n",
    "                print(f\"    {key}: {type(value)}\")\n",
    "                if hasattr(value, 'shape'):\n",
    "                    print(f\"      Shape: {value.shape}\")\n",
    "                elif isinstance(value, (list, dict)):\n",
    "                    print(f\"      Length: {len(value)}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"✗ End-to-end pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\nelse:\n",
    "    print(\"No end-to-end pipeline available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test individual pipeline components\n",
    "print(\"Testing individual pipeline components...\")\n",
    "\n",
    "# Test embedding pipeline\n",
    "print(\"\\n1. Testing Embedding Pipeline:\")\n",
    "try:\n",
    "    embedding_pipeline = EmbeddingPipeline(config)\n",
    "    # Use smaller subset for testing\n",
    "    small_data = test_data.head(5)\n",
    "    embeddings = embedding_pipeline.run(small_data)\n",
    "    print(f\"  ✓ Embedding pipeline completed\")\n",
    "    print(f\"    Result type: {type(embeddings)}\")\nexcept Exception as e:\n",
    "    print(f\"  ✗ Embedding pipeline failed: {e}\")\n",
    "\n",
    "# Test classification pipeline\n",
    "print(\"\\n2. Testing Classification Pipeline:\")\n",
    "try:\n",
    "    classification_pipeline = ClassificationPipeline(config)\n",
    "    # Split data for classification\n",
    "    split_idx = int(len(test_data) * 0.8)\n",
    "    train_data = test_data[:split_idx]\n",
    "    test_data_split = test_data[split_idx:]\n",
    "    \n",
    "    results = classification_pipeline.run(train_data, test_data_split)\n",
    "    print(f\"  ✓ Classification pipeline completed\")\n",
    "    print(f\"    Result type: {type(results)}\")\nexcept Exception as e:\n",
    "    print(f\"  ✗ Classification pipeline failed: {e}\")\n",
    "\n",
    "# Test evaluation pipeline\n",
    "print(\"\\n3. Testing Evaluation Pipeline:\")\n",
    "try:\n",
    "    evaluation_pipeline = EvaluationPipeline(config)\n",
    "    eval_results = evaluation_pipeline.run(test_data.head(5))\n",
    "    print(f\"  ✓ Evaluation pipeline completed\")\n",
    "    print(f\"    Result type: {type(eval_results)}\")\nexcept Exception as e:\n",
    "    print(f\"  ✗ Evaluation pipeline failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pipeline with different configurations\n",
    "print(\"Testing with different configurations...\")\n",
    "\n",
    "config_files = [\n",
    "    \"/home/kosaraju/mgpt-serve/mgpt_eval/configs/examples/config_embeddings_only.yaml\",\n",
    "    \"/home/kosaraju/mgpt-serve/mgpt_eval/configs/examples/config_evaluation_only.yaml\",\n",
    "    \"/home/kosaraju/mgpt-serve/mgpt_eval/configs/examples/config_training_from_embeddings.yaml\"\n",
    "]\n",
    "\n",
    "for config_file in config_files:\n",
    "    if os.path.exists(config_file):\n",
    "        print(f\"\\nTesting with {os.path.basename(config_file)}:\")\n",
    "        try:\n",
    "            test_config = PipelineConfig.from_yaml(config_file)\n",
    "            print(f\"  Config loaded: {test_config.job.job_name}\")\n",
    "            \n",
    "            # Try to initialize appropriate pipeline\n",
    "            if 'embeddings_only' in config_file:\n",
    "                pipeline = EmbeddingPipeline(test_config)\n",
    "                result = pipeline.run(test_data.head(3))\n",
    "                print(f\"  ✓ Embeddings-only pipeline completed\")\n",
    "                \n",
    "            elif 'evaluation_only' in config_file:\n",
    "                pipeline = EvaluationPipeline(test_config)\n",
    "                result = pipeline.run(test_data.head(3))\n",
    "                print(f\"  ✓ Evaluation-only pipeline completed\")\n",
    "                \n",
    "            elif 'training_from_embeddings' in config_file:\n",
    "                pipeline = ClassificationPipeline(test_config)\n",
    "                train_subset = test_data.head(15)\n",
    "                test_subset = test_data.tail(5)\n",
    "                result = pipeline.run(train_subset, test_subset)\n",
    "                print(f\"  ✓ Training-from-embeddings pipeline completed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Pipeline failed: {e}\")\n",
    "    else:\n",
    "        print(f\"\\nConfig file not found: {config_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug pipeline data flow\n",
    "print(\"=== Pipeline Data Flow Debug ===\")\n",
    "\n",
    "if e2e_pipeline:\n",
    "    # Check pipeline attributes\n",
    "    print(f\"Pipeline config: {hasattr(e2e_pipeline, 'config')}\")\n",
    "    print(f\"Pipeline logger: {hasattr(e2e_pipeline, 'logger')}\")\n",
    "    print(f\"Pipeline components: {hasattr(e2e_pipeline, 'components')}\")\n",
    "    \n",
    "    # Check for sub-pipelines\n",
    "    sub_pipelines = ['embedding_pipeline', 'classification_pipeline', 'evaluation_pipeline']\n",
    "    for sub_pipeline_name in sub_pipelines:\n",
    "        if hasattr(e2e_pipeline, sub_pipeline_name):\n",
    "            sub_pipeline = getattr(e2e_pipeline, sub_pipeline_name)\n",
    "            print(f\"\\n{sub_pipeline_name}: {type(sub_pipeline)}\")\n",
    "            if hasattr(sub_pipeline, 'run'):\n",
    "                print(f\"  Has run method: ✓\")\n",
    "            else:\n",
    "                print(f\"  Has run method: ✗\")\n",
    "        else:\n",
    "            print(f\"\\n{sub_pipeline_name}: Not found\")\n",
    "    \n",
    "    # Check all methods and attributes\n",
    "    all_attrs = [attr for attr in dir(e2e_pipeline) if not attr.startswith('_')]\n",
    "    methods = [attr for attr in all_attrs if callable(getattr(e2e_pipeline, attr))]\n",
    "    properties = [attr for attr in all_attrs if not callable(getattr(e2e_pipeline, attr))]\n",
    "    \n",
    "    print(f\"\\nMethods: {methods}\")\n",
    "    print(f\"Properties: {properties}\")\nelse:\n",
    "    print(\"No end-to-end pipeline available for debugging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pipeline step-by-step execution\n",
    "print(\"Testing step-by-step pipeline execution...\")\n",
    "\n",
    "if e2e_pipeline and hasattr(e2e_pipeline, 'run_step_by_step'):\n",
    "    try:\n",
    "        step_results = e2e_pipeline.run_step_by_step(test_data.head(5))\n",
    "        print(f\"✓ Step-by-step execution completed\")\n",
    "        for step, result in step_results.items():\n",
    "            print(f\"  Step {step}: {type(result)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Step-by-step execution failed: {e}\")\nelse:\n",
    "    print(\"Step-by-step execution not available\")\n",
    "    \n",
    "    # Try manual step-by-step\n",
    "    print(\"\\nTrying manual step-by-step execution...\")\n",
    "    small_data = test_data.head(5)\n",
    "    \n",
    "    # Step 1: Embeddings\n",
    "    try:\n",
    "        embedding_pipeline = EmbeddingPipeline(config)\n",
    "        step1_result = embedding_pipeline.run(small_data)\n",
    "        print(f\"Step 1 (Embeddings): ✓ {type(step1_result)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Step 1 (Embeddings): ✗ {e}\")\n",
    "        step1_result = None\n",
    "    \n",
    "    # Step 2: Classification (if embeddings succeeded)\n",
    "    if step1_result is not None:\n",
    "        try:\n",
    "            classification_pipeline = ClassificationPipeline(config)\n",
    "            # Use embeddings from step 1 if available\n",
    "            step2_result = classification_pipeline.run(small_data[:3], small_data[3:])\n",
    "            print(f\"Step 2 (Classification): ✓ {type(step2_result)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Step 2 (Classification): ✗ {e}\")\n",
    "    \n",
    "    # Step 3: Evaluation\n",
    "    try:\n",
    "        evaluation_pipeline = EvaluationPipeline(config)\n",
    "        step3_result = evaluation_pipeline.run(small_data)\n",
    "        print(f\"Step 3 (Evaluation): ✓ {type(step3_result)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Step 3 (Evaluation): ✗ {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pipeline output handling\n",
    "print(\"Testing pipeline output handling...\")\n",
    "\n",
    "# Check if output directory exists\n",
    "output_dir = config.job.output_base_dir\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Directory exists: {os.path.exists(output_dir)}\")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"✓ Created output directory: {output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to create output directory: {e}\")\n",
    "\n",
    "# Test if pipeline can save results\n",
    "if e2e_pipeline and hasattr(e2e_pipeline, 'save_results'):\n",
    "    try:\n",
    "        mock_results = {'test': 'data', 'embeddings': [[1, 2, 3]], 'predictions': [1, 0, 1]}\n",
    "        e2e_pipeline.save_results(mock_results)\n",
    "        print(f\"✓ Results saving tested\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Results saving failed: {e}\")\nelse:\n",
    "    print(\"No save_results method available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}