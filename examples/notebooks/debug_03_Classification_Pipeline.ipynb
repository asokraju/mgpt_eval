{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Pipeline Debug\n",
    "\n",
    "This notebook debugs the classification pipeline using the actual ClassificationPipeline class.\n",
    "Tests classification training and evaluation using real pipeline methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline modules\n",
    "import sys\n",
    "sys.path.append('/home/kosaraju/mgpt-serve/mgpt_eval')\n",
    "\n",
    "from models.config_models import PipelineConfig\n",
    "from pipelines.classification_pipeline import ClassificationPipeline\n",
    "from pipelines.embedding_pipeline import EmbeddingPipeline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"/home/kosaraju/mgpt-serve/mgpt_eval/configs/examples/config_training_from_embeddings.yaml\"\n",
    "config = PipelineConfig.from_yaml(config_path)\n",
    "print(f\"Config loaded: {config.job.job_name}\")\n",
    "print(f\"Classification config: {config.classification}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classification pipeline\n",
    "classification_pipeline = ClassificationPipeline(config)\n",
    "print(f\"Classification pipeline initialized\")\n",
    "print(f\"Pipeline methods: {[m for m in dir(classification_pipeline) if not m.startswith('_') and callable(getattr(classification_pipeline, m))]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test data\n",
    "train_data = pd.DataFrame({\n",
    "    'mcid': [f'TRAIN_{i:03d}' for i in range(20)],\n",
    "    'claims': [\n",
    "        'N6320 G0378 |eoc| Z91048 M1710',\n",
    "        'E119 A1234 |eoc| B5678 C9012',\n",
    "        'Z03818 D3456 |eoc| F7890 G1234',\n",
    "        'H5678 I9012 |eoc| J1234 K5678',\n",
    "        'L9012 M3456 |eoc| N6320 O7890'\n",
    "    ] * 4,\n",
    "    'label': [1, 1, 1, 0, 1] * 4\n",
    "})\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'mcid': [f'TEST_{i:03d}' for i in range(10)],\n",
    "    'claims': [\n",
    "        'P1234 Q5678 |eoc| E119 R9012',\n",
    "        'S3456 T7890 |eoc| U1234 V5678',\n",
    "        'W9012 X3456 |eoc| Z03818 Y7890',\n",
    "        'Z1234 A5678 |eoc| B9012 C3456',\n",
    "        'D7890 E1234 |eoc| N6320 F5678'\n",
    "    ] * 2,\n",
    "    'label': [1, 0, 1, 0, 1] * 2\n",
    "})\n",
    "\n",
    "print(f\"Training data: {len(train_data)} samples\")\n",
    "print(f\"Test data: {len(test_data)} samples\")\n",
    "print(f\"Train labels distribution: {train_data['label'].value_counts().to_dict()}\")\n",
    "print(f\"Test labels distribution: {test_data['label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for classification (using mock embeddings for testing)\n",
    "print(\"Generating embeddings for classification...\")\n",
    "\n",
    "# Create mock embeddings or use actual embedding pipeline\n",
    "try:\n",
    "    # Try to use actual embedding pipeline\n",
    "    embedding_pipeline = EmbeddingPipeline(config)\n",
    "    train_embeddings = embedding_pipeline.generate_embeddings(train_data)\n",
    "    test_embeddings = embedding_pipeline.generate_embeddings(test_data)\n",
    "    print(f\"✓ Real embeddings generated\")\n",
    "    print(f\"  Train embeddings: {train_embeddings.shape}\")\n",
    "    print(f\"  Test embeddings: {test_embeddings.shape}\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"Real embeddings failed: {e}\")\n",
    "    print(\"Using mock embeddings...\")\n",
    "    \n",
    "    # Create mock embeddings\n",
    "    np.random.seed(42)\n",
    "    train_embeddings = np.random.randn(len(train_data), 768)\n",
    "    test_embeddings = np.random.randn(len(test_data), 768)\n",
    "    print(f\"✓ Mock embeddings created\")\n",
    "    print(f\"  Train embeddings: {train_embeddings.shape}\")\n",
    "    print(f\"  Test embeddings: {test_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test classification training\n",
    "print(\"Testing classification training...\")\n",
    "\n",
    "try:\n",
    "    # Check if pipeline has train method\n",
    "    if hasattr(classification_pipeline, 'train'):\n",
    "        models = classification_pipeline.train(train_embeddings, train_data['label'])\n",
    "        print(f\"✓ Training completed\")\n",
    "        print(f\"  Models trained: {list(models.keys()) if isinstance(models, dict) else type(models)}\")\n",
    "        \n",
    "    elif hasattr(classification_pipeline, 'fit'):\n",
    "        classification_pipeline.fit(train_embeddings, train_data['label'])\n",
    "        print(f\"✓ Fit completed\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No train/fit method found\")\n",
    "        print(f\"Available methods: {[m for m in dir(classification_pipeline) if not m.startswith('_') and callable(getattr(classification_pipeline, m))]}\")\n",
    "        \nexcept Exception as e:\n",
    "    print(f\"✗ Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test classification prediction\n",
    "print(\"Testing classification prediction...\")\n",
    "\n",
    "try:\n",
    "    # Check if pipeline has predict method\n",
    "    if hasattr(classification_pipeline, 'predict'):\n",
    "        predictions = classification_pipeline.predict(test_embeddings)\n",
    "        print(f\"✓ Predictions completed\")\n",
    "        print(f\"  Predictions shape: {predictions.shape if hasattr(predictions, 'shape') else len(predictions)}\")\n",
    "        print(f\"  Predictions: {predictions[:5]}\")\n",
    "        \n",
    "    elif hasattr(classification_pipeline, 'evaluate'):\n",
    "        results = classification_pipeline.evaluate(test_embeddings, test_data['label'])\n",
    "        print(f\"✓ Evaluation completed\")\n",
    "        print(f\"  Results: {results}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No predict/evaluate method found\")\n",
    "        \nexcept Exception as e:\n",
    "    print(f\"✗ Prediction failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test full pipeline run\n",
    "print(\"Testing full classification pipeline run...\")\n",
    "\n",
    "try:\n",
    "    # Use the main run method\n",
    "    results = classification_pipeline.run(train_data, test_data)\n",
    "    print(f\"✓ Pipeline run completed\")\n",
    "    print(f\"  Results type: {type(results)}\")\n",
    "    print(f\"  Results keys: {list(results.keys()) if isinstance(results, dict) else 'Not a dict'}\")\n",
    "    \n",
    "    if isinstance(results, dict):\n",
    "        for key, value in results.items():\n",
    "            print(f\"    {key}: {type(value)}\")\n",
    "            if hasattr(value, 'shape'):\n",
    "                print(f\"      Shape: {value.shape}\")\n",
    "            elif isinstance(value, (list, dict)):\n",
    "                print(f\"      Length: {len(value)}\")\n",
    "            \nexcept Exception as e:\n",
    "    print(f\"✗ Pipeline run failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with existing embeddings\n",
    "print(\"Testing with pre-existing embeddings...\")\n",
    "\n",
    "try:\n",
    "    # Check if pipeline can load existing embeddings\n",
    "    if hasattr(classification_pipeline, 'load_embeddings'):\n",
    "        # Test with a mock embeddings file path\n",
    "        embeddings_path = \"test_embeddings.pkl\"\n",
    "        embeddings = classification_pipeline.load_embeddings(embeddings_path)\n",
    "        print(f\"✓ Embeddings loaded from {embeddings_path}\")\n",
    "        \n",
    "    elif hasattr(classification_pipeline, 'set_embeddings'):\n",
    "        classification_pipeline.set_embeddings(train_embeddings, test_embeddings)\n",
    "        print(f\"✓ Embeddings set directly\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No embedding loading/setting methods found\")\n",
    "        \nexcept Exception as e:\n",
    "    print(f\"Embedding loading test: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug classification pipeline internals\n",
    "print(\"=== Classification Pipeline Debug ===\")\n",
    "\n",
    "# Check pipeline attributes\n",
    "print(f\"Pipeline config: {hasattr(classification_pipeline, 'config')}\")\n",
    "print(f\"Pipeline logger: {hasattr(classification_pipeline, 'logger')}\")\n",
    "print(f\"Pipeline models: {hasattr(classification_pipeline, 'models')}\")\n",
    "\n",
    "# Check configuration\n",
    "if hasattr(classification_pipeline, 'config'):\n",
    "    print(f\"\\nConfig classification: {classification_pipeline.config.classification}\")\n",
    "    if hasattr(classification_pipeline.config, 'classification') and classification_pipeline.config.classification:\n",
    "        print(f\"Classification models: {classification_pipeline.config.classification.models}\")\n",
    "        print(f\"CV folds: {classification_pipeline.config.classification.cross_validation_folds}\")\n",
    "        print(f\"Hyperparameter search: {classification_pipeline.config.classification.hyperparameter_search}\")\n",
    "\n",
    "# Check all methods and attributes\n",
    "all_attrs = [attr for attr in dir(classification_pipeline) if not attr.startswith('_')]\n",
    "methods = [attr for attr in all_attrs if callable(getattr(classification_pipeline, attr))]\n",
    "properties = [attr for attr in all_attrs if not callable(getattr(classification_pipeline, attr))]\n",
    "\n",
    "print(f\"\\nMethods: {methods}\")\n",
    "print(f\"Properties: {properties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test individual classifier methods if available\n",
    "print(\"Testing individual classifier methods...\")\n",
    "\n",
    "# Check if pipeline has individual classifier methods\n",
    "classifier_methods = ['train_logistic_regression', 'train_svm', 'train_random_forest']\n",
    "\n",
    "for method_name in classifier_methods:\n",
    "    if hasattr(classification_pipeline, method_name):\n",
    "        print(f\"Found method: {method_name}\")\n",
    "        try:\n",
    "            method = getattr(classification_pipeline, method_name)\n",
    "            result = method(train_embeddings, train_data['label'])\n",
    "            print(f\"  ✓ {method_name} completed: {type(result)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ {method_name} failed: {e}\")\n",
    "    else:\n",
    "        print(f\"Method not found: {method_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}