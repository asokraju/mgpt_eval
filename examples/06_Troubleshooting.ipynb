{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß Troubleshooting Guide\n",
    "\n",
    "## Overview\n",
    "\n",
    "This guide helps you diagnose and fix common issues with the MGPT-Eval pipeline. Issues are organized by category with symptoms, causes, and step-by-step solutions.\n",
    "\n",
    "## üö® Quick Diagnostics\n",
    "\n",
    "### First Steps for Any Issue:\n",
    "\n",
    "1. **Check the logs**:\n",
    "   ```bash\n",
    "   tail -50 outputs/your_job/logs/pipeline.log\n",
    "   grep -i error outputs/your_job/logs/pipeline.log\n",
    "   ```\n",
    "\n",
    "2. **Validate your configuration**:\n",
    "   ```bash\n",
    "   python main.py validate --config your_config.yaml\n",
    "   ```\n",
    "\n",
    "3. **Test API connectivity**:\n",
    "   ```bash\n",
    "   curl -X GET http://your-server:8000/health\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó API Connection Issues\n",
    "\n",
    "### Issue: Connection Refused\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "ConnectionError: HTTPConnectionPool(host='localhost', port=8000): \n",
    "Max retries exceeded with url: /embeddings_batch\n",
    "```\n",
    "\n",
    "**Diagnosis:**\n",
    "```bash\n",
    "# Test if server is running\n",
    "curl -X GET http://localhost:8000/health\n",
    "\n",
    "# Check if port is open\n",
    "netstat -tlnp | grep 8000\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Start your model server**:\n",
    "   ```bash\n",
    "   # Example server startup\n",
    "   cd /path/to/your/model/server\n",
    "   python app.py\n",
    "   ```\n",
    "\n",
    "2. **Check correct URL in config**:\n",
    "   ```yaml\n",
    "   model_api:\n",
    "     base_url: \"http://localhost:8000\"  # ‚úÖ Correct format\n",
    "     # base_url: \"localhost:8000\"       # ‚ùå Missing protocol\n",
    "     # base_url: \"http://localhost:8000/\" # ‚ùå Trailing slash\n",
    "   ```\n",
    "\n",
    "3. **Try different host configurations**:\n",
    "   ```yaml\n",
    "   # If running in Docker\n",
    "   base_url: \"http://host.docker.internal:8000\"\n",
    "   \n",
    "   # If running on different machine\n",
    "   base_url: \"http://192.168.1.100:8000\"\n",
    "   \n",
    "   # If using domain name\n",
    "   base_url: \"https://mgpt-api.yourcompany.com\"\n",
    "   ```\n",
    "\n",
    "### Issue: Request Timeouts\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "ReadTimeout: HTTPSConnectionPool(host='api.example.com', port=443): \n",
    "Read timed out. (read timeout=300)\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Increase timeout**:\n",
    "   ```yaml\n",
    "   model_api:\n",
    "     timeout: 600    # 10 minutes instead of 5\n",
    "   ```\n",
    "\n",
    "2. **Reduce batch size**:\n",
    "   ```yaml\n",
    "   model_api:\n",
    "     batch_size: 8   # Smaller batches process faster\n",
    "   ```\n",
    "\n",
    "3. **Check server performance**:\n",
    "   ```bash\n",
    "   # Monitor server resources\n",
    "   htop\n",
    "   nvidia-smi  # If using GPU\n",
    "   ```\n",
    "\n",
    "### Issue: Authentication Errors\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "HTTPError: 401 Unauthorized\n",
    "HTTPError: 403 Forbidden\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Add authentication headers** (if required by your server):\n",
    "   ```python\n",
    "   # In your server configuration, add:\n",
    "   headers = {\n",
    "       \"Authorization\": \"Bearer your-api-key\",\n",
    "       \"X-API-Key\": \"your-api-key\"\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. **Check API key validity**:\n",
    "   ```bash\n",
    "   curl -H \"Authorization: Bearer your-key\" http://your-server:8000/health\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Data Issues\n",
    "\n",
    "### Issue: File Not Found\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "FileNotFoundError: [Errno 2] No such file or directory: 'data/medical_claims.csv'\n",
    "```\n",
    "\n",
    "**Diagnosis:**\n",
    "```bash\n",
    "# Check if file exists\n",
    "ls -la data/medical_claims.csv\n",
    "\n",
    "# Check current directory\n",
    "pwd\n",
    "ls -la\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Use absolute path**:\n",
    "   ```yaml\n",
    "   input:\n",
    "     dataset_path: \"/full/path/to/data/medical_claims.csv\"\n",
    "   ```\n",
    "\n",
    "2. **Check relative path from script location**:\n",
    "   ```bash\n",
    "   # Run from mgpt_eval directory\n",
    "   cd /path/to/mgpt_eval\n",
    "   python main.py run-all --config your_config.yaml\n",
    "   ```\n",
    "\n",
    "3. **Verify file permissions**:\n",
    "   ```bash\n",
    "   ls -la data/medical_claims.csv\n",
    "   # Should show read permissions (r)\n",
    "   ```\n",
    "\n",
    "### Issue: Invalid CSV Format\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "ValueError: Missing required columns. Expected: ['mcid', 'claims', 'label']\n",
    "Found: ['id', 'text', 'target']\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Check CSV format**:\n",
    "   ```bash\n",
    "   head -3 data/medical_claims.csv\n",
    "   ```\n",
    "\n",
    "2. **Required format**:\n",
    "   ```csv\n",
    "   mcid,claims,label\n",
    "   CLAIM_001,\"N6320 G0378 |eoc| Z91048 M1710\",1\n",
    "   CLAIM_002,\"E119 76642 |eoc| K9289 O0903\",0\n",
    "   ```\n",
    "\n",
    "3. **Fix column names** in your CSV:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   \n",
    "   # Load and rename columns\n",
    "   df = pd.read_csv('data/your_file.csv')\n",
    "   df = df.rename(columns={\n",
    "       'id': 'mcid',\n",
    "       'text': 'claims', \n",
    "       'target': 'label'\n",
    "   })\n",
    "   df.to_csv('data/medical_claims.csv', index=False)\n",
    "   ```\n",
    "\n",
    "### Issue: Empty or Invalid Data\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "ValueError: Dataset is empty after loading\n",
    "ValueError: All labels are the same class\n",
    "```\n",
    "\n",
    "**Diagnosis:**\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/medical_claims.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Label distribution: {df['label'].value_counts()}\")\n",
    "print(f\"Missing values: {df.isnull().sum()}\")\n",
    "print(f\"Sample rows:\")\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Remove empty rows**:\n",
    "   ```python\n",
    "   df = df.dropna(subset=['mcid', 'claims', 'label'])\n",
    "   ```\n",
    "\n",
    "2. **Check label format**:\n",
    "   ```python\n",
    "   # Labels must be 0 or 1\n",
    "   print(df['label'].unique())  # Should show [0, 1] or [1, 0]\n",
    "   \n",
    "   # Convert if necessary\n",
    "   df['label'] = df['label'].astype(int)\n",
    "   ```\n",
    "\n",
    "3. **Ensure minimum dataset size**:\n",
    "   ```python\n",
    "   if len(df) < 100:\n",
    "       print(\"Warning: Very small dataset, results may be unreliable\")\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration Issues\n",
    "\n",
    "### Issue: Invalid Configuration Values\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "ValidationError: split_ratio must be between 0.1 and 0.9\n",
    "ValidationError: target_codes is required when target_word_eval is enabled\n",
    "```\n",
    "\n",
    "**Common Fixes:**\n",
    "\n",
    "1. **Split ratio issues**:\n",
    "   ```yaml\n",
    "   input:\n",
    "     split_ratio: 0.8    # ‚úÖ Valid (0.1 to 0.9)\n",
    "     # split_ratio: 1.2  # ‚ùå Invalid (>0.9)\n",
    "     # split_ratio: 0.05 # ‚ùå Invalid (<0.1)\n",
    "   ```\n",
    "\n",
    "2. **Missing target codes**:\n",
    "   ```yaml\n",
    "   pipeline_stages:\n",
    "     target_word_eval: true\n",
    "   \n",
    "   target_word_evaluation:\n",
    "     enable: true\n",
    "     target_codes: [\"E119\", \"I10\"]  # ‚úÖ Required when enabled\n",
    "   ```\n",
    "\n",
    "3. **Invalid batch sizes**:\n",
    "   ```yaml\n",
    "   model_api:\n",
    "     batch_size: 32      # ‚úÖ Valid (1-512)\n",
    "     # batch_size: 0     # ‚ùå Invalid (too low)\n",
    "     # batch_size: 1000  # ‚ùå Invalid (too high)\n",
    "   ```\n",
    "\n",
    "### Issue: Conflicting Configuration\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "ConfigError: Cannot use both dataset_path and train_dataset_path\n",
    "ConfigError: target_word_eval enabled but no target_codes provided\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Input data conflicts**:\n",
    "   ```yaml\n",
    "   # ‚úÖ Option 1: Single dataset\n",
    "   input:\n",
    "     dataset_path: \"data/claims.csv\"\n",
    "     split_ratio: 0.8\n",
    "   \n",
    "   # ‚úÖ Option 2: Separate files  \n",
    "   input:\n",
    "     train_dataset_path: \"data/train.csv\"\n",
    "     test_dataset_path: \"data/test.csv\"\n",
    "   \n",
    "   # ‚ùå Don't use both\n",
    "   ```\n",
    "\n",
    "2. **Stage dependencies**:\n",
    "   ```yaml\n",
    "   # ‚úÖ Valid: Enable target evaluation with codes\n",
    "   pipeline_stages:\n",
    "     target_word_eval: true\n",
    "   target_word_evaluation:\n",
    "     target_codes: [\"E119\"]\n",
    "   \n",
    "   # ‚úÖ Valid: Disable target evaluation\n",
    "   pipeline_stages:\n",
    "     target_word_eval: false\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Embedding Generation Issues\n",
    "\n",
    "### Issue: Out of Memory\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "RuntimeError: CUDA out of memory\n",
    "MemoryError: Unable to allocate array\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Reduce batch sizes**:\n",
    "   ```yaml\n",
    "   model_api:\n",
    "     batch_size: 8         # Reduce from 32\n",
    "   \n",
    "   embedding_generation:\n",
    "     batch_size: 4         # Process fewer at once\n",
    "   ```\n",
    "\n",
    "2. **Reduce sequence length**:\n",
    "   ```yaml\n",
    "   data_processing:\n",
    "     max_sequence_length: 256  # Reduce from 512\n",
    "   ```\n",
    "\n",
    "3. **Enable more frequent checkpoints**:\n",
    "   ```yaml\n",
    "   embedding_generation:\n",
    "     save_interval: 25     # Save more often\n",
    "   ```\n",
    "\n",
    "4. **Use CSV format for large datasets**:\n",
    "   ```yaml\n",
    "   data_processing:\n",
    "     output_format: \"csv\"  # More memory efficient\n",
    "   ```\n",
    "\n",
    "### Issue: Slow Embedding Generation\n",
    "\n",
    "**Symptoms:**\n",
    "- Process takes much longer than expected\n",
    "- Low GPU/CPU utilization\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Increase batch sizes** (if memory allows):\n",
    "   ```yaml\n",
    "   model_api:\n",
    "     batch_size: 64        # Increase if server can handle\n",
    "   ```\n",
    "\n",
    "2. **Optimize server settings**:\n",
    "   ```python\n",
    "   # In your model server\n",
    "   # Enable batch processing\n",
    "   # Use GPU if available\n",
    "   # Optimize tokenization\n",
    "   ```\n",
    "\n",
    "3. **Monitor progress**:\n",
    "   ```bash\n",
    "   tail -f outputs/your_job/logs/pipeline.log\n",
    "   ```\n",
    "\n",
    "### Issue: Embedding Dimension Mismatch\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "ValueError: All embeddings must have the same dimension\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Check API response format**:\n",
    "   ```bash\n",
    "   curl -X POST http://localhost:8000/embeddings_batch \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"texts\": [\"E119 I10\"]}'\n",
    "   ```\n",
    "\n",
    "2. **Ensure consistent model**:\n",
    "   - Don't change model during embedding generation\n",
    "   - Use same model configuration\n",
    "\n",
    "3. **Clear corrupted checkpoints**:\n",
    "   ```bash\n",
    "   rm -rf outputs/checkpoints/*\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Classification Issues\n",
    "\n",
    "### Issue: Poor Classification Performance\n",
    "\n",
    "**Symptoms:**\n",
    "- All classifiers get <70% accuracy\n",
    "- ROC-AUC near 0.5 (random performance)\n",
    "\n",
    "**Diagnosis:**\n",
    "```python\n",
    "# Check data quality\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load embeddings\n",
    "with open('outputs/job/embeddings/train_embeddings.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Check label distribution\n",
    "labels = data['labels']\n",
    "print(f\"Label distribution: {pd.Series(labels).value_counts()}\")\n",
    "\n",
    "# Check embedding quality\n",
    "embeddings = data['embeddings']\n",
    "print(f\"Embedding shape: {len(embeddings)} x {len(embeddings[0])}\")\n",
    "print(f\"Embedding stats: mean={np.mean(embeddings):.3f}, std={np.std(embeddings):.3f}\")\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Check data quality**:\n",
    "   - Verify labels are correct\n",
    "   - Ensure sufficient data (>1000 samples recommended)\n",
    "   - Check for data leakage\n",
    "\n",
    "2. **Verify model compatibility**:\n",
    "   ```yaml\n",
    "   # Ensure your model was trained on medical data\n",
    "   # Check that embeddings capture medical semantics\n",
    "   ```\n",
    "\n",
    "3. **Try different hyperparameters**:\n",
    "   ```yaml\n",
    "   classification:\n",
    "     hyperparameter_search:\n",
    "       logistic_regression:\n",
    "         C: [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]  # Wider range\n",
    "   ```\n",
    "\n",
    "### Issue: Overfitting\n",
    "\n",
    "**Symptoms:**\n",
    "- High training accuracy, low test accuracy\n",
    "- Large gap between CV and test scores\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Increase regularization**:\n",
    "   ```yaml\n",
    "   classification:\n",
    "     hyperparameter_search:\n",
    "       logistic_regression:\n",
    "         C: [0.001, 0.01, 0.1]  # Lower C = more regularization\n",
    "   ```\n",
    "\n",
    "2. **Increase dataset size**:\n",
    "   - Collect more training data\n",
    "   - Use data augmentation if applicable\n",
    "\n",
    "3. **Use more cross-validation folds**:\n",
    "   ```yaml\n",
    "   classification:\n",
    "     cross_validation:\n",
    "       n_folds: 10  # More thorough validation\n",
    "   ```\n",
    "\n",
    "### Issue: Training Takes Too Long\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Reduce hyperparameter search space**:\n",
    "   ```yaml\n",
    "   classification:\n",
    "     hyperparameter_search:\n",
    "       logistic_regression:\n",
    "         C: [0.1, 1, 10]  # Fewer options\n",
    "       svm:\n",
    "         C: [1]           # Skip SVM for large datasets\n",
    "   ```\n",
    "\n",
    "2. **Use parallel processing**:\n",
    "   ```yaml\n",
    "   classification:\n",
    "     cross_validation:\n",
    "       n_jobs: -1  # Use all CPU cores\n",
    "   ```\n",
    "\n",
    "3. **Train fewer models**:\n",
    "   ```yaml\n",
    "   classification:\n",
    "     models: [\"logistic_regression\"]  # Skip slower models\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Target Word Evaluation Issues\n",
    "\n",
    "### Issue: Low Recall (Missing Positive Cases)\n",
    "\n",
    "**Symptoms:**\n",
    "- Target word method has much lower recall than embedding method\n",
    "- Many true positive cases get 0 predictions\n",
    "\n",
    "**Diagnosis:**\n",
    "```bash\n",
    "# Check target word results\n",
    "grep \"found_count\" outputs/job/metrics/target_word_evaluation/target_word_eval_summary.json\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Increase generation diversity**:\n",
    "   ```yaml\n",
    "   target_word_evaluation:\n",
    "     temperature: 0.9           # More randomness\n",
    "     top_k: 100                 # Wider vocabulary\n",
    "     generations_per_prompt: 20 # More attempts\n",
    "   ```\n",
    "\n",
    "2. **Add related target codes**:\n",
    "   ```yaml\n",
    "   target_word_evaluation:\n",
    "     target_codes: [\n",
    "       \"E119\",   # Type 2 diabetes\n",
    "       \"E1022\",  # Type 1 diabetes with CKD\n",
    "       \"E1040\",  # Type 1 diabetes with neuropathy\n",
    "       \"E1051\"   # Type 1 diabetes with circulatory complications\n",
    "     ]\n",
    "   ```\n",
    "\n",
    "3. **Increase generation length**:\n",
    "   ```yaml\n",
    "   target_word_evaluation:\n",
    "     max_new_tokens: 300  # Longer sequences for more chances\n",
    "   ```\n",
    "\n",
    "### Issue: High False Positive Rate\n",
    "\n",
    "**Symptoms:**\n",
    "- Target codes appear in negative cases\n",
    "- Much higher false positive rate than embedding method\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Use more specific target codes**:\n",
    "   ```yaml\n",
    "   target_word_evaluation:\n",
    "     # Instead of general codes like \"Z0000\"\n",
    "     target_codes: [\"E119\", \"I259\"]  # More specific conditions\n",
    "   ```\n",
    "\n",
    "2. **Reduce generation randomness**:\n",
    "   ```yaml\n",
    "   target_word_evaluation:\n",
    "     temperature: 0.6     # Less randomness\n",
    "     top_k: 30           # More focused vocabulary\n",
    "   ```\n",
    "\n",
    "3. **Review label quality**:\n",
    "   ```python\n",
    "   # Check if negative cases should actually be positive\n",
    "   false_positives = df[(df['true_label'] == 0) & (df['predicted_label'] == 1)]\n",
    "   print(false_positives[['mcid', 'input_claim', 'found_codes']].head())\n",
    "   ```\n",
    "\n",
    "### Issue: No Target Codes Found\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "Warning: No target codes found in any generations\n",
    "All predictions are 0 (negative)\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Check target code validity**:\n",
    "   ```python\n",
    "   # Verify codes exist in your domain\n",
    "   target_codes = [\"E119\", \"I10\", \"N6320\"]\n",
    "   \n",
    "   # Check if codes appear in training data\n",
    "   training_claims = df['claims'].str.cat(sep=' ')\n",
    "   for code in target_codes:\n",
    "       count = training_claims.count(code)\n",
    "       print(f\"{code}: {count} occurrences\")\n",
    "   ```\n",
    "\n",
    "2. **Test generation manually**:\n",
    "   ```bash\n",
    "   curl -X POST http://localhost:8000/generate_batch \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\n",
    "       \"prompts\": [\"E119 I10 |eoc|\"],\n",
    "       \"max_new_tokens\": 100,\n",
    "       \"temperature\": 0.8,\n",
    "       \"num_return_sequences\": 5\n",
    "     }'\n",
    "   ```\n",
    "\n",
    "3. **Adjust generation parameters**:\n",
    "   ```yaml\n",
    "   target_word_evaluation:\n",
    "     max_new_tokens: 500    # Much longer generations\n",
    "     temperature: 1.0       # Maximum diversity\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Performance & Resource Issues\n",
    "\n",
    "### Issue: High Memory Usage\n",
    "\n",
    "**Symptoms:**\n",
    "- Process killed by OOM killer\n",
    "- System becomes unresponsive\n",
    "\n",
    "**Monitor memory usage**:\n",
    "```bash\n",
    "# Monitor during pipeline execution\n",
    "watch -n 5 'free -h && ps aux | grep python | head -5'\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Memory-efficient configuration**:\n",
    "   ```yaml\n",
    "   data_processing:\n",
    "     output_format: \"csv\"          # More efficient than JSON\n",
    "   \n",
    "   embedding_generation:\n",
    "     batch_size: 8                 # Smaller batches\n",
    "     save_interval: 25             # Frequent saves, less memory\n",
    "   \n",
    "   model_api:\n",
    "     batch_size: 16                # Smaller API batches\n",
    "   ```\n",
    "\n",
    "2. **Process data in chunks**:\n",
    "   ```python\n",
    "   # For very large datasets, split into smaller files\n",
    "   import pandas as pd\n",
    "   \n",
    "   df = pd.read_csv('large_dataset.csv')\n",
    "   chunk_size = 1000\n",
    "   \n",
    "   for i in range(0, len(df), chunk_size):\n",
    "       chunk = df[i:i+chunk_size]\n",
    "       chunk.to_csv(f'chunk_{i//chunk_size}.csv', index=False)\n",
    "   ```\n",
    "\n",
    "3. **Use streaming processing** (for future optimization):\n",
    "   ```yaml\n",
    "   # Enable memory cleanup (if available)\n",
    "   system:\n",
    "     cleanup_intermediate_files: true\n",
    "     memory_limit_mb: 8192\n",
    "   ```\n",
    "\n",
    "### Issue: Slow Performance\n",
    "\n",
    "**Diagnosis:**\n",
    "```bash\n",
    "# Check what's taking time\n",
    "grep \"took\" outputs/job/logs/pipeline.log\n",
    "grep \"completed\" outputs/job/logs/pipeline.log\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Optimize bottlenecks**:\n",
    "   ```yaml\n",
    "   # If embedding generation is slow\n",
    "   model_api:\n",
    "     batch_size: 64        # Larger batches (if memory allows)\n",
    "   \n",
    "   # If classification is slow\n",
    "   classification:\n",
    "     models: [\"logistic_regression\"]  # Skip slower models\n",
    "     cross_validation:\n",
    "       n_folds: 3          # Fewer folds\n",
    "   ```\n",
    "\n",
    "2. **Parallel processing**:\n",
    "   ```yaml\n",
    "   classification:\n",
    "     cross_validation:\n",
    "       n_jobs: -1          # Use all CPU cores\n",
    "   ```\n",
    "\n",
    "3. **Skip unnecessary stages**:\n",
    "   ```yaml\n",
    "   pipeline_stages:\n",
    "     embeddings: false     # Use existing embeddings\n",
    "     target_word_eval: false  # Skip if not needed\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Debugging Strategies\n",
    "\n",
    "### Enable Debug Logging\n",
    "\n",
    "```yaml\n",
    "logging:\n",
    "  level: \"DEBUG\"              # Show detailed information\n",
    "  console_level: \"DEBUG\"      # Also show on console\n",
    "```\n",
    "\n",
    "### Test with Small Dataset\n",
    "\n",
    "```python\n",
    "# Create small test dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/medical_claims.csv')\n",
    "small_df = df.head(100)  # Just 100 samples\n",
    "small_df.to_csv('data/test_small.csv', index=False)\n",
    "```\n",
    "\n",
    "```yaml\n",
    "# Test configuration\n",
    "input:\n",
    "  dataset_path: \"data/test_small.csv\"\n",
    "\n",
    "target_word_evaluation:\n",
    "  generations_per_prompt: 3   # Fewer generations for speed\n",
    "```\n",
    "\n",
    "### Validate Each Stage Separately\n",
    "\n",
    "```bash\n",
    "# Test embedding generation only\n",
    "python main.py run-embeddings --config test_config.yaml\n",
    "\n",
    "# Test classification only (requires embeddings)\n",
    "python main.py run-classification --config test_config.yaml\n",
    "\n",
    "# Test target evaluation only\n",
    "python main.py run-target-eval --config test_config.yaml\n",
    "```\n",
    "\n",
    "### Check Intermediate Files\n",
    "\n",
    "```bash\n",
    "# Verify embeddings were generated\n",
    "ls -la outputs/job/embeddings/\n",
    "head -5 outputs/job/embeddings/train_embeddings.json\n",
    "\n",
    "# Check model files\n",
    "ls -la outputs/job/models/\n",
    "\n",
    "# Verify metrics\n",
    "ls -la outputs/job/metrics/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üÜò Getting Help\n",
    "\n",
    "### Collect Diagnostic Information\n",
    "\n",
    "When reporting issues, include:\n",
    "\n",
    "1. **Configuration file**:\n",
    "   ```bash\n",
    "   cat your_config.yaml\n",
    "   ```\n",
    "\n",
    "2. **Error logs**:\n",
    "   ```bash\n",
    "   tail -100 outputs/job/logs/pipeline.log\n",
    "   ```\n",
    "\n",
    "3. **System information**:\n",
    "   ```bash\n",
    "   python --version\n",
    "   pip list | grep -E \"pandas|numpy|sklearn|pydantic\"\n",
    "   free -h\n",
    "   df -h\n",
    "   ```\n",
    "\n",
    "4. **Data sample**:\n",
    "   ```bash\n",
    "   head -5 data/medical_claims.csv\n",
    "   wc -l data/medical_claims.csv\n",
    "   ```\n",
    "\n",
    "### Common Solutions Checklist\n",
    "\n",
    "Before seeking help, try these common fixes:\n",
    "\n",
    "- [ ] **Restart model server** and test API connectivity\n",
    "- [ ] **Check file paths** are correct and files exist\n",
    "- [ ] **Validate configuration** using built-in validation\n",
    "- [ ] **Test with small dataset** (100 samples) first\n",
    "- [ ] **Check logs** for specific error messages\n",
    "- [ ] **Verify data format** matches requirements\n",
    "- [ ] **Ensure sufficient disk space** and memory\n",
    "- [ ] **Try default template** configuration first\n",
    "\n",
    "### Progressive Debugging Approach\n",
    "\n",
    "1. **Start simple**: Use template configuration with small dataset\n",
    "2. **Test connectivity**: Verify API endpoints work\n",
    "3. **Validate data**: Ensure CSV format is correct\n",
    "4. **Run one stage**: Test embedding generation alone\n",
    "5. **Scale up gradually**: Increase dataset size and complexity\n",
    "6. **Monitor resources**: Watch memory and CPU usage\n",
    "7. **Read logs carefully**: Error messages are usually informative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Next Steps\n",
    "\n",
    "- **[07_Advanced_Usage.ipynb](07_Advanced_Usage.ipynb)** - Production deployment and optimization\n",
    "- **[01_Introduction_to_MGPT_Eval.ipynb](01_Introduction_to_MGPT_Eval.ipynb)** - Return to basics if needed\n",
    "- **[04_Configuration_Guide.ipynb](04_Configuration_Guide.ipynb)** - Review configuration options\n",
    "\n",
    "### Quick Diagnostic Commands:\n",
    "\n",
    "```bash\n",
    "# Test API connectivity\n",
    "curl -X GET http://localhost:8000/health\n",
    "\n",
    "# Validate configuration\n",
    "python main.py validate --config your_config.yaml\n",
    "\n",
    "# Check recent logs\n",
    "tail -50 outputs/*/logs/pipeline.log\n",
    "\n",
    "# Monitor resource usage\n",
    "htop\n",
    "```\n",
    "\n",
    "Remember: Most issues are configuration-related and can be fixed by carefully reading error messages and checking the examples in this troubleshooting guide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}