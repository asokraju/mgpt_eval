{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e-title",
   "metadata": {},
   "source": [
    "# End-to-End Pipeline Example\n",
    "\n",
    "This notebook demonstrates the complete MediClaimGPT pipeline from raw medical claims to trained classifiers.\n",
    "\n",
    "## Overview\n",
    "- **Input**: Raw medical claims CSV file\n",
    "- **Process**: Generate embeddings ‚Üí Train classifiers ‚Üí Evaluate models\n",
    "- **Output**: Embeddings, trained models, performance metrics, and reports\n",
    "\n",
    "## Pipeline Flow\n",
    "1. **Data Loading**: Load and preprocess medical claims\n",
    "2. **Embedding Generation**: Generate embeddings using MediClaimGPT API\n",
    "3. **Classification Training**: Train multiple binary classifiers\n",
    "4. **Model Evaluation**: Evaluate and compare model performance\n",
    "5. **Results Summary**: Generate comprehensive reports\n",
    "\n",
    "## Prerequisites\n",
    "- MediClaimGPT API server running on `http://localhost:8000`\n",
    "- Medical claims dataset in CSV format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e-setup",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/kosaraju/mgpt_eval\n",
      "Working directory: /home/kosaraju/mgpt_eval/examples\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from pipelines.end_to_end_pipeline import EndToEndPipeline\n",
    "from models.config_models import PipelineConfig\n",
    "from models.pipeline_models import PipelineJob\n",
    "from utils.logging_utils import get_logger\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api-check",
   "metadata": {},
   "source": [
    "## API Server Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "check-api",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Server Status: ‚úÖ Available\n",
      "API URL: http://localhost:8000\n",
      "\n",
      "‚úÖ API server is ready for embedding generation!\n"
     ]
    }
   ],
   "source": [
    "# Check if MediClaimGPT API is running\n",
    "api_url = \"http://localhost:8000\"\n",
    "\n",
    "def check_api_health(base_url, timeout=5):\n",
    "    \"\"\"Check if API server is running and responsive.\"\"\"\n",
    "    try:\n",
    "        # Try to reach the health endpoint or any basic endpoint\n",
    "        response = requests.get(f\"{base_url}/docs\", timeout=timeout)\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False\n",
    "\n",
    "# Test API connectivity\n",
    "api_available = check_api_health(api_url)\n",
    "print(f\"API Server Status: {'‚úÖ Available' if api_available else '‚ùå Not Available'}\")\n",
    "print(f\"API URL: {api_url}\")\n",
    "\n",
    "if not api_available:\n",
    "    print(\"\\n‚ö†Ô∏è Warning: API server is not available!\")\n",
    "    print(\"Please start the MediClaimGPT API server before running this pipeline.\")\n",
    "    print(\"The pipeline will fail without a running API server.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ API server is ready for embedding generation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e-config",
   "metadata": {},
   "source": [
    "## Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e-load-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "Job name: end_to_end_pipeline_example\n",
      "Dataset path: /home/kosaraju/mgpt_eval/examples/data/medical_claims_complete.csv\n",
      "Output directory: /home/kosaraju/mgpt_eval/examples/outputs\n",
      "\n",
      "Pipeline stages enabled:\n",
      "  ‚úÖ embeddings\n",
      "  ‚úÖ classification\n",
      "  ‚úÖ evaluation\n",
      "  ‚ùå target_word_eval\n",
      "  ‚úÖ summary_report\n",
      "  ‚úÖ method_comparison\n",
      "\n",
      "Classifiers to train: ['logistic_regression', 'svm', 'random_forest']\n",
      "API base URL: http://localhost:8000\n",
      "Embedding batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# Load end-to-end configuration\n",
    "config_path = \"configs/end_to_end_example_config.yaml\"\n",
    "config = PipelineConfig.from_yaml(config_path)\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Job name: {config.job.name}\")\n",
    "print(f\"Dataset path: {config.input.dataset_path}\")\n",
    "print(f\"Output directory: {config.job.output_dir}\")\n",
    "print(f\"\\nPipeline stages enabled:\")\n",
    "for stage, enabled in config.pipeline_stages.model_dump().items():\n",
    "    status = \"‚úÖ\" if enabled else \"‚ùå\"\n",
    "    print(f\"  {status} {stage}\")\n",
    "\n",
    "print(f\"\\nClassifiers to train: {config.classification.models}\")\n",
    "print(f\"API base URL: {config.model_api.base_url}\")\n",
    "print(f\"Embedding batch size: {config.embedding_generation.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e-data-check",
   "metadata": {},
   "source": [
    "## Input Data Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e-check-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /home/kosaraju/mgpt_eval/examples/data/medical_claims_complete.csv\n",
      "Dataset exists: True\n",
      "\n",
      "Dataset shape: (50, 3)\n",
      "Columns: ['mcid', 'claims', 'label']\n",
      "\n",
      "‚ùå Missing required columns: ['claim']\n"
     ]
    }
   ],
   "source": [
    "# Check input dataset\n",
    "dataset_path = Path(config.input.dataset_path)\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "print(f\"Dataset exists: {dataset_path.exists()}\")\n",
    "\n",
    "if dataset_path.exists():\n",
    "    # Load and inspect the dataset\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(f\"\\nDataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_cols = ['mcid', 'claim', 'label']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"\\n‚ùå Missing required columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ All required columns present: {required_cols}\")\n",
    "        \n",
    "        # Show data statistics\n",
    "        print(f\"\\nData statistics:\")\n",
    "        print(f\"  Total claims: {len(df)}\")\n",
    "        print(f\"  Unique MCIDs: {df['mcid'].nunique()}\")\n",
    "        print(f\"  Label distribution:\")\n",
    "        print(f\"    {df['label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nSample data:\")\n",
    "        print(df.head(3).to_string(max_colwidth=50))\n",
    "else:\n",
    "    print(\"\\n‚ùå Dataset file not found!\")\n",
    "    print(\"Please ensure the dataset exists or update the config file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e-pipeline-init",
   "metadata": {},
   "source": [
    "## Initialize End-to-End Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e-init-pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End-to-end pipeline initialized successfully!\n",
      "Pipeline will process: /home/kosaraju/mgpt_eval/examples/data/medical_claims_complete.csv\n",
      "Results will be saved to: /home/kosaraju/mgpt_eval/examples/outputs\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logger = get_logger(\"end_to_end_pipeline\", config.logging)\n",
    "\n",
    "# Create job configuration from the pipeline config\n",
    "job_config = PipelineJob(\n",
    "    dataset_path=config.input.dataset_path,\n",
    "    job_name=config.job.name\n",
    ")\n",
    "\n",
    "# Initialize end-to-end pipeline\n",
    "pipeline = EndToEndPipeline(config, job_config)\n",
    "\n",
    "print(\"End-to-end pipeline initialized successfully!\")\n",
    "print(f\"Pipeline will process: {config.input.dataset_path}\")\n",
    "print(f\"Results will be saved to: {config.job.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e-run-pipeline",
   "metadata": {},
   "source": [
    "## Run End-to-End Pipeline\n",
    "\n",
    "This will execute all enabled pipeline stages in sequence:\n",
    "1. **Embedding Generation**: Process claims through MediClaimGPT API\n",
    "2. **Classification Training**: Train binary classifiers on embeddings\n",
    "3. **Model Evaluation**: Evaluate trained models\n",
    "4. **Report Generation**: Create comprehensive performance reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e-execute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting end-to-end pipeline execution...\n",
      "This may take several minutes depending on dataset size and API response time.\n",
      "\n",
      "2025-06-04 00:28:00,090 - end_to_end_pipeline - \u001b[31mERROR\u001b[0m - Configuration validation failed: Target word evaluation enabled but no target words specified\n",
      "\n",
      "‚ùå Pipeline execution failed after 0.03 seconds\n",
      "Error: Configuration validation failed: Target word evaluation enabled but no target words specified\n",
      "\n",
      "Full error traceback:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_126133/1660034385.py\", line 8, in <module>\n",
      "    results = pipeline.run()\n",
      "  File \"/home/kosaraju/mgpt_eval/pipelines/end_to_end_pipeline.py\", line 386, in run\n",
      "    raise ValueError(error_msg)\n",
      "ValueError: Configuration validation failed: Target word evaluation enabled but no target words specified\n"
     ]
    }
   ],
   "source": [
    "# Run the complete end-to-end pipeline\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    print(\"Starting end-to-end pipeline execution...\")\n",
    "    print(\"This may take several minutes depending on dataset size and API response time.\\n\")\n",
    "    \n",
    "    results = pipeline.run()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nüéâ End-to-end pipeline completed successfully!\")\n",
    "    print(f\"‚è±Ô∏è  Total execution time: {execution_time:.2f} seconds ({execution_time/60:.1f} minutes)\")\n",
    "    print(f\"üìä Results: {results}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n‚ùå Pipeline execution failed after {execution_time:.2f} seconds\")\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "    import traceback\n",
    "    print(\"\\nFull error traceback:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e-results-overview",
   "metadata": {},
   "source": [
    "## Results Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2e-results-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç PIPELINE OUTPUTS SUMMARY\n",
      "==================================================\n",
      "Job output directory: /home/kosaraju/mgpt_eval/examples/outputs/end_to_end_pipeline_example\n",
      "\n",
      "üìä Embeddings (0 files):\n",
      "\n",
      "ü§ñ Trained Models:\n",
      "  Model directories: 0\n",
      "  Model files: 0\n",
      "\n",
      "üìà Evaluation Results:\n",
      "  - Metric files: 0\n",
      "  - CSV reports: 0\n",
      "\n",
      "üìù Log Files: 6\n",
      "  - embedding_pipeline.log\n",
      "  - end_to_end_pipeline.log\n",
      "  - classification_pipeline.log\n",
      "  - end_to_end_pipeline.json\n",
      "  - classification_pipeline.json\n",
      "  - embedding_pipeline.json\n"
     ]
    }
   ],
   "source": [
    "# Analyze generated outputs\n",
    "output_dir = Path(config.job.output_dir)\n",
    "\n",
    "# The EndToEndPipeline creates a job-specific subdirectory\n",
    "job_output_dir = output_dir / config.job.name\n",
    "embeddings_dir = job_output_dir / \"embeddings\"\n",
    "models_dir = job_output_dir / \"models\"\n",
    "metrics_dir = job_output_dir / \"metrics\"\n",
    "logs_dir = output_dir / \"logs\"\n",
    "\n",
    "print(\"üîç PIPELINE OUTPUTS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Job output directory: {job_output_dir}\")\n",
    "\n",
    "# Embeddings\n",
    "if embeddings_dir.exists():\n",
    "    embedding_files = list(embeddings_dir.glob(\"*.csv\")) + list(embeddings_dir.glob(\"*.json\"))\n",
    "    print(f\"\\nüìä Embeddings ({len(embedding_files)} files):\")\n",
    "    for file in embedding_files:\n",
    "        if file.suffix == '.csv':\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                print(f\"  - {file.name}: {df.shape[0]} embeddings\")\n",
    "            except:\n",
    "                print(f\"  - {file.name}: (could not read)\")\n",
    "        else:\n",
    "            print(f\"  - {file.name}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå No embeddings found in {embeddings_dir}\")\n",
    "\n",
    "# Models\n",
    "if models_dir.exists():\n",
    "    model_dirs = [d for d in models_dir.iterdir() if d.is_dir()]\n",
    "    model_files = list(models_dir.glob(\"*.pkl\"))\n",
    "    print(f\"\\nü§ñ Trained Models:\")\n",
    "    print(f\"  Model directories: {len(model_dirs)}\")\n",
    "    print(f\"  Model files: {len(model_files)}\")\n",
    "    \n",
    "    for model_dir in model_dirs:\n",
    "        model_files_in_dir = list(model_dir.glob(\"*.pkl\"))\n",
    "        metric_files_in_dir = list(model_dir.glob(\"*.json\"))\n",
    "        print(f\"    - {model_dir.name}: {len(model_files_in_dir)} models, {len(metric_files_in_dir)} metrics\")\n",
    "    \n",
    "    for model_file in model_files:\n",
    "        print(f\"    - {model_file.name}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå No models found in {models_dir}\")\n",
    "\n",
    "# Metrics\n",
    "if metrics_dir.exists():\n",
    "    metric_files = list(metrics_dir.glob(\"*.json\"))\n",
    "    csv_files = list(metrics_dir.glob(\"*.csv\"))\n",
    "    print(f\"\\nüìà Evaluation Results:\")\n",
    "    print(f\"  - Metric files: {len(metric_files)}\")\n",
    "    print(f\"  - CSV reports: {len(csv_files)}\")\n",
    "    for file in metric_files + csv_files:\n",
    "        print(f\"    ‚Ä¢ {file.name}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå No metrics found in {metrics_dir}\")\n",
    "\n",
    "# Logs\n",
    "if logs_dir.exists():\n",
    "    log_files = list(logs_dir.glob(\"*.log\")) + list(logs_dir.glob(\"*.json\"))\n",
    "    print(f\"\\nüìù Log Files: {len(log_files)}\")\n",
    "    for file in log_files:\n",
    "        print(f\"  - {file.name}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå No logs found in {logs_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e-performance",
   "metadata": {},
   "source": [
    "## Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2e-analyze-performance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ MODEL PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "No model metrics found in the expected format.\n"
     ]
    }
   ],
   "source": [
    "# Load and compare model performance\n",
    "output_dir = Path(config.job.output_dir)\n",
    "job_output_dir = output_dir / config.job.name\n",
    "models_dir = job_output_dir / \"models\"\n",
    "\n",
    "if models_dir.exists():\n",
    "    model_dirs = [d for d in models_dir.iterdir() if d.is_dir()]\n",
    "    model_files = list(models_dir.glob(\"*.pkl\"))\n",
    "    \n",
    "    # Check for metrics in both subdirectories and root models directory\n",
    "    all_metric_files = list(models_dir.glob(\"*.json\"))\n",
    "    for model_dir in model_dirs:\n",
    "        all_metric_files.extend(list(model_dir.glob(\"*.json\")))\n",
    "    \n",
    "    performance_data = []\n",
    "    \n",
    "    print(\"üèÜ MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for metrics_file in all_metric_files:\n",
    "        if '_metrics_' in metrics_file.name:\n",
    "            try:\n",
    "                with open(metrics_file, 'r') as f:\n",
    "                    metrics = json.load(f)\n",
    "                \n",
    "                # Extract classifier type from directory name or filename\n",
    "                if metrics_file.parent != models_dir:\n",
    "                    # File is in a subdirectory\n",
    "                    classifier_type = metrics_file.parent.name.split('_')[-1]\n",
    "                else:\n",
    "                    # File is in root models directory\n",
    "                    classifier_type = metrics_file.name.split('_metrics_')[0]\n",
    "                \n",
    "                # Get test metrics\n",
    "                test_metrics = metrics.get('test_metrics', {})\n",
    "                \n",
    "                performance = {\n",
    "                    'Classifier': classifier_type.replace('_', ' ').title(),\n",
    "                    'Accuracy': test_metrics.get('accuracy', 'N/A'),\n",
    "                    'Precision': test_metrics.get('precision', 'N/A'),\n",
    "                    'Recall': test_metrics.get('recall', 'N/A'),\n",
    "                    'F1 Score': test_metrics.get('f1_score', 'N/A'),\n",
    "                    'ROC AUC': test_metrics.get('roc_auc', 'N/A')\n",
    "                }\n",
    "                \n",
    "                performance_data.append(performance)\n",
    "                \n",
    "                # Display individual model performance\n",
    "                print(f\"\\n{performance['Classifier'].upper()}:\")\n",
    "                for metric, value in performance.items():\n",
    "                    if metric != 'Classifier':\n",
    "                        if isinstance(value, float):\n",
    "                            print(f\"  {metric:12}: {value:.4f}\")\n",
    "                        else:\n",
    "                            print(f\"  {metric:12}: {value}\")\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error loading metrics for {metrics_file.name}: {e}\")\n",
    "    \n",
    "    # Create comparison table\n",
    "    if performance_data:\n",
    "        comparison_df = pd.DataFrame(performance_data)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìä COMPREHENSIVE PERFORMANCE COMPARISON\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Format numeric columns for better display\n",
    "        numeric_cols = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "        for col in numeric_cols:\n",
    "            if col in comparison_df.columns:\n",
    "                comparison_df[col] = comparison_df[col].apply(\n",
    "                    lambda x: f\"{x:.4f}\" if isinstance(x, float) else str(x)\n",
    "                )\n",
    "        \n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        # Find best performing model for each metric\n",
    "        print(\"\\nü•á BEST PERFORMERS:\")\n",
    "        for col in numeric_cols:\n",
    "            if col in comparison_df.columns:\n",
    "                # Convert back to float for comparison\n",
    "                numeric_values = pd.to_numeric(comparison_df[col], errors='coerce')\n",
    "                if not numeric_values.isna().all():\n",
    "                    best_idx = numeric_values.idxmax()\n",
    "                    best_model = comparison_df.loc[best_idx, 'Classifier']\n",
    "                    best_value = comparison_df.loc[best_idx, col]\n",
    "                    print(f\"  {col:12}: {best_model} ({best_value})\")\n",
    "    else:\n",
    "        print(\"No model metrics found in the expected format.\")\n",
    "                    \n",
    "else:\n",
    "    print(\"‚ùå No model performance data available.\")\n",
    "    print(\"The classification stage may not have completed successfully.\")\n",
    "    print(f\"Expected models directory: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e-data-insights",
   "metadata": {},
   "source": [
    "## Data and Processing Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2e-insights",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ PROCESSING INSIGHTS\n",
      "==================================================\n",
      "\n",
      "‚è±Ô∏è  Processing Time Analysis - end_to_end_pipeline.json:\n",
      "  Log level distribution: {'UNKNOWN': 1}\n",
      "\n",
      "‚è±Ô∏è  Processing Time Analysis - classification_pipeline.json:\n",
      "  Log level distribution: {}\n",
      "\n",
      "‚è±Ô∏è  Processing Time Analysis - embedding_pipeline.json:\n",
      "  Log level distribution: {'UNKNOWN': 7}\n",
      "\n",
      "‚úÖ Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Analyze embedding quality and processing statistics\n",
    "print(\"üî¨ PROCESSING INSIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Embedding statistics\n",
    "if embeddings_dir.exists():\n",
    "    embedding_files = list(embeddings_dir.glob(\"*.csv\"))\n",
    "    \n",
    "    for file in embedding_files:\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        print(f\"\\nüìä Embeddings Analysis - {file.name}:\")\n",
    "        print(f\"  Total samples: {len(df)}\")\n",
    "        \n",
    "        if 'label' in df.columns:\n",
    "            label_dist = df['label'].value_counts()\n",
    "            print(f\"  Label distribution: {label_dist.to_dict()}\")\n",
    "            print(f\"  Class balance: {label_dist.min() / label_dist.max():.3f}\")\n",
    "        \n",
    "        # Analyze embedding dimensions\n",
    "        embedding_cols = [col for col in df.columns if col.startswith('dim_') or col.isdigit()]\n",
    "        if not embedding_cols:\n",
    "            # Try to find embedding columns differently\n",
    "            non_meta_cols = [col for col in df.columns if col not in ['mcid', 'label', 'claim']]\n",
    "            if non_meta_cols:\n",
    "                embedding_cols = non_meta_cols\n",
    "        \n",
    "        if embedding_cols:\n",
    "            embeddings = df[embedding_cols].astype(float)\n",
    "            print(f\"  Embedding dimensions: {len(embedding_cols)}\")\n",
    "            print(f\"  Mean embedding norm: {np.linalg.norm(embeddings.values, axis=1).mean():.4f}\")\n",
    "            print(f\"  Embedding variance: {embeddings.var().mean():.6f}\")\n",
    "\n",
    "# Processing time analysis from logs\n",
    "if logs_dir.exists():\n",
    "    log_files = list(logs_dir.glob(\"*.json\"))\n",
    "    \n",
    "    for log_file in log_files:\n",
    "        try:\n",
    "            with open(log_file, 'r') as f:\n",
    "                log_data = [json.loads(line) for line in f if line.strip()]\n",
    "            \n",
    "            print(f\"\\n‚è±Ô∏è  Processing Time Analysis - {log_file.name}:\")\n",
    "            \n",
    "            # Look for timing information in logs\n",
    "            start_times = [log for log in log_data if 'started' in log.get('message', '').lower()]\n",
    "            end_times = [log for log in log_data if 'completed' in log.get('message', '').lower()]\n",
    "            \n",
    "            if start_times and end_times:\n",
    "                print(f\"  Pipeline stages logged: {len(start_times)} starts, {len(end_times)} completions\")\n",
    "            \n",
    "            # Count different log levels\n",
    "            log_levels = [log.get('levelname', 'UNKNOWN') for log in log_data]\n",
    "            level_counts = pd.Series(log_levels).value_counts()\n",
    "            print(f\"  Log level distribution: {level_counts.to_dict()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error analyzing {log_file.name}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e-next-steps",
   "metadata": {},
   "source": [
    "## Next Steps and Recommendations\n",
    "\n",
    "After running the complete end-to-end pipeline:\n",
    "\n",
    "### üéØ **Model Selection**\n",
    "- Review the performance comparison above\n",
    "- Choose the best model based on your primary metric (accuracy, precision, recall, F1, or AUC)\n",
    "- Consider the trade-offs between different metrics for your use case\n",
    "\n",
    "### üîß **Performance Optimization**\n",
    "1. **Hyperparameter Tuning**: Adjust the search grids in the config for better performance\n",
    "2. **Feature Engineering**: Consider additional preprocessing or feature extraction\n",
    "3. **Data Augmentation**: Add more training data if available\n",
    "4. **Ensemble Methods**: Combine multiple models for improved performance\n",
    "\n",
    "### üìä **Data Analysis**\n",
    "- Analyze misclassified examples to understand model limitations\n",
    "- Check for class imbalance and consider resampling techniques\n",
    "- Examine embedding quality and clustering patterns\n",
    "\n",
    "### üöÄ **Production Deployment**\n",
    "- Save the best performing model for production use\n",
    "- Set up model monitoring and performance tracking\n",
    "- Implement A/B testing for model updates\n",
    "\n",
    "### üìà **Scaling Considerations**\n",
    "- For larger datasets, consider increasing API batch sizes\n",
    "- Implement parallel processing for embedding generation\n",
    "- Use distributed training for very large datasets\n",
    "\n",
    "## Configuration Customization\n",
    "\n",
    "To adapt this pipeline for your specific needs:\n",
    "\n",
    "1. **Data Source**: Update `dataset_path` to point to your medical claims data\n",
    "2. **API Configuration**: Adjust `model_api` settings for your deployment\n",
    "3. **Classifier Selection**: Modify the `models` list to include/exclude classifiers\n",
    "4. **Hyperparameters**: Customize search grids for each classifier\n",
    "5. **Evaluation Metrics**: Add/remove metrics based on your requirements\n",
    "6. **Output Structure**: Adjust output directories and file formats\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "Common issues and solutions:\n",
    "\n",
    "- **API Connection Issues**: Ensure MediClaimGPT server is running and accessible\n",
    "- **Memory Errors**: Reduce batch sizes or process data in smaller chunks\n",
    "- **Performance Issues**: Adjust hyperparameter grids or try different classifiers\n",
    "- **Data Format Errors**: Verify CSV format and required columns (mcid, claim, label)\n",
    "\n",
    "## Files Generated\n",
    "\n",
    "This complete pipeline generates:\n",
    "- **Embeddings**: Train/test embedding files (.csv)\n",
    "- **Models**: Trained classifier models (.pkl files)\n",
    "- **Metrics**: Performance metrics and evaluation results (JSON/CSV)\n",
    "- **Logs**: Detailed execution logs for debugging\n",
    "- **Reports**: Comprehensive performance comparison reports"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgpt-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
