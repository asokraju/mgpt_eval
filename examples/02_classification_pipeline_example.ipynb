{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "classification-title",
   "metadata": {},
   "source": "# Classification Pipeline Example\n\nThis notebook demonstrates the **pure config-driven binary classification pipeline**.\n\n## Key Features\n- âœ… **Pure config-driven**: All parameters defined in YAML with variable references\n- âœ… **No parameter duplication**: Variables defined once and referenced throughout\n- âœ… **Template resolution**: Uses `${variable}` syntax for dynamic paths\n- âœ… **Binary classification**: Supports logistic regression, SVM, and random forest\n- âœ… **Hyperparameter tuning**: Grid search with cross-validation\n- âœ… **Auto class balancing**: Detects and handles imbalanced datasets\n- âœ… **Comprehensive metrics**: Accuracy, precision, recall, F1-score, ROC-AUC\n\n## Prerequisites\nThis pipeline requires embeddings generated by the embedding pipeline.\nMake sure you have:\n- `train_embeddings.csv` \n- `test_embeddings.csv`\n\nIf you don't have these, run the embedding pipeline first (see `01_embedding_pipeline_example.ipynb`).",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "setup-imports",
   "metadata": {},
   "source": "## Step 1: Import Dependencies",
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('..')\n\nfrom models.config_models import PipelineConfig\nfrom pipelines.classification_pipeline import ClassificationPipeline\nimport json\nfrom pathlib import Path"
  },
  {
   "cell_type": "markdown",
   "id": "config-section",
   "metadata": {},
   "source": "## Step 2: Load Configuration\n\nThe configuration uses template variables for complete config-driven approach:\n- `${job.name}` â†’ `binary_classification_example`\n- `${job.output_dir}` â†’ `examples/outputs`\n- `${output.embeddings_dir}` â†’ `examples/outputs/embeddings`\n- `${output.logs_dir}` â†’ `examples/outputs/logs`",
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "load-config",
   "metadata": {},
   "outputs": [],
   "source": "# Load the pure config-driven configuration\nconfig = PipelineConfig.from_yaml('configs/classification_example_config.yaml')\n\nprint(f\"Job name: {config.job.name}\")\nprint(f\"Output directory: {config.job.output_dir}\")\nprint(f\"Available classifiers: {config.classification.models}\")\nprint(f\"Cross-validation folds: {config.classification.cross_validation.n_folds}\")\nprint(f\"Scoring metric: {config.classification.cross_validation.scoring}\")\n\n# Show template resolution\nprint(\"\\n=== Template Resolution ====\")\nprint(f\"Input train path: {config.input.train_embeddings_path}\")\nprint(f\"Resolved train path: {config.resolve_template_string(config.input.train_embeddings_path)}\")\nprint(f\"Log file template: {config.logging.file}\")\nprint(f\"Resolved log file: {config.resolve_template_string(config.logging.file)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "data-check",
   "metadata": {},
   "source": "## Step 3: Initialize Pipeline\n\nThe pipeline is completely config-driven - no parameters needed!",
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "check-embeddings",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize the classification pipeline\npipeline = ClassificationPipeline(config)\nprint(\"Classification pipeline initialized successfully!\")\nprint(f\"Random seed: {config.job.random_seed}\")\nprint(f\"Available classifiers: {list(pipeline.classifiers.keys())}\")"
  },
  {
   "cell_type": "markdown",
   "id": "fallback-data",
   "metadata": {},
   "source": "## Step 4: Check Input Data\n\nVerify that the embedding files exist and have the correct format.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "fallback-check",
   "metadata": {},
   "outputs": [],
   "source": "# Check if embedding files exist\ntrain_path = config.resolve_template_string(config.input.train_embeddings_path)\ntest_path = config.resolve_template_string(config.input.test_embeddings_path)\n\nprint(f\"Train embeddings: {train_path}\")\nprint(f\"  Exists: {Path(train_path).exists()}\")\nif Path(train_path).exists():\n    import pandas as pd\n    train_df = pd.read_csv(train_path)\n    print(f\"  Rows: {len(train_df)}\")\n    print(f\"  Columns: {list(train_df.columns)}\")\n    print(f\"  Label distribution: {train_df['label'].value_counts().to_dict()}\")\n\nprint(f\"\\nTest embeddings: {test_path}\")\nprint(f\"  Exists: {Path(test_path).exists()}\")\nif Path(test_path).exists():\n    test_df = pd.read_csv(test_path)\n    print(f\"  Rows: {len(test_df)}\")\n    print(f\"  Columns: {list(test_df.columns)}\")\n    print(f\"  Label distribution: {test_df['label'].value_counts().to_dict()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-setup",
   "metadata": {},
   "source": "## Step 5: Run Classification Pipeline\n\n### 5.1 Single Classifier Example",
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "init-pipeline",
   "metadata": {},
   "outputs": [],
   "source": "# Run with logistic regression (first classifier in config)\nprint(\"=== Running Logistic Regression ===\")\nresult_lr = pipeline.run(classifier_type='logistic_regression')\n\nprint(f\"\\nResults:\")\nprint(f\"Classifier: {result_lr['classifier_type']}\")\nprint(f\"Best CV Score: {result_lr['best_cv_score']:.4f}\")\nprint(f\"Best Parameters: {result_lr['best_params']}\")\nprint(f\"\\nTest Metrics:\")\nfor metric, value in result_lr['test_metrics'].items():\n    if metric != 'confusion_matrix' and value is not None:\n        print(f\"  {metric}: {value:.4f}\")\nprint(f\"\\nModel saved to: {result_lr['model_path']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "run-pipeline",
   "metadata": {},
   "source": "### 5.2 All Classifiers Comparison",
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "execute-pipeline",
   "metadata": {},
   "outputs": [],
   "source": "# Run all classifiers from config and compare results\nprint(\"=== Running All Classifiers ===\")\nresults = {}\n\nfor classifier_type in config.classification.models:\n    print(f\"\\n--- {classifier_type.upper()} ---\")\n    result = pipeline.run(classifier_type=classifier_type)\n    results[classifier_type] = result\n    \n    print(f\"Best CV Score: {result['best_cv_score']:.4f}\")\n    print(f\"Test Accuracy: {result['test_metrics']['accuracy']:.4f}\")\n    print(f\"Test F1-Score: {result['test_metrics']['f1_score']:.4f}\")\n    if result['test_metrics']['roc_auc'] is not None:\n        print(f\"Test ROC-AUC: {result['test_metrics']['roc_auc']:.4f}\")\n    print(f\"Model path: {result['model_path']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "results-analysis",
   "metadata": {},
   "source": "### 5.3 Results Summary",
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "analyze-results",
   "metadata": {},
   "outputs": [],
   "source": "# Create a comparison summary\nprint(\"=== CLASSIFICATION RESULTS SUMMARY ===\")\nprint(f\"{'Classifier':<20} {'CV Score':<10} {'Accuracy':<10} {'F1-Score':<10} {'ROC-AUC':<10}\")\nprint(\"-\" * 70)\n\nfor classifier_type, result in results.items():\n    cv_score = result['best_cv_score']\n    accuracy = result['test_metrics']['accuracy']\n    f1_score = result['test_metrics']['f1_score']\n    roc_auc = result['test_metrics']['roc_auc']\n    roc_auc_str = f\"{roc_auc:.4f}\" if roc_auc is not None else \"N/A\"\n    \n    print(f\"{classifier_type:<20} {cv_score:<10.4f} {accuracy:<10.4f} {f1_score:<10.4f} {roc_auc_str:<10}\")\n\n# Find best performer\nbest_classifier = max(results.keys(), key=lambda k: results[k]['test_metrics']['accuracy'])\nprint(f\"\\nBest Performer: {best_classifier} (Accuracy: {results[best_classifier]['test_metrics']['accuracy']:.4f})\")"
  },
  {
   "cell_type": "markdown",
   "id": "load-metrics",
   "metadata": {},
   "source": "## Step 6: Inspect Saved Models",
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "display-metrics",
   "metadata": {},
   "outputs": [],
   "source": "# Check what models were saved\nimport pickle\n\nmodels_base_dir = config.resolve_template_string(\"${job.output_dir}/models\")\nmodels_path = Path(models_base_dir)\n\nprint(f\"Models directory: {models_path}\")\nprint(f\"Models saved:\")\n\nif models_path.exists():\n    for model_dir in models_path.iterdir():\n        if model_dir.is_dir():\n            print(f\"\\n  {model_dir.name}/\")\n            for file in model_dir.iterdir():\n                print(f\"    {file.name}\")\n                \n                # Load and inspect one model\n                if file.suffix == '.pkl' and 'logistic_regression' in file.name:\n                    with open(file, 'rb') as f:\n                        model_data = pickle.load(f)\n                    print(f\"      Pipeline version: {model_data.get('pipeline_version', 'N/A')}\")\n                    print(f\"      Job name: {model_data.get('job_name', 'N/A')}\")\n                    print(f\"      Timestamp: {model_data.get('timestamp', 'N/A')}\")"
  },
  {
   "cell_type": "code",
   "source": "# Verify that all paths are properly resolved from config\nprint(\"=== Configuration Validation ===\")\nprint(f\"Job name: {config.job.name}\")\nprint(f\"Output directory: {config.job.output_dir}\")\nprint(f\"Random seed: {config.job.random_seed}\")\n\nprint(\"\\nPath Resolution:\")\nprint(f\"Train embeddings: {config.input.train_embeddings_path}\")\nprint(f\"  â†’ {config.resolve_template_string(config.input.train_embeddings_path)}\")\nprint(f\"Test embeddings: {config.input.test_embeddings_path}\")\nprint(f\"  â†’ {config.resolve_template_string(config.input.test_embeddings_path)}\")\nprint(f\"Log file: {config.logging.file}\")\nprint(f\"  â†’ {config.resolve_template_string(config.logging.file)}\")\n\nprint(\"\\nHyperparameter Grids:\")\nfor classifier in config.classification.models:\n    params = getattr(config.classification.hyperparameter_search, classifier)\n    print(f\"  {classifier}: {len(params)} parameters\")\n    for param, values in params.items():\n        print(f\"    {param}: {values}\")\n\nprint(f\"\\nCross-validation: {config.classification.cross_validation.n_folds} folds\")\nprint(f\"Scoring metric: {config.classification.cross_validation.scoring}\")\nprint(f\"Parallel jobs: {config.classification.cross_validation.n_jobs}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrated the **pure config-driven classification pipeline** with:\n\n### âœ… **Key Achievements**\n- **No parameter duplication**: All parameters defined once in config\n- **Template variables**: Dynamic path resolution with `${variable}` syntax\n- **Sequential config usage**: Load config â†’ Initialize pipeline â†’ Run\n- **Multiple classifiers**: Logistic regression, SVM, random forest\n- **Hyperparameter tuning**: Grid search with cross-validation\n- **Comprehensive evaluation**: Multiple metrics with model saving\n\n### ðŸ”§ **Configuration Features**\n- Variable references: `${job.name}`, `${job.output_dir}`, `${output.logs_dir}`\n- Binary classification focus (labels: 0, 1)\n- Automatic class imbalance detection\n- Configurable hyperparameter grids\n- Cross-validation settings\n- Logging configuration\n\n### ðŸ“Š **Output Structure**\n```\nexamples/outputs/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ binary_classification_example_logistic_regression/\nâ”‚   â”œâ”€â”€ binary_classification_example_svm/\nâ”‚   â””â”€â”€ binary_classification_example_random_forest/\nâ””â”€â”€ logs/\n    â””â”€â”€ classification_pipeline.log\n```\n\nThe pipeline is now fully config-driven with no hardcoded parameters!",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgpt-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}