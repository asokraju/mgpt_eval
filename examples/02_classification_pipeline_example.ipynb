{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "classification-title",
   "metadata": {},
   "source": [
    "# Classification Pipeline Example\n",
    "\n",
    "This notebook demonstrates how to train binary classifiers using pre-generated embeddings from the MediClaimGPT embedding pipeline.\n",
    "\n",
    "## Overview\n",
    "- **Input**: Pre-generated embedding files from embedding pipeline\n",
    "- **Process**: Train multiple classifiers (Logistic Regression, SVM, Random Forest)\n",
    "- **Output**: Trained models, performance metrics, and comparison reports\n",
    "\n",
    "## Prerequisites\n",
    "1. Run the embedding pipeline first (see `01_embedding_pipeline_example.ipynb`)\n",
    "2. Ensure embedding files exist in `examples/outputs/embeddings/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-imports",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/kosaraju/mgpt_eval\n",
      "Working directory: /home/kosaraju/mgpt_eval/examples\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from pipelines.classification_pipeline import ClassificationPipeline\n",
    "from models.config_models import PipelineConfig\n",
    "from utils.logging_utils import get_logger\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-section",
   "metadata": {},
   "source": [
    "## Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "Job name: classification_training_example\n",
      "Output directory: /home/kosaraju/mgpt_eval/examples/outputs\n",
      "Pipeline stages: embeddings=False classification=True evaluation=True target_word_eval=False summary_report=True method_comparison=True\n",
      "Classifiers: ['logistic_regression', 'svm', 'random_forest']\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config_path = \"configs/classification_example_config.yaml\"\n",
    "config = PipelineConfig.from_yaml(config_path)\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Job name: {config.job.name}\")\n",
    "print(f\"Output directory: {config.job.output_dir}\")\n",
    "print(f\"Pipeline stages: {config.pipeline_stages}\")\n",
    "print(f\"Classifiers: {config.classification.models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-check",
   "metadata": {},
   "source": [
    "## Input Data Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "check-embeddings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for embedding files...\n",
      "Train embeddings: /home/kosaraju/mgpt_eval/examples/outputs/embeddings/train_embeddings.csv\n",
      "  - Exists: True\n",
      "  - Shape: (16, 3)\n",
      "  - Columns: ['mcid', 'label', 'embedding']\n",
      "  - Sample data:\n",
      "   mcid  label                                          embedding\n",
      "0    20      0  [0.02810053527355194, -0.2548440098762512, -1....\n",
      "1    16      0  [-0.134785994887352, -0.01184933539479971, -0....\n",
      "\n",
      "Test embeddings: /home/kosaraju/mgpt_eval/examples/outputs/embeddings/test_embeddings.csv\n",
      "  - Exists: True\n",
      "  - Shape: (4, 3)\n",
      "  - Columns: ['mcid', 'label', 'embedding']\n"
     ]
    }
   ],
   "source": [
    "# Check if embedding files exist\n",
    "train_embeddings_path = Path(config.input.train_embeddings_path)\n",
    "test_embeddings_path = Path(config.input.test_embeddings_path)\n",
    "\n",
    "print(\"Checking for embedding files...\")\n",
    "print(f\"Train embeddings: {train_embeddings_path}\")\n",
    "print(f\"  - Exists: {train_embeddings_path.exists()}\")\n",
    "if train_embeddings_path.exists():\n",
    "    train_df = pd.read_csv(train_embeddings_path)\n",
    "    print(f\"  - Shape: {train_df.shape}\")\n",
    "    print(f\"  - Columns: {list(train_df.columns)}\")\n",
    "    print(f\"  - Sample data:\")\n",
    "    print(train_df.head(2))\n",
    "\n",
    "print(f\"\\nTest embeddings: {test_embeddings_path}\")\n",
    "print(f\"  - Exists: {test_embeddings_path.exists()}\")\n",
    "if test_embeddings_path.exists():\n",
    "    test_df = pd.read_csv(test_embeddings_path)\n",
    "    print(f\"  - Shape: {test_df.shape}\")\n",
    "    print(f\"  - Columns: {list(test_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallback-data",
   "metadata": {},
   "source": [
    "### Fallback: Use Available Data if Embeddings Don't Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fallback-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available embedding files in outputs/embeddings:\n",
      "  - sample_embeddings.csv\n",
      "  - train_embeddings.csv\n",
      "  - test_embeddings.csv\n",
      "\n",
      "Using sample_embeddings.csv for this example...\n",
      "Updated config to use available data.\n"
     ]
    }
   ],
   "source": [
    "# If embeddings don't exist, check for available embedding files\n",
    "embeddings_dir = Path(\"outputs/embeddings\")\n",
    "if embeddings_dir.exists():\n",
    "    embedding_files = list(embeddings_dir.glob(\"*.csv\"))\n",
    "    print(f\"Available embedding files in {embeddings_dir}:\")\n",
    "    for file in embedding_files:\n",
    "        print(f\"  - {file.name}\")\n",
    "        \n",
    "    if embedding_files:\n",
    "        # Use first available embedding file for demonstration\n",
    "        sample_file = embedding_files[0]\n",
    "        print(f\"\\nUsing {sample_file.name} for this example...\")\n",
    "        \n",
    "        # Update config to use available data\n",
    "        config.input.train_embeddings_path = str(sample_file)\n",
    "        config.input.test_embeddings_path = str(sample_file)  # Same file for demo\n",
    "        \n",
    "        print(\"Updated config to use available data.\")\n",
    "else:\n",
    "    print(\"No embedding files found. Please run the embedding pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-setup",
   "metadata": {},
   "source": [
    "## Initialize Classification Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "init-pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification pipeline initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logger = get_logger(\"classification_pipeline\", config.logging)\n",
    "\n",
    "# Initialize classification pipeline\n",
    "pipeline = ClassificationPipeline(config)\n",
    "\n",
    "\n",
    "print(\"Classification pipeline initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-pipeline",
   "metadata": {},
   "source": [
    "## Run Classification Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "execute-pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Limited samples (20) for 48 parameter combinations. Consider reducing parameter grid complexity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training logistic_regression classifier...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "Limited samples (20) for 144 parameter combinations. Consider reducing parameter grid complexity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ logistic_regression completed successfully!\n",
      "   Test accuracy: N/A\n",
      "\n",
      "üöÄ Training svm classifier...\n",
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Limited samples (20) for 432 parameter combinations. Consider reducing parameter grid complexity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ svm completed successfully!\n",
      "   Test accuracy: N/A\n",
      "\n",
      "üöÄ Training random_forest classifier...\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "‚ùå Pipeline execution failed: Grid search failed: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'None' instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "           ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/joblib/parallel.py\", line 598, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "            ~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'None' instead.\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kosaraju/mgpt_eval/pipelines/classification_pipeline.py\", line 317, in _train_classifier\n",
      "    grid_search.fit(X_train, y_train)\n",
      "    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/model_selection/_search.py\", line 1024, in fit\n",
      "    self._run_search(evaluate_candidates)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/model_selection/_search.py\", line 1571, in _run_search\n",
      "    evaluate_candidates(ParameterGrid(self.param_grid))\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/model_selection/_search.py\", line 970, in evaluate_candidates\n",
      "    out = parallel(\n",
      "        delayed(_fit_and_score)(\n",
      "    ...<13 lines>...\n",
      "        )\n",
      "    )\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/joblib/parallel.py\", line 2007, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ~~~~^^^^^^^^\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/joblib/parallel.py\", line 1650, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/joblib/parallel.py\", line 1754, in _retrieve\n",
      "    self._raise_error_fast()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/joblib/parallel.py\", line 1789, in _raise_error_fast\n",
      "    error_job.get_result(self.timeout)\n",
      "    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/joblib/parallel.py\", line 745, in get_result\n",
      "    return self._return_or_raise()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/kosaraju/miniconda3/envs/mgpt-eval/lib/python3.13/site-packages/joblib/parallel.py\", line 763, in _return_or_raise\n",
      "    raise self._result\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'None' instead.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_124832/3964010670.py\", line 9, in <module>\n",
      "    results = pipeline.run(\n",
      "        train_embeddings=config.input.train_embeddings_path,\n",
      "    ...<2 lines>...\n",
      "        output_dir=config.job.output_dir\n",
      "    )\n",
      "  File \"/home/kosaraju/mgpt_eval/pipelines/classification_pipeline.py\", line 158, in run\n",
      "    best_model, best_params, best_score = self._train_classifier(\n",
      "                                          ~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        X_train_scaled, y_train, classifier_type, use_balanced\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/kosaraju/mgpt_eval/pipelines/classification_pipeline.py\", line 321, in _train_classifier\n",
      "    raise RuntimeError(f\"Grid search failed: {e}\")\n",
      "RuntimeError: Grid search failed: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'None' instead.\n"
     ]
    }
   ],
   "source": [
    "# Run the classification pipeline for each classifier\n",
    "results_summary = {}\n",
    "\n",
    "try:\n",
    "    for classifier_type in config.classification.models:\n",
    "        print(f\"\\nüöÄ Training {classifier_type} classifier...\")\n",
    "        \n",
    "        # Run pipeline for this classifier\n",
    "        results = pipeline.run(\n",
    "            train_embeddings=config.input.train_embeddings_path,\n",
    "            test_embeddings=config.input.test_embeddings_path,\n",
    "            classifier_type=classifier_type,\n",
    "            output_dir=config.job.output_dir\n",
    "        )\n",
    "        \n",
    "        results_summary[classifier_type] = results\n",
    "        print(f\"‚úÖ {classifier_type} completed successfully!\")\n",
    "        print(f\"   Test accuracy: {results.get('test_accuracy', 'N/A'):.4f}\" if isinstance(results.get('test_accuracy'), float) else f\"   Test accuracy: {results.get('test_accuracy', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\nüéâ All classifiers completed successfully!\")\n",
    "    print(f\"Trained {len(results_summary)} classifiers: {list(results_summary.keys())}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Pipeline execution failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-analysis",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "analyze-results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /home/kosaraju/mgpt_eval/examples/outputs\n",
      "Models directory: /home/kosaraju/mgpt_eval/examples/outputs/models\n",
      "Metrics directory: /home/kosaraju/mgpt_eval/examples/outputs/metrics\n",
      "\n",
      "Generated model files: 3\n",
      "  - svm_model_20250604_002255.pkl\n",
      "  - logistic_regression_model_20250604_002255.pkl\n",
      "  - logistic_regression_model_20250604_002100.pkl\n",
      "\n",
      "Generated metrics files: 3\n",
      "  - logistic_regression_metrics_20250604_002255.json\n",
      "  - svm_metrics_20250604_002255.json\n",
      "  - logistic_regression_metrics_20250604_002100.json\n",
      "\n",
      "Models directory does not exist: /home/kosaraju/mgpt_eval/examples/outputs/models\n"
     ]
    }
   ],
   "source": [
    "# Check output directory for results\n",
    "output_dir = Path(config.job.output_dir)\n",
    "models_dir = output_dir / \"models\"\n",
    "metrics_dir = output_dir / \"metrics\"\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Models directory: {models_dir}\")\n",
    "print(f\"Metrics directory: {metrics_dir}\")\n",
    "\n",
    "# List generated models (they may be in the root output directory)\n",
    "model_files = list(output_dir.glob(\"*_model_*.pkl\"))\n",
    "metric_files = list(output_dir.glob(\"*_metrics_*.json\"))\n",
    "\n",
    "print(f\"\\nGenerated model files: {len(model_files)}\")\n",
    "for model_file in model_files:\n",
    "    print(f\"  - {model_file.name}\")\n",
    "\n",
    "print(f\"\\nGenerated metrics files: {len(metric_files)}\")\n",
    "for metric_file in metric_files:\n",
    "    print(f\"  - {metric_file.name}\")\n",
    "\n",
    "# Also check if there are any files in the models subdirectory\n",
    "if models_dir.exists():\n",
    "    subdir_models = list(models_dir.glob(\"**/*.pkl\"))\n",
    "    subdir_metrics = list(models_dir.glob(\"**/*.json\"))\n",
    "    if subdir_models or subdir_metrics:\n",
    "        print(f\"\\nFiles in models subdirectory:\")\n",
    "        print(f\"  Models: {len(subdir_models)}, Metrics: {len(subdir_metrics)}\")\n",
    "        for file in subdir_models + subdir_metrics:\n",
    "            print(f\"    - {file.name}\")\n",
    "else:\n",
    "    print(f\"\\nModels directory does not exist: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-metrics",
   "metadata": {},
   "source": [
    "## Load and Display Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "display-metrics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ MODEL PERFORMANCE SUMMARY\n",
      "==================================================\n",
      "\n",
      "LOGISTIC REGRESSION Performance:\n",
      "  Accuracy    : 1.0000\n",
      "  Precision   : 1.0000\n",
      "  Recall      : 1.0000\n",
      "  F1 Score    : 1.0000\n",
      "  Roc Auc     : 1.0000\n",
      "\n",
      "SVM Performance:\n",
      "  Accuracy    : 1.0000\n",
      "  Precision   : 1.0000\n",
      "  Recall      : 1.0000\n",
      "  F1 Score    : 1.0000\n",
      "  Roc Auc     : 0.0000\n",
      "\n",
      "LOGISTIC REGRESSION Performance:\n",
      "  Accuracy    : 1.0000\n",
      "  Precision   : 1.0000\n",
      "  Recall      : 1.0000\n",
      "  F1 Score    : 1.0000\n",
      "  Roc Auc     : 1.0000\n",
      "\n",
      "============================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "============================================================\n",
      "              model accuracy precision recall f1_score roc_auc\n",
      "Logistic Regression   1.0000    1.0000 1.0000   1.0000  1.0000\n",
      "                Svm   1.0000    1.0000 1.0000   1.0000  0.0000\n",
      "Logistic Regression   1.0000    1.0000 1.0000   1.0000  1.0000\n",
      "\n",
      "ü•á BEST PERFORMERS:\n",
      "  Accuracy    : Logistic Regression (1.0000)\n",
      "  Precision   : Logistic Regression (1.0000)\n",
      "  Recall      : Logistic Regression (1.0000)\n",
      "  F1 Score    : Logistic Regression (1.0000)\n",
      "  Roc Auc     : Logistic Regression (1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Load and display model metrics\n",
    "output_dir = Path(config.job.output_dir)\n",
    "\n",
    "# Get all metric files from the output directory\n",
    "metric_files = list(output_dir.glob(\"*_metrics_*.json\"))\n",
    "\n",
    "if metric_files:\n",
    "    performance_summary = []\n",
    "    \n",
    "    print(\"üèÜ MODEL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for metrics_file in metric_files:\n",
    "        try:\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "            \n",
    "            # Extract classifier type from filename\n",
    "            # Format: classifier_type_metrics_timestamp.json\n",
    "            model_name = metrics_file.name.split('_metrics_')[0]\n",
    "            \n",
    "            # Get test metrics\n",
    "            test_metrics = metrics.get('test_metrics', {})\n",
    "            \n",
    "            perf = {\n",
    "                'model': model_name.replace('_', ' ').title(),\n",
    "                'accuracy': test_metrics.get('accuracy', 'N/A'),\n",
    "                'precision': test_metrics.get('precision', 'N/A'),\n",
    "                'recall': test_metrics.get('recall', 'N/A'),\n",
    "                'f1_score': test_metrics.get('f1_score', 'N/A'),\n",
    "                'roc_auc': test_metrics.get('roc_auc', 'N/A')\n",
    "            }\n",
    "            \n",
    "            performance_summary.append(perf)\n",
    "            \n",
    "            print(f\"\\n{perf['model'].upper()} Performance:\")\n",
    "            for metric, value in perf.items():\n",
    "                if metric != 'model':\n",
    "                    if isinstance(value, float):\n",
    "                        print(f\"  {metric.replace('_', ' ').title():12}: {value:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"  {metric.replace('_', ' ').title():12}: {value}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading metrics for {metrics_file.name}: {e}\")\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    if performance_summary:\n",
    "        comparison_df = pd.DataFrame(performance_summary)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MODEL COMPARISON SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Format numeric columns for better display\n",
    "        numeric_cols = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
    "        for col in numeric_cols:\n",
    "            if col in comparison_df.columns:\n",
    "                comparison_df[col] = comparison_df[col].apply(\n",
    "                    lambda x: f\"{x:.4f}\" if isinstance(x, float) else str(x)\n",
    "                )\n",
    "        \n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        # Find best performing model for each metric\n",
    "        print(\"\\nü•á BEST PERFORMERS:\")\n",
    "        for col in numeric_cols:\n",
    "            if col in comparison_df.columns:\n",
    "                # Convert back to float for comparison\n",
    "                numeric_values = pd.to_numeric(comparison_df[col], errors='coerce')\n",
    "                if not numeric_values.isna().all():\n",
    "                    best_idx = numeric_values.idxmax()\n",
    "                    best_model = comparison_df.loc[best_idx, 'model']\n",
    "                    best_value = comparison_df.loc[best_idx, col]\n",
    "                    print(f\"  {col.replace('_', ' ').title():12}: {best_model} ({best_value})\")\n",
    "else:\n",
    "    print(\"No model metrics found. The classification pipeline may not have completed successfully.\")\n",
    "    print(f\"Checked directory: {output_dir}\")\n",
    "    print(\"Available files:\", list(output_dir.glob(\"*.json\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After running this classification pipeline:\n",
    "\n",
    "1. **Model Evaluation**: Review the performance metrics above to compare classifiers\n",
    "2. **Best Model Selection**: Choose the model with the best performance for your use case\n",
    "3. **Hyperparameter Tuning**: Adjust the hyperparameter grids in the config for better performance\n",
    "4. **End-to-End Pipeline**: Run the complete pipeline (embeddings + classification) using the end-to-end example\n",
    "\n",
    "## Configuration Customization\n",
    "\n",
    "To customize this example for your data:\n",
    "\n",
    "1. **Input Paths**: Update `train_embeddings_path` and `test_embeddings_path` in the config\n",
    "2. **Classifiers**: Modify the `models` list to include/exclude classifiers\n",
    "3. **Hyperparameters**: Adjust the search grids for each classifier\n",
    "4. **Evaluation Metrics**: Add/remove metrics in the evaluation configuration\n",
    "5. **Cross-Validation**: Change the number of folds or scoring metric\n",
    "\n",
    "## Files Generated\n",
    "\n",
    "This pipeline generates:\n",
    "- **Models**: Trained classifier models (.pkl files)\n",
    "- **Metrics**: Performance metrics (JSON files)\n",
    "- **Logs**: Detailed execution logs\n",
    "- **Reports**: Comparison reports (if enabled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgpt-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
