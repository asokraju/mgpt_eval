{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤖 Classification Pipeline Debug Testing\n",
    "\n",
    "## Purpose\n",
    "This notebook tests the classification pipeline step-by-step using the actual pipeline code to debug classifier training, hyperparameter search, cross-validation, and model evaluation issues.\n",
    "\n",
    "## What This Tests\n",
    "- Classification pipeline initialization and configuration\n",
    "- Embedding data loading and validation\n",
    "- Classifier training (Logistic Regression, SVM, Random Forest)\n",
    "- Hyperparameter search and cross-validation\n",
    "- Model serialization and loading\n",
    "- Performance metrics calculation\n",
    "- Memory usage and training time analysis\n",
    "- Error handling for edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# Add the mgpt_eval directory to Python path\n",
    "mgpt_eval_path = Path.cwd().parent if Path.cwd().name == 'examples' else Path.cwd()\n",
    "sys.path.insert(0, str(mgpt_eval_path))\n",
    "\n",
    "# Import actual pipeline modules\n",
    "from models.config_models import PipelineConfig\n",
    "from models.data_models import DataSample, DataBatch\n",
    "from pipelines.classification_pipeline import ClassificationPipeline\n",
    "from utils.logging_utils import setup_logging\n",
    "\n",
    "# Import ML libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(f\"✅ Working directory: {Path.cwd()}\")\n",
    "print(f\"✅ MGPT-eval path: {mgpt_eval_path}\")\n",
    "print(f\"✅ Imports successful\")\n",
    "\n",
    "# Setup logging for debugging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('classification_debug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Test Embeddings Data\n",
    "Create mock embeddings data to test the classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test embeddings data for classification\n",
    "def create_classification_test_data(num_samples=50, embedding_dim=768):\n",
    "    \"\"\"Create test embeddings data for classification pipeline testing.\"\"\"\n",
    "    \n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    \n",
    "    # Generate synthetic embeddings with some pattern for classification\n",
    "    # Class 0: embeddings centered around -0.5\n",
    "    # Class 1: embeddings centered around +0.5\n",
    "    \n",
    "    mcids = [f'CLASS_TEST_{i:03d}' for i in range(num_samples)]\n",
    "    labels = [i % 2 for i in range(num_samples)]  # Alternating 0, 1\n",
    "    \n",
    "    embeddings = []\n",
    "    for i, label in enumerate(labels):\n",
    "        # Create embeddings with different patterns for each class\n",
    "        if label == 0:\n",
    "            # Class 0: negative bias\n",
    "            embedding = np.random.normal(-0.1, 0.3, embedding_dim).tolist()\n",
    "        else:\n",
    "            # Class 1: positive bias\n",
    "            embedding = np.random.normal(0.1, 0.3, embedding_dim).tolist()\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    return {\n",
    "        'mcids': mcids,\n",
    "        'labels': labels,\n",
    "        'embeddings': embeddings\n",
    "    }\n",
    "\n",
    "# Create train and test embeddings\n",
    "train_data = create_classification_test_data(40, 768)  # 40 training samples\n",
    "test_data = create_classification_test_data(20, 768)   # 20 test samples\n",
    "\n",
    "# Modify test data MCIDs to avoid conflicts\n",
    "test_data['mcids'] = [f'TEST_{mcid}' for mcid in test_data['mcids']]\n",
    "\n",
    "print(f\"📊 Created training data: {len(train_data['mcids'])} samples\")\n",
    "print(f\"📊 Created test data: {len(test_data['mcids'])} samples\")\n",
    "print(f\"📊 Embedding dimension: {len(train_data['embeddings'][0])}\")\n",
    "print(f\"📊 Train label distribution: {pd.Series(train_data['labels']).value_counts().to_dict()}\")\n",
    "print(f\"📊 Test label distribution: {pd.Series(test_data['labels']).value_counts().to_dict()}\")\n",
    "\n",
    "# Verify embeddings have different patterns\n",
    "train_embeddings_array = np.array(train_data['embeddings'])\n",
    "class_0_mean = train_embeddings_array[np.array(train_data['labels']) == 0].mean()\n",
    "class_1_mean = train_embeddings_array[np.array(train_data['labels']) == 1].mean()\n",
    "\n",
    "print(f\"\\n🔍 Embedding pattern verification:\")\n",
    "print(f\"   Class 0 mean: {class_0_mean:.3f}\")\n",
    "print(f\"   Class 1 mean: {class_1_mean:.3f}\")\n",
    "print(f\"   Difference: {abs(class_1_mean - class_0_mean):.3f} (should be > 0.1 for good separation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to files for testing\n",
    "def save_embeddings_data(data: Dict, file_path: Path, format_type='json'):\n",
    "    \"\"\"Save embeddings data in specified format.\"\"\"\n",
    "    \n",
    "    if format_type == 'json':\n",
    "        # JSON format (pipeline default)\n",
    "        output_data = {\n",
    "            'job_info': {\n",
    "                'job_name': 'classification_test',\n",
    "                'timestamp': time.time(),\n",
    "                'total_samples': len(data['mcids'])\n",
    "            },\n",
    "            'data': {\n",
    "                'mcids': data['mcids'],\n",
    "                'labels': data['labels'],\n",
    "                'embeddings': data['embeddings']\n",
    "            },\n",
    "            'metadata': {\n",
    "                'embedding_dimension': len(data['embeddings'][0]),\n",
    "                'format': 'json',\n",
    "                'version': '1.0'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(output_data, f, indent=2)\n",
    "    \n",
    "    elif format_type == 'csv':\n",
    "        # CSV format (alternative format)\n",
    "        csv_data = []\n",
    "        for mcid, label, embedding in zip(data['mcids'], data['labels'], data['embeddings']):\n",
    "            row = {'mcid': mcid, 'label': label}\n",
    "            for i, emb_val in enumerate(embedding):\n",
    "                row[f'emb_{i}'] = emb_val\n",
    "            csv_data.append(row)\n",
    "        \n",
    "        pd.DataFrame(csv_data).to_csv(file_path, index=False)\n",
    "\n",
    "# Create test directory and save embeddings\n",
    "test_embeddings_dir = mgpt_eval_path / \"examples\" / \"test_classification_data\"\n",
    "test_embeddings_dir.mkdir(exist_ok=True)\n",
    "\n",
    "train_embeddings_path = test_embeddings_dir / \"train_embeddings.json\"\n",
    "test_embeddings_path = test_embeddings_dir / \"test_embeddings.json\"\n",
    "\n",
    "save_embeddings_data(train_data, train_embeddings_path, 'json')\n",
    "save_embeddings_data(test_data, test_embeddings_path, 'json')\n",
    "\n",
    "print(f\"💾 Saved train embeddings: {train_embeddings_path}\")\n",
    "print(f\"💾 Saved test embeddings: {test_embeddings_path}\")\n",
    "print(f\"📊 Train file size: {train_embeddings_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"📊 Test file size: {test_embeddings_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Classification Configuration\n",
    "Set up configuration for classification pipeline testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test configuration for classification pipeline\n",
    "def create_classification_test_config(train_emb_path: str, test_emb_path: str):\n",
    "    \"\"\"Create configuration optimized for classification pipeline testing.\"\"\"\n",
    "    \n",
    "    test_config = {\n",
    "        'input': {\n",
    "            'train_embeddings_path': str(train_emb_path),\n",
    "            'test_embeddings_path': str(test_emb_path)\n",
    "        },\n",
    "        'job': {\n",
    "            'name': 'classification_pipeline_test',\n",
    "            'output_dir': str(mgpt_eval_path / \"examples\" / \"test_classification_outputs\"),\n",
    "            'random_seed': 42\n",
    "        },\n",
    "        'model_api': {\n",
    "            'base_url': 'http://localhost:8000',  # Not used for classification\n",
    "            'batch_size': 32,\n",
    "            'timeout': 300,\n",
    "            'max_retries': 3\n",
    "        },\n",
    "        'pipeline_stages': {\n",
    "            'embeddings': False,  # Skip embedding generation\n",
    "            'classification': True,  # Focus on classification\n",
    "            'evaluation': True,  # Test evaluation too\n",
    "            'target_word_eval': False,\n",
    "            'summary_report': True,\n",
    "            'method_comparison': False\n",
    "        },\n",
    "        'data_processing': {\n",
    "            'random_seed': 42,\n",
    "            'max_sequence_length': 512,\n",
    "            'include_mcid': True,\n",
    "            'output_format': 'json',\n",
    "            'train_test_split': 0.8\n",
    "        },\n",
    "        'embedding_generation': {\n",
    "            'batch_size': 16,\n",
    "            'save_interval': 100,\n",
    "            'checkpoint_dir': 'outputs/checkpoints',\n",
    "            'resume_from_checkpoint': True,\n",
    "            'tokenizer_path': '/app/tokenizer'\n",
    "        },\n",
    "        'classification': {\n",
    "            'models': ['logistic_regression', 'svm', 'random_forest'],  # Test all classifiers\n",
    "            'cross_validation': {\n",
    "                'n_folds': 3,  # Smaller for faster testing\n",
    "                'scoring': 'roc_auc',\n",
    "                'n_jobs': 1  # Single threaded for easier debugging\n",
    "            },\n",
    "            'hyperparameter_search': {\n",
    "                'logistic_regression': {\n",
    "                    'C': [0.1, 1.0],  # Reduced search space for testing\n",
    "                    'penalty': ['l2'],\n",
    "                    'solver': ['liblinear']\n",
    "                },\n",
    "                'svm': {\n",
    "                    'C': [1.0],  # Single value for faster testing\n",
    "                    'kernel': ['rbf'],\n",
    "                    'gamma': ['scale']\n",
    "                },\n",
    "                'random_forest': {\n",
    "                    'n_estimators': [50],  # Fewer trees for faster testing\n",
    "                    'max_depth': [10],\n",
    "                    'min_samples_split': [2]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'evaluation': {\n",
    "            'metrics': ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'confusion_matrix'],\n",
    "            'visualization': {\n",
    "                'generate_plots': False,  # Skip plots for testing\n",
    "                'plot_formats': ['png'],\n",
    "                'dpi': 150\n",
    "            }\n",
    "        },\n",
    "        'target_word_evaluation': {\n",
    "            'enable': False,\n",
    "            'target_codes': ['E119']\n",
    "        },\n",
    "        'output': {\n",
    "            'embeddings_dir': 'embeddings',\n",
    "            'models_dir': 'models',\n",
    "            'metrics_dir': 'metrics',\n",
    "            'logs_dir': 'logs',\n",
    "            'save_best_model_only': False,  # Save all models for testing\n",
    "            'model_format': 'pickle'\n",
    "        },\n",
    "        'logging': {\n",
    "            'level': 'DEBUG',\n",
    "            'console_level': 'INFO',\n",
    "            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            'file': 'logs/classification_test.log'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return test_config\n",
    "\n",
    "# Create and validate test configuration\n",
    "test_config_dict = create_classification_test_config(train_embeddings_path, test_embeddings_path)\n",
    "\n",
    "try:\n",
    "    test_config = PipelineConfig(**test_config_dict)\n",
    "    print(f\"✅ Test configuration validated\")\n",
    "    \n",
    "    print(f\"\\n🔧 Configuration summary:\")\n",
    "    print(f\"   Job name: {test_config.job.name}\")\n",
    "    print(f\"   Train embeddings: {test_config.input.train_embeddings_path}\")\n",
    "    print(f\"   Test embeddings: {test_config.input.test_embeddings_path}\")\n",
    "    print(f\"   Classifiers: {test_config.classification.models}\")\n",
    "    print(f\"   CV folds: {test_config.classification.cross_validation.n_folds}\")\n",
    "    print(f\"   CV scoring: {test_config.classification.cross_validation.scoring}\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"❌ Configuration validation failed: {e}\")\n",
    "    print(f\"\\nError details:\")\n",
    "    traceback.print_exc()\n",
    "    test_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Embeddings Data Loading\n",
    "Test loading embeddings data using the classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embeddings data loading\n",
    "def test_embeddings_loading(config: PipelineConfig):\n",
    "    \"\"\"Test loading embeddings data for classification.\"\"\"\n",
    "    \n",
    "    print(f\"\\n📂 Testing embeddings data loading...\")\n",
    "    \n",
    "    try:\n",
    "        # Load train embeddings\n",
    "        print(f\"   📄 Loading train embeddings: {config.input.train_embeddings_path}\")\n",
    "        \n",
    "        with open(config.input.train_embeddings_path, 'r') as f:\n",
    "            train_data = json.load(f)\n",
    "        \n",
    "        # Extract data (handle both formats)\n",
    "        if 'data' in train_data:  # New format with metadata\n",
    "            train_mcids = train_data['data']['mcids']\n",
    "            train_labels = train_data['data']['labels']\n",
    "            train_embeddings = train_data['data']['embeddings']\n",
    "            train_metadata = train_data.get('metadata', {})\n",
    "        else:  # Direct format\n",
    "            train_mcids = train_data['mcids']\n",
    "            train_labels = train_data['labels']\n",
    "            train_embeddings = train_data['embeddings']\n",
    "            train_metadata = {}\n",
    "        \n",
    "        print(f\"      ✅ Train data loaded: {len(train_mcids)} samples\")\n",
    "        print(f\"      📊 Embedding dimension: {len(train_embeddings[0]) if train_embeddings else 0}\")\n",
    "        print(f\"      📊 Label distribution: {pd.Series(train_labels).value_counts().to_dict()}\")\n",
    "        \n",
    "        # Load test embeddings\n",
    "        print(f\"   📄 Loading test embeddings: {config.input.test_embeddings_path}\")\n",
    "        \n",
    "        with open(config.input.test_embeddings_path, 'r') as f:\n",
    "            test_data = json.load(f)\n",
    "        \n",
    "        # Extract test data\n",
    "        if 'data' in test_data:\n",
    "            test_mcids = test_data['data']['mcids']\n",
    "            test_labels = test_data['data']['labels']\n",
    "            test_embeddings = test_data['data']['embeddings']\n",
    "            test_metadata = test_data.get('metadata', {})\n",
    "        else:\n",
    "            test_mcids = test_data['mcids']\n",
    "            test_labels = test_data['labels']\n",
    "            test_embeddings = test_data['embeddings']\n",
    "            test_metadata = {}\n",
    "        \n",
    "        print(f\"      ✅ Test data loaded: {len(test_mcids)} samples\")\n",
    "        print(f\"      📊 Embedding dimension: {len(test_embeddings[0]) if test_embeddings else 0}\")\n",
    "        print(f\"      📊 Label distribution: {pd.Series(test_labels).value_counts().to_dict()}\")\n",
    "        \n",
    "        # Validation checks\n",
    "        print(f\"   🔍 Data validation:\")\n",
    "        \n",
    "        # Check dimension consistency\n",
    "        train_dim = len(train_embeddings[0]) if train_embeddings else 0\n",
    "        test_dim = len(test_embeddings[0]) if test_embeddings else 0\n",
    "        \n",
    "        if train_dim == test_dim and train_dim > 0:\n",
    "            print(f\"      ✅ Embedding dimensions match: {train_dim}\")\n",
    "        else:\n",
    "            print(f\"      ❌ Embedding dimension mismatch: train={train_dim}, test={test_dim}\")\n",
    "        \n",
    "        # Check label format\n",
    "        train_unique_labels = set(train_labels)\n",
    "        test_unique_labels = set(test_labels)\n",
    "        \n",
    "        if train_unique_labels.issubset({0, 1}) and test_unique_labels.issubset({0, 1}):\n",
    "            print(f\"      ✅ Valid binary labels\")\n",
    "        else:\n",
    "            print(f\"      ❌ Invalid labels found: train={train_unique_labels}, test={test_unique_labels}\")\n",
    "        \n",
    "        # Check for minimum samples per class\n",
    "        train_label_counts = pd.Series(train_labels).value_counts()\n",
    "        min_class_count = train_label_counts.min()\n",
    "        \n",
    "        if min_class_count >= config.classification.cross_validation.n_folds:\n",
    "            print(f\"      ✅ Sufficient samples for CV: min={min_class_count}, folds={config.classification.cross_validation.n_folds}\")\n",
    "        else:\n",
    "            print(f\"      ⚠️  Warning: May not have enough samples for CV: min={min_class_count}, folds={config.classification.cross_validation.n_folds}\")\n",
    "        \n",
    "        # Convert to numpy arrays for ML processing\n",
    "        X_train = np.array(train_embeddings)\n",
    "        y_train = np.array(train_labels)\n",
    "        X_test = np.array(test_embeddings)\n",
    "        y_test = np.array(test_labels)\n",
    "        \n",
    "        print(f\"      ✅ Data converted to numpy arrays\")\n",
    "        print(f\"      📊 Train shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "        print(f\"      📊 Test shape: X={X_test.shape}, y={y_test.shape}\")\n",
    "        \n",
    "        return {\n",
    "            'train': {\n",
    "                'mcids': train_mcids,\n",
    "                'X': X_train,\n",
    "                'y': y_train,\n",
    "                'metadata': train_metadata\n",
    "            },\n",
    "            'test': {\n",
    "                'mcids': test_mcids,\n",
    "                'X': X_test,\n",
    "                'y': y_test,\n",
    "                'metadata': test_metadata\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Data loading failed: {e}\")\n",
    "        print(f\"\\nError details:\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Test data loading\n",
    "if test_config:\n",
    "    loaded_data = test_embeddings_loading(test_config)\nelse:\n",
    "    print(f\"⚠️  Skipping data loading test due to config issues\")\n",
    "    loaded_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Individual Classifiers\n",
    "Test each classifier individually to debug training and hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test individual classifiers\n",
    "def test_individual_classifier(classifier_name: str, config: PipelineConfig, data: Dict):\n",
    "    \"\"\"Test a single classifier with hyperparameter search.\"\"\"\n",
    "    \n",
    "    print(f\"\\n🤖 Testing {classifier_name} classifier...\")\n",
    "    \n",
    "    if data is None:\n",
    "        print(f\"   ❌ No data available for testing\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        X_train = data['train']['X']\n",
    "        y_train = data['train']['y']\n",
    "        X_test = data['test']['X']\n",
    "        y_test = data['test']['y']\n",
    "        \n",
    "        print(f\"   📊 Training on {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "        print(f\"   📊 Testing on {X_test.shape[0]} samples\")\n",
    "        \n",
    "        # Get hyperparameter grid for this classifier\n",
    "        param_grid = config.classification.hyperparameter_search.get(classifier_name, {})\n",
    "        cv_config = config.classification.cross_validation\n",
    "        \n",
    "        print(f\"   🔧 Hyperparameter grid: {param_grid}\")\n",
    "        print(f\"   🔧 CV config: {cv_config.n_folds} folds, scoring={cv_config.scoring}\")\n",
    "        \n",
    "        # Initialize base classifier\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if classifier_name == 'logistic_regression':\n",
    "            base_classifier = LogisticRegression(random_state=config.job.random_seed, max_iter=1000)\n",
    "        elif classifier_name == 'svm':\n",
    "            base_classifier = SVC(random_state=config.job.random_seed, probability=True)\n",
    "        elif classifier_name == 'random_forest':\n",
    "            base_classifier = RandomForestClassifier(random_state=config.job.random_seed)\n",
    "        else:\n",
    "            print(f\"   ❌ Unknown classifier: {classifier_name}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"   ✅ Base classifier initialized: {type(base_classifier).__name__}\")\n",
    "        \n",
    "        # Test without hyperparameter search first (faster)\n",
    "        print(f\"   🧪 Testing base classifier without hyperparameter search...\")\n",
    "        \n",
    "        base_classifier.fit(X_train, y_train)\n",
    "        train_score = base_classifier.score(X_train, y_train)\n",
    "        test_score = base_classifier.score(X_test, y_test)\n",
    "        \n",
    "        print(f\"      ✅ Base training score: {train_score:.3f}\")\n",
    "        print(f\"      ✅ Base test score: {test_score:.3f}\")\n",
    "        \n",
    "        # Test cross-validation\n",
    "        print(f\"   🔄 Testing cross-validation...\")\n",
    "        \n",
    "        cv_scores = cross_val_score(\n",
    "            base_classifier, \n",
    "            X_train, \n",
    "            y_train, \n",
    "            cv=cv_config.n_folds,\n",
    "            scoring=cv_config.scoring,\n",
    "            n_jobs=cv_config.n_jobs\n",
    "        )\n",
    "        \n",
    "        print(f\"      ✅ CV scores: {cv_scores}\")\n",
    "        print(f\"      ✅ CV mean: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "        \n",
    "        # Test hyperparameter search (if parameters provided)\n",
    "        best_estimator = base_classifier\n",
    "        best_params = {}\n",
    "        best_cv_score = cv_scores.mean()\n",
    "        \n",
    "        if param_grid:\n",
    "            print(f\"   🔍 Testing hyperparameter search...\")\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                base_classifier,\n",
    "                param_grid,\n",
    "                cv=cv_config.n_folds,\n",
    "                scoring=cv_config.scoring,\n",
    "                n_jobs=cv_config.n_jobs,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            best_estimator = grid_search.best_estimator_\n",
    "            best_params = grid_search.best_params_\n",
    "            best_cv_score = grid_search.best_score_\n",
    "            \n",
    "            print(f\"      ✅ Best parameters: {best_params}\")\n",
    "            print(f\"      ✅ Best CV score: {best_cv_score:.3f}\")\n",
    "            print(f\"      📊 Grid search tested {len(grid_search.cv_results_['params'])} combinations\")\n",
    "        \n",
    "        # Test final model on test set\n",
    "        print(f\"   📊 Testing final model on test set...\")\n",
    "        \n",
    "        test_predictions = best_estimator.predict(X_test)\n",
    "        test_probabilities = None\n",
    "        \n",
    "        # Get probabilities if available\n",
    "        if hasattr(best_estimator, 'predict_proba'):\n",
    "            test_probabilities = best_estimator.predict_proba(X_test)[:, 1]\n",
    "        elif hasattr(best_estimator, 'decision_function'):\n",
    "            test_probabilities = best_estimator.decision_function(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, test_predictions),\n",
    "            'precision': precision_score(y_test, test_predictions, average='binary', zero_division=0),\n",
    "            'recall': recall_score(y_test, test_predictions, average='binary', zero_division=0),\n",
    "            'f1_score': f1_score(y_test, test_predictions, average='binary', zero_division=0)\n",
    "        }\n",
    "        \n",
    "        if test_probabilities is not None:\n",
    "            try:\n",
    "                metrics['roc_auc'] = roc_auc_score(y_test, test_probabilities)\n",
    "            except ValueError:\n",
    "                metrics['roc_auc'] = None  # May fail if only one class in test set\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, test_predictions)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"      ✅ Final test metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            if value is not None:\n",
    "                print(f\"         {metric}: {value:.3f}\")\n",
    "        \n",
    "        print(f\"      📊 Confusion matrix:\")\n",
    "        print(f\"         TN={cm[0,0]}, FP={cm[0,1]}\")\n",
    "        print(f\"         FN={cm[1,0]}, TP={cm[1,1]}\")\n",
    "        \n",
    "        print(f\"      ⏱️  Total training time: {training_time:.2f}s\")\n",
    "        \n",
    "        return {\n",
    "            'classifier_name': classifier_name,\n",
    "            'best_estimator': best_estimator,\n",
    "            'best_params': best_params,\n",
    "            'best_cv_score': best_cv_score,\n",
    "            'test_metrics': metrics,\n",
    "            'confusion_matrix': cm,\n",
    "            'training_time': training_time,\n",
    "            'predictions': test_predictions,\n",
    "            'probabilities': test_probabilities\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Classifier testing failed: {e}\")\n",
    "        print(f\"\\nError details:\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Test all classifiers\n",
    "classifier_results = {}\n",
    "\n",
    "if test_config and loaded_data:\n",
    "    for classifier_name in test_config.classification.models:\n",
    "        result = test_individual_classifier(classifier_name, test_config, loaded_data)\n",
    "        if result:\n",
    "            classifier_results[classifier_name] = result\nelse:\n",
    "    print(f\"⚠️  Skipping classifier tests due to config or data issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Model Serialization and Loading\n",
    "Test saving and loading trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model serialization and loading\n",
    "def test_model_serialization(results: Dict, config: PipelineConfig):\n",
    "    \"\"\"Test saving and loading trained models.\"\"\"\n",
    "    \n",
    "    print(f\"\\n💾 Testing model serialization and loading...\")\n",
    "    \n",
    "    if not results:\n",
    "        print(f\"   ❌ No trained models available for testing\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Create models directory\n",
    "        output_dir = Path(config.job.output_dir) / config.job.name\n",
    "        models_dir = output_dir / config.output.models_dir\n",
    "        models_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"   📁 Models directory: {models_dir}\")\n",
    "        \n",
    "        serialization_results = {}\n",
    "        \n",
    "        for classifier_name, result in results.items():\n",
    "            print(f\"\\n   💾 Testing {classifier_name} serialization...\")\n",
    "            \n",
    "            model = result['best_estimator']\n",
    "            model_file = models_dir / f\"{classifier_name}_model.pkl\"\n",
    "            \n",
    "            # Test saving\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with open(model_file, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "            \n",
    "            save_time = time.time() - start_time\n",
    "            file_size = model_file.stat().st_size\n",
    "            \n",
    "            print(f\"      ✅ Model saved: {model_file}\")\n",
    "            print(f\"      📊 File size: {file_size / 1024:.1f} KB\")\n",
    "            print(f\"      ⏱️  Save time: {save_time:.3f}s\")\n",
    "            \n",
    "            # Test loading\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with open(model_file, 'rb') as f:\n",
    "                loaded_model = pickle.load(f)\n",
    "            \n",
    "            load_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"      ✅ Model loaded successfully\")\n",
    "            print(f\"      ⏱️  Load time: {load_time:.3f}s\")\n",
    "            \n",
    "            # Test that loaded model works\n",
    "            if loaded_data:\n",
    "                X_test = loaded_data['test']['X']\n",
    "                original_predictions = model.predict(X_test)\n",
    "                loaded_predictions = loaded_model.predict(X_test)\n",
    "                \n",
    "                predictions_match = np.array_equal(original_predictions, loaded_predictions)\n",
    "                \n",
    "                if predictions_match:\n",
    "                    print(f\"      ✅ Loaded model predictions match original\")\n",
    "                else:\n",
    "                    print(f\"      ❌ Loaded model predictions differ from original\")\n",
    "                \n",
    "                # Test model parameters\n",
    "                if hasattr(model, 'get_params') and hasattr(loaded_model, 'get_params'):\n",
    "                    original_params = model.get_params()\n",
    "                    loaded_params = loaded_model.get_params()\n",
    "                    \n",
    "                    params_match = original_params == loaded_params\n",
    "                    if params_match:\n",
    "                        print(f\"      ✅ Model parameters preserved\")\n",
    "                    else:\n",
    "                        print(f\"      ⚠️  Some model parameters may differ\")\n",
    "            \n",
    "            serialization_results[classifier_name] = {\n",
    "                'file_path': model_file,\n",
    "                'file_size_kb': file_size / 1024,\n",
    "                'save_time': save_time,\n",
    "                'load_time': load_time,\n",
    "                'loaded_successfully': True\n",
    "            }\n",
    "        \n",
    "        # Summary of serialization results\n",
    "        print(f\"\\n   📊 Serialization summary:\")\n",
    "        total_size = sum(r['file_size_kb'] for r in serialization_results.values())\n",
    "        print(f\"      📊 Total models size: {total_size:.1f} KB\")\n",
    "        print(f\"      📊 Models saved: {len(serialization_results)}\")\n",
    "        \n",
    "        # Test loading all models at once (memory test)\n",
    "        print(f\"\\n   🧠 Testing loading all models simultaneously...\")\n",
    "        all_models = {}\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for classifier_name, result in serialization_results.items():\n",
    "            with open(result['file_path'], 'rb') as f:\n",
    "                all_models[classifier_name] = pickle.load(f)\n",
    "        \n",
    "        load_all_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"      ✅ All models loaded: {len(all_models)} models in {load_all_time:.3f}s\")\n",
    "        \n",
    "        # Cleanup test files\n",
    "        for result in serialization_results.values():\n",
    "            try:\n",
    "                result['file_path'].unlink()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(f\"   🧹 Cleaned up test model files\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Model serialization test failed: {e}\")\n",
    "        print(f\"\\nError details:\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Test model serialization\n",
    "if classifier_results and test_config:\n",
    "    serialization_success = test_model_serialization(classifier_results, test_config)\nelse:\n",
    "    print(f\"⚠️  Skipping model serialization test\")\n",
    "    serialization_success = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Edge Cases and Error Handling\n",
    "Test various edge cases that might cause classification issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test edge cases and error handling\n",
    "def test_classification_edge_cases():\n",
    "    \"\"\"Test edge cases for classification pipeline.\"\"\"\n",
    "    \n",
    "    print(f\"\\n🧪 Testing classification edge cases...\")\n",
    "    \n",
    "    # Test case 1: Single class data\n",
    "    print(f\"\\n1. Testing single class data...\")\n",
    "    try:\n",
    "        # Create data with only one class\n",
    "        single_class_X = np.random.normal(0, 1, (20, 10))\n",
    "        single_class_y = np.ones(20)  # All class 1\n",
    "        \n",
    "        lr = LogisticRegression(random_state=42)\n",
    "        lr.fit(single_class_X, single_class_y)\n",
    "        \n",
    "        predictions = lr.predict(single_class_X)\n",
    "        unique_predictions = np.unique(predictions)\n",
    "        \n",
    "        print(f\"   ✅ Single class training completed\")\n",
    "        print(f\"   📊 Unique predictions: {unique_predictions}\")\n",
    "        \n",
    "        # Try to calculate ROC-AUC (should fail gracefully)\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(single_class_y, predictions)\n",
    "            print(f\"   ⚠️  ROC-AUC calculated: {roc_auc} (unexpected)\")\n",
    "        except ValueError as e:\n",
    "            print(f\"   ✅ ROC-AUC correctly failed: {str(e)[:50]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Single class test failed: {e}\")\n",
    "    \n",
    "    # Test case 2: Perfect separation\n",
    "    print(f\"\\n2. Testing perfectly separable data...\")\n",
    "    try:\n",
    "        # Create perfectly separable data\n",
    "        perfect_X = np.vstack([\n",
    "            np.random.normal(-2, 0.1, (20, 2)),  # Class 0: far left\n",
    "            np.random.normal(2, 0.1, (20, 2))    # Class 1: far right\n",
    "        ])\n",
    "        perfect_y = np.hstack([np.zeros(20), np.ones(20)])\n",
    "        \n",
    "        lr = LogisticRegression(random_state=42)\n",
    "        lr.fit(perfect_X, perfect_y)\n",
    "        \n",
    "        train_accuracy = lr.score(perfect_X, perfect_y)\n",
    "        print(f\"   ✅ Perfect separation training completed\")\n",
    "        print(f\"   📊 Training accuracy: {train_accuracy:.3f}\")\n",
    "        \n",
    "        if train_accuracy > 0.95:\n",
    "            print(f\"   ✅ Data is indeed perfectly/nearly separable\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Perfect separation test failed: {e}\")\n",
    "    \n",
    "    # Test case 3: Very small dataset\n",
    "    print(f\"\\n3. Testing very small dataset...\")\n",
    "    try:\n",
    "        # Create tiny dataset\n",
    "        tiny_X = np.random.normal(0, 1, (4, 5))  # Only 4 samples\n",
    "        tiny_y = np.array([0, 1, 0, 1])\n",
    "        \n",
    "        lr = LogisticRegression(random_state=42)\n",
    "        lr.fit(tiny_X, tiny_y)\n",
    "        \n",
    "        # Try cross-validation (should handle small dataset)\n",
    "        try:\n",
    "            cv_scores = cross_val_score(lr, tiny_X, tiny_y, cv=2)  # Only 2 folds possible\n",
    "            print(f\"   ✅ Small dataset training completed\")\n",
    "            print(f\"   📊 CV scores: {cv_scores}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"   ⚠️  CV failed on small dataset: {str(e)[:50]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Small dataset test failed: {e}\")\n",
    "    \n",
    "    # Test case 4: High-dimensional data (more features than samples)\n",
    "    print(f\"\\n4. Testing high-dimensional data...\")\n",
    "    try:\n",
    "        # Create data with more features than samples\n",
    "        high_dim_X = np.random.normal(0, 1, (10, 50))  # 10 samples, 50 features\n",
    "        high_dim_y = np.array([0, 1] * 5)\n",
    "        \n",
    "        lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        lr.fit(high_dim_X, high_dim_y)\n",
    "        \n",
    "        train_accuracy = lr.score(high_dim_X, high_dim_y)\n",
    "        print(f\"   ✅ High-dimensional training completed\")\n",
    "        print(f\"   📊 Training accuracy: {train_accuracy:.3f}\")\n",
    "        \n",
    "        if train_accuracy > 0.9:\n",
    "            print(f\"   ⚠️  Possible overfitting detected (high accuracy on small sample)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ High-dimensional test failed: {e}\")\n",
    "    \n",
    "    # Test case 5: NaN/Inf values\n",
    "    print(f\"\\n5. Testing data with NaN/Inf values...\")\n",
    "    try:\n",
    "        # Create data with problematic values\n",
    "        problematic_X = np.random.normal(0, 1, (20, 5))\n",
    "        problematic_X[0, 0] = np.nan  # NaN value\n",
    "        problematic_X[1, 1] = np.inf  # Inf value\n",
    "        problematic_y = np.array([0, 1] * 10)\n",
    "        \n",
    "        lr = LogisticRegression(random_state=42)\n",
    "        \n",
    "        try:\n",
    "            lr.fit(problematic_X, problematic_y)\n",
    "            print(f\"   ⚠️  Training with NaN/Inf succeeded (unexpected)\")\n",
    "        except ValueError as e:\n",
    "            print(f\"   ✅ Training correctly failed with NaN/Inf: {str(e)[:50]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ NaN/Inf test failed: {e}\")\n",
    "    \n",
    "    print(f\"\\n   📊 Edge case testing completed\")\n",
    "\n",
    "# Run edge case tests\n",
    "test_classification_edge_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Performance and Memory Analysis\n",
    "Analyze training performance and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance and memory analysis\n",
    "def test_classification_performance(results: Dict, config: PipelineConfig):\n",
    "    \"\"\"Analyze classification performance and memory usage.\"\"\"\n",
    "    \n",
    "    print(f\"\\n⚡ Classification performance analysis...\")\n",
    "    \n",
    "    if not results:\n",
    "        print(f\"   ❌ No results available for analysis\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Performance comparison\n",
    "        print(f\"   📊 Classifier performance comparison:\")\n",
    "        \n",
    "        performance_data = []\n",
    "        for classifier_name, result in results.items():\n",
    "            metrics = result['test_metrics']\n",
    "            performance_data.append({\n",
    "                'Classifier': classifier_name,\n",
    "                'Accuracy': f\"{metrics['accuracy']:.3f}\",\n",
    "                'Precision': f\"{metrics['precision']:.3f}\",\n",
    "                'Recall': f\"{metrics['recall']:.3f}\",\n",
    "                'F1-Score': f\"{metrics['f1_score']:.3f}\",\n",
    "                'ROC-AUC': f\"{metrics.get('roc_auc', 'N/A'):.3f}\" if metrics.get('roc_auc') else 'N/A',\n",
    "                'Training Time': f\"{result['training_time']:.2f}s\"\n",
    "            })\n",
    "        \n",
    "        performance_df = pd.DataFrame(performance_data)\n",
    "        print(\"\")\n",
    "        print(performance_df.to_string(index=False))\n",
    "        \n",
    "        # Find best performer\n",
    "        best_accuracy = max(results.items(), key=lambda x: x[1]['test_metrics']['accuracy'])\n",
    "        best_f1 = max(results.items(), key=lambda x: x[1]['test_metrics']['f1_score'])\n",
    "        fastest = min(results.items(), key=lambda x: x[1]['training_time'])\n",
    "        \n",
    "        print(f\"\\n   🏆 Performance highlights:\")\n",
    "        print(f\"      Best accuracy: {best_accuracy[0]} ({best_accuracy[1]['test_metrics']['accuracy']:.3f})\")\n",
    "        print(f\"      Best F1-score: {best_f1[0]} ({best_f1[1]['test_metrics']['f1_score']:.3f})\")\n",
    "        print(f\"      Fastest training: {fastest[0]} ({fastest[1]['training_time']:.2f}s)\")\n",
    "        \n",
    "        # Training time analysis\n",
    "        print(f\"\\n   ⏱️  Training time analysis:\")\n",
    "        \n",
    "        total_time = sum(r['training_time'] for r in results.values())\n",
    "        print(f\"      Total training time: {total_time:.2f}s\")\n",
    "        \n",
    "        # Estimate scaling\n",
    "        current_samples = len(loaded_data['train']['X']) if loaded_data else 0\n",
    "        if current_samples > 0:\n",
    "            time_per_sample = total_time / current_samples\n",
    "            \n",
    "            print(f\"      Time per sample: {time_per_sample:.4f}s\")\n",
    "            print(f\"      Estimated time for larger datasets:\")\n",
    "            \n",
    "            for dataset_size in [1000, 10000, 100000]:\n",
    "                estimated_time = time_per_sample * dataset_size\n",
    "                if estimated_time < 60:\n",
    "                    print(f\"         {dataset_size:,} samples: {estimated_time:.1f}s\")\n",
    "                elif estimated_time < 3600:\n",
    "                    print(f\"         {dataset_size:,} samples: {estimated_time/60:.1f}m\")\n",
    "                else:\n",
    "                    print(f\"         {dataset_size:,} samples: {estimated_time/3600:.1f}h\")\n",
    "        \n",
    "        # Memory analysis\n",
    "        print(f\"\\n   🧠 Memory analysis:\")\n",
    "        \n",
    "        if loaded_data:\n",
    "            train_memory = loaded_data['train']['X'].nbytes / 1024 / 1024  # MB\n",
    "            test_memory = loaded_data['test']['X'].nbytes / 1024 / 1024   # MB\n",
    "            \n",
    "            print(f\"      Training data: {train_memory:.2f} MB\")\n",
    "            print(f\"      Test data: {test_memory:.2f} MB\")\n",
    "            print(f\"      Total data: {train_memory + test_memory:.2f} MB\")\n",
    "            \n",
    "            # Estimate memory for larger datasets\n",
    "            bytes_per_sample = loaded_data['train']['X'].nbytes / len(loaded_data['train']['X'])\n",
    "            print(f\"      Memory per sample: {bytes_per_sample / 1024:.1f} KB\")\n",
    "            \n",
    "            print(f\"      Estimated memory for larger datasets:\")\n",
    "            for dataset_size in [1000, 10000, 100000]:\n",
    "                estimated_mb = (bytes_per_sample * dataset_size) / 1024 / 1024\n",
    "                if estimated_mb < 1024:\n",
    "                    print(f\"         {dataset_size:,} samples: {estimated_mb:.1f} MB\")\n",
    "                else:\n",
    "                    print(f\"         {dataset_size:,} samples: {estimated_mb/1024:.1f} GB\")\n",
    "        \n",
    "        # Configuration recommendations\n",
    "        print(f\"\\n   💡 Configuration recommendations:\")\n",
    "        \n",
    "        # CV recommendations\n",
    "        current_folds = config.classification.cross_validation.n_folds\n",
    "        if current_samples and current_samples < current_folds * 10:\n",
    "            recommended_folds = max(2, current_samples // 10)\n",
    "            print(f\"      🔧 Consider reducing CV folds to {recommended_folds} for small datasets\")\n",
    "        \n",
    "        # Hyperparameter search recommendations\n",
    "        if fastest[1]['training_time'] > 60:  # If slowest is > 1 minute\n",
    "            print(f\"      🔧 Consider reducing hyperparameter search space for faster training\")\n",
    "        \n",
    "        # Model selection recommendations\n",
    "        accuracy_diff = max(r['test_metrics']['accuracy'] for r in results.values()) - \\\n",
    "                       min(r['test_metrics']['accuracy'] for r in results.values())\n",
    "        \n",
    "        if accuracy_diff < 0.05:  # Less than 5% difference\n",
    "            print(f\"      📊 All classifiers perform similarly - consider using the fastest ({fastest[0]})\")\n",
    "        else:\n",
    "            print(f\"      📊 Significant performance differences - use best performer ({best_accuracy[0]})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Performance analysis failed: {e}\")\n",
    "        print(f\"\\nError details:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run performance analysis\n",
    "if classifier_results and test_config:\n",
    "    test_classification_performance(classifier_results, test_config)\nelse:\n",
    "    print(f\"⚠️  Skipping performance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Summary and Recommendations\n",
    "Summarize all test results and provide actionable recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize classification pipeline test results\n",
    "print(\"\\n📋 Classification Pipeline Debug Test Summary:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Collect test results\n",
    "test_results = [\n",
    "    (\"Configuration Validation\", test_config is not None),\n",
    "    (\"Embeddings Data Loading\", loaded_data is not None),\n",
    "    (\"Classifier Training\", len(classifier_results) > 0),\n",
    "    (\"Model Serialization\", serialization_success),\n",
    "]\n",
    "\n",
    "# Calculate success rate\n",
    "passed_tests = [name for name, result in test_results if result]\n",
    "failed_tests = [name for name, result in test_results if not result]\n",
    "success_rate = len(passed_tests) / len(test_results) * 100\n",
    "\n",
    "print(f\"\\n🎯 Test Results Summary:\")\n",
    "for test_name, result in test_results:\n",
    "    status = \"✅ PASS\" if result else \"❌ FAIL\"\n",
    "    print(f\"   {status} {test_name}\")\n",
    "\n",
    "print(f\"\\n📊 Overall Success Rate: {success_rate:.0f}%\")\n",
    "\n",
    "# Detailed analysis\n",
    "print(f\"\\n🔍 Detailed Analysis:\")\n",
    "\n",
    "if classifier_results:\n",
    "    print(f\"   ✅ Successfully trained {len(classifier_results)} classifiers\")\n",
    "    \n",
    "    best_performer = max(classifier_results.items(), \n",
    "                        key=lambda x: x[1]['test_metrics']['accuracy'])\n",
    "    \n",
    "    print(f\"   🏆 Best performer: {best_performer[0]} (accuracy: {best_performer[1]['test_metrics']['accuracy']:.3f})\")\n",
    "    \n",
    "    # Check if results are reasonable\n",
    "    accuracies = [r['test_metrics']['accuracy'] for r in classifier_results.values()]\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    \n",
    "    if avg_accuracy > 0.9:\n",
    "        print(f\"   📊 Excellent performance: average accuracy {avg_accuracy:.3f}\")\n",
    "    elif avg_accuracy > 0.7:\n",
    "        print(f\"   📊 Good performance: average accuracy {avg_accuracy:.3f}\")\n",
    "    elif avg_accuracy > 0.5:\n",
    "        print(f\"   📊 Moderate performance: average accuracy {avg_accuracy:.3f}\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  Low performance: average accuracy {avg_accuracy:.3f} - check data quality\")\n",
    "\n",
    "else:\n",
    "    print(f\"   ❌ No classifiers were successfully trained\")\n",
    "\n",
    "# Recommendations based on results\n",
    "print(f\"\\n💡 Recommendations:\")\n",
    "\n",
    "if success_rate >= 75:\n",
    "    print(f\"   🎉 Classification pipeline is working well!\")\n",
    "    print(f\"   ➡️  Next: Test evaluation pipeline to generate metrics and plots\")\n",
    "    print(f\"   ➡️  Or: Test with real embeddings from your embedding pipeline\")\nelif success_rate >= 50:\n",
    "    print(f\"   ⚠️  Most components working, but some issues need attention\")\n",
    "    print(f\"   🔧 Focus on fixing the failed tests listed above\")\nelse:\n",
    "    print(f\"   🚨 Significant issues detected - troubleshooting needed\")\n",
    "    print(f\"   🔧 Address data loading and configuration issues first\")\n",
    "\n",
    "# Configuration recommendations\n",
    "if test_config and classifier_results:\n",
    "    print(f\"\\n🔧 Configuration for next steps:\")\n",
    "    print(f\"   # Use this configuration for evaluation pipeline testing\")\n",
    "    print(f\"   input:\")\n",
    "    print(f\"     train_embeddings_path: \\\"{test_config.input.train_embeddings_path}\\\"\")\n",
    "    print(f\"     test_embeddings_path: \\\"{test_config.input.test_embeddings_path}\\\"\")\n",
    "    print(f\"   classification:\")\n",
    "    print(f\"     models: {test_config.classification.models}\")\n",
    "    print(f\"     cross_validation:\")\n",
    "    print(f\"       n_folds: {test_config.classification.cross_validation.n_folds}\")\n",
    "    print(f\"       scoring: \\\"{test_config.classification.cross_validation.scoring}\\\"\")\n",
    "\n",
    "print(f\"\\n📚 Next testing steps:\")\n",
    "if success_rate >= 75:\n",
    "    print(f\"   1. Run test_05_Target_Word_Evaluation_Debug.ipynb\")\n",
    "    print(f\"   2. Run test_06_Evaluation_Pipeline_Debug.ipynb\")\n",
    "    print(f\"   3. Test with real embeddings from your embedding pipeline\")\nelse:\n",
    "    print(f\"   1. Fix failed tests in this notebook\")\n",
    "    print(f\"   2. Verify embeddings data format and quality\")\n",
    "    print(f\"   3. Re-run this notebook until all tests pass\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\n🧹 Cleaning up test files...\")\n",
    "try:\n",
    "    # Clean up test embeddings directory\n",
    "    import shutil\n",
    "    if test_embeddings_dir.exists():\n",
    "        shutil.rmtree(test_embeddings_dir)\n",
    "        print(f\"   🗑️  Removed: {test_embeddings_dir}\")\n",
    "    \n",
    "    # Clean up test outputs directory\n",
    "    test_output_dir = mgpt_eval_path / \"examples\" / \"test_classification_outputs\"\n",
    "    if test_output_dir.exists():\n",
    "        shutil.rmtree(test_output_dir)\n",
    "        print(f\"   🗑️  Removed: test_classification_outputs directory\")\n",
    "        \nexcept Exception as e:\n",
    "    print(f\"   ⚠️  Cleanup warning: {e}\")\n",
    "\n",
    "print(f\"\\n✅ Classification pipeline debugging complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Debug Cell (Run if needed)\n",
    "Use this cell to test specific scenarios or debug issues found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug cell - modify as needed for specific testing\n",
    "\n",
    "# Example: Test with different hyperparameter grids\n",
    "# if loaded_data:\n",
    "#     print(\"Testing custom hyperparameter grid:\")\n",
    "#     custom_grid = {'C': [0.01, 0.1, 1.0, 10.0], 'penalty': ['l1', 'l2']}\n",
    "#     \n",
    "#     lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "#     grid_search = GridSearchCV(lr, custom_grid, cv=3, scoring='roc_auc')\n",
    "#     grid_search.fit(loaded_data['train']['X'], loaded_data['train']['y'])\n",
    "#     \n",
    "#     print(f\"Best params: {grid_search.best_params_}\")\n",
    "#     print(f\"Best score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Example: Test feature scaling\n",
    "# if loaded_data:\n",
    "#     print(\"Testing with feature scaling:\")\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(loaded_data['train']['X'])\n",
    "#     X_test_scaled = scaler.transform(loaded_data['test']['X'])\n",
    "#     \n",
    "#     lr = LogisticRegression(random_state=42)\n",
    "#     lr.fit(X_train_scaled, loaded_data['train']['y'])\n",
    "#     \n",
    "#     scaled_accuracy = lr.score(X_test_scaled, loaded_data['test']['y'])\n",
    "#     print(f\"Scaled accuracy: {scaled_accuracy:.3f}\")\n",
    "\n",
    "# Example: Analyze feature importance (for Random Forest)\n",
    "# if 'random_forest' in classifier_results:\n",
    "#     rf_model = classifier_results['random_forest']['best_estimator']\n",
    "#     if hasattr(rf_model, 'feature_importances_'):\n",
    "#         importances = rf_model.feature_importances_\n",
    "#         top_features = np.argsort(importances)[-10:]  # Top 10 features\n",
    "#         print(f\"Top 10 feature importances: {importances[top_features]}\")\n",
    "#         print(f\"Top 10 feature indices: {top_features}\")\n",
    "\n",
    "# Example: Test cross-validation stability\n",
    "# if loaded_data:\n",
    "#     print(\"Testing CV stability across multiple runs:\")\n",
    "#     lr = LogisticRegression(random_state=42)\n",
    "#     \n",
    "#     for run in range(3):\n",
    "#         cv_scores = cross_val_score(lr, loaded_data['train']['X'], loaded_data['train']['y'], \n",
    "#                                   cv=3, scoring='accuracy')\n",
    "#         print(f\"Run {run + 1}: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "print(\"💡 Use this cell to run custom classification tests and debug specific issues.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}