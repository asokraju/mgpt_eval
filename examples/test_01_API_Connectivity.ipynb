{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîå API Connectivity Testing\n",
    "\n",
    "## Purpose\n",
    "This notebook tests the MGPT API connectivity step-by-step to debug connection issues, validate API responses, and ensure proper communication with your model server.\n",
    "\n",
    "## What This Tests\n",
    "- Configuration loading and validation\n",
    "- Basic API connectivity (health checks)\n",
    "- Individual endpoint testing (`/embeddings`, `/embeddings_batch`, `/generate`, `/generate_batch`)\n",
    "- Error handling and retry mechanisms\n",
    "- Response format validation\n",
    "- Performance and timeout testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Add the mgpt_eval directory to Python path to import pipeline modules\n",
    "mgpt_eval_path = Path.cwd().parent if Path.cwd().name == 'examples' else Path.cwd()\n",
    "sys.path.insert(0, str(mgpt_eval_path))\n",
    "\n",
    "# Import actual pipeline modules\n",
    "from models.config_models import PipelineConfig\n",
    "from utils.logging_utils import setup_logging\n",
    "\n",
    "print(f\"‚úÖ Working directory: {Path.cwd()}\")\n",
    "print(f\"‚úÖ MGPT-eval path: {mgpt_eval_path}\")\n",
    "print(f\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Configuration\n",
    "First, we'll load a test configuration and validate it using the actual config validation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from template\n",
    "config_path = mgpt_eval_path / \"configs\" / \"templates\" / \"04_full_pipeline.yaml\"\n",
    "\n",
    "print(f\"üìÑ Loading configuration from: {config_path}\")\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "print(\"\\nüîß Raw configuration loaded:\")\n",
    "print(json.dumps({\n",
    "    'job_name': config_dict.get('job', {}).get('name'),\n",
    "    'model_api_url': config_dict.get('model_api', {}).get('base_url'),\n",
    "    'batch_size': config_dict.get('model_api', {}).get('batch_size'),\n",
    "    'timeout': config_dict.get('model_api', {}).get('timeout')\n",
    "}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate configuration using actual PipelineConfig model\n",
    "try:\n",
    "    config = PipelineConfig(**config_dict)\n",
    "    print(\"‚úÖ Configuration validation successful\")\n",
    "    \n",
    "    # Extract API configuration\n",
    "    api_config = config.model_api\n",
    "    print(f\"\\nüåê API Configuration:\")\n",
    "    print(f\"  Base URL: {api_config.base_url}\")\n",
    "    print(f\"  Batch size: {api_config.batch_size}\")\n",
    "    print(f\"  Timeout: {api_config.timeout}s\")\n",
    "    print(f\"  Max retries: {api_config.max_retries}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration validation failed: {e}\")\n",
    "    # You can modify the config_dict here to fix issues\n",
    "    print(\"\\nüí° Tip: Check the configuration format and required fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Basic Connectivity Test\n",
    "Test basic HTTP connectivity to the API server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override API URL for testing (modify as needed)\n",
    "# Uncomment and modify the line below to test with your actual API\n",
    "# api_config.base_url = \"http://localhost:8000\"  # Your actual API URL\n",
    "\n",
    "base_url = api_config.base_url\n",
    "timeout = api_config.timeout\n",
    "\n",
    "print(f\"üîç Testing connectivity to: {base_url}\")\n",
    "\n",
    "# Test basic connectivity\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    response = requests.get(f\"{base_url}/health\", timeout=10)\n",
    "    response_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Connection successful!\")\n",
    "    print(f\"   Status code: {response.status_code}\")\n",
    "    print(f\"   Response time: {response_time:.2f}s\")\n",
    "    print(f\"   Response headers: {dict(response.headers)}\")\n",
    "    \n",
    "    # Try to parse response\n",
    "    try:\n",
    "        response_data = response.json()\n",
    "        print(f\"   Response data: {json.dumps(response_data, indent=2)}\")\n",
    "    except:\n",
    "        print(f\"   Response text: {response.text[:200]}...\")\n",
    "        \n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting tips:\")\n",
    "    print(\"   1. Check if the server is running\")\n",
    "    print(\"   2. Verify the URL and port\")\n",
    "    print(\"   3. Check firewall/network settings\")\n",
    "    \n",
    "except requests.exceptions.Timeout as e:\n",
    "    print(f\"‚ùå Connection timeout: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting tips:\")\n",
    "    print(\"   1. Increase timeout value\")\n",
    "    print(\"   2. Check server performance\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Individual Endpoints\n",
    "Test each API endpoint individually with sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample medical claims data for testing\n",
    "sample_claims = [\n",
    "    \"N6320 G0378 |eoc| Z91048 M1710\",\n",
    "    \"E119 76642 |eoc| K9289 O0903\",\n",
    "    \"I10 E785 |eoc| Z1239 M549\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Sample claims for testing:\")\n",
    "for i, claim in enumerate(sample_claims, 1):\n",
    "    print(f\"  {i}. {claim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test /embeddings endpoint (single embedding)\n",
    "def test_embeddings_endpoint():\n",
    "    endpoint = f\"{base_url}/embeddings\"\n",
    "    payload = {\"text\": sample_claims[0]}\n",
    "    \n",
    "    print(f\"\\nüî¨ Testing /embeddings endpoint\")\n",
    "    print(f\"   URL: {endpoint}\")\n",
    "    print(f\"   Payload: {payload}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            endpoint,\n",
    "            json=payload,\n",
    "            timeout=timeout,\n",
    "            headers={\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"   ‚úÖ Status: {response.status_code}\")\n",
    "        print(f\"   ‚è±Ô∏è  Response time: {response_time:.2f}s\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if 'embedding' in data:\n",
    "                embedding = data['embedding']\n",
    "                print(f\"   üìä Embedding received: {len(embedding)} dimensions\")\n",
    "                print(f\"   üìä Sample values: {embedding[:5]}...\")\n",
    "                return True, data\n",
    "            else:\n",
    "                print(f\"   ‚ùå No 'embedding' field in response: {data}\")\n",
    "                return False, data\n",
    "        else:\n",
    "            print(f\"   ‚ùå Error response: {response.text}\")\n",
    "            return False, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Request failed: {e}\")\n",
    "        return False, None\n",
    "\n",
    "# Run the test\n",
    "embeddings_success, embeddings_data = test_embeddings_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test /embeddings_batch endpoint (multiple embeddings)\n",
    "def test_embeddings_batch_endpoint():\n",
    "    endpoint = f\"{base_url}/embeddings_batch\"\n",
    "    payload = {\"texts\": sample_claims}\n",
    "    \n",
    "    print(f\"\\nüî¨ Testing /embeddings_batch endpoint\")\n",
    "    print(f\"   URL: {endpoint}\")\n",
    "    print(f\"   Payload: {len(sample_claims)} texts\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            endpoint,\n",
    "            json=payload,\n",
    "            timeout=timeout,\n",
    "            headers={\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"   ‚úÖ Status: {response.status_code}\")\n",
    "        print(f\"   ‚è±Ô∏è  Response time: {response_time:.2f}s\")\n",
    "        print(f\"   üìä Throughput: {len(sample_claims)/response_time:.1f} texts/sec\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if 'embeddings' in data:\n",
    "                embeddings = data['embeddings']\n",
    "                print(f\"   üìä Embeddings received: {len(embeddings)} embeddings\")\n",
    "                print(f\"   üìä Embedding dimensions: {len(embeddings[0])}\")\n",
    "                print(f\"   üìä Sample values: {embeddings[0][:3]}...\")\n",
    "                return True, data\n",
    "            else:\n",
    "                print(f\"   ‚ùå No 'embeddings' field in response: {data}\")\n",
    "                return False, data\n",
    "        else:\n",
    "            print(f\"   ‚ùå Error response: {response.text}\")\n",
    "            return False, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Request failed: {e}\")\n",
    "        return False, None\n",
    "\n",
    "# Run the test\n",
    "batch_embeddings_success, batch_embeddings_data = test_embeddings_batch_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test /generate endpoint (single text generation)\n",
    "def test_generate_endpoint():\n",
    "    endpoint = f\"{base_url}/generate\"\n",
    "    payload = {\n",
    "        \"prompt\": sample_claims[0],\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_k\": 50\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüî¨ Testing /generate endpoint\")\n",
    "    print(f\"   URL: {endpoint}\")\n",
    "    print(f\"   Prompt: {payload['prompt']}\")\n",
    "    print(f\"   Parameters: max_tokens={payload['max_new_tokens']}, temp={payload['temperature']}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            endpoint,\n",
    "            json=payload,\n",
    "            timeout=timeout,\n",
    "            headers={\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"   ‚úÖ Status: {response.status_code}\")\n",
    "        print(f\"   ‚è±Ô∏è  Response time: {response_time:.2f}s\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if 'generated_text' in data:\n",
    "                generated_text = data['generated_text']\n",
    "                print(f\"   üìù Generated text: {generated_text}\")\n",
    "                print(f\"   üìä Length: {len(generated_text.split())} tokens\")\n",
    "                return True, data\n",
    "            else:\n",
    "                print(f\"   ‚ùå No 'generated_text' field in response: {data}\")\n",
    "                return False, data\n",
    "        else:\n",
    "            print(f\"   ‚ùå Error response: {response.text}\")\n",
    "            return False, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Request failed: {e}\")\n",
    "        return False, None\n",
    "\n",
    "# Run the test\n",
    "generate_success, generate_data = test_generate_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test /generate_batch endpoint (multiple text generations)\n",
    "def test_generate_batch_endpoint():\n",
    "    endpoint = f\"{base_url}/generate_batch\"\n",
    "    payload = {\n",
    "        \"prompts\": sample_claims[:2],  # Test with 2 prompts\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_k\": 50,\n",
    "        \"num_return_sequences\": 3  # Generate 3 sequences per prompt\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüî¨ Testing /generate_batch endpoint\")\n",
    "    print(f\"   URL: {endpoint}\")\n",
    "    print(f\"   Prompts: {len(payload['prompts'])} prompts\")\n",
    "    print(f\"   Sequences per prompt: {payload['num_return_sequences']}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            endpoint,\n",
    "            json=payload,\n",
    "            timeout=timeout * 2,  # Longer timeout for batch generation\n",
    "            headers={\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"   ‚úÖ Status: {response.status_code}\")\n",
    "        print(f\"   ‚è±Ô∏è  Response time: {response_time:.2f}s\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if 'generated_texts' in data:\n",
    "                generated_texts = data['generated_texts']\n",
    "                print(f\"   üìù Generated texts: {len(generated_texts)} prompt groups\")\n",
    "                \n",
    "                for i, prompt_generations in enumerate(generated_texts):\n",
    "                    print(f\"   üìù Prompt {i+1}: {len(prompt_generations)} generations\")\n",
    "                    for j, text in enumerate(prompt_generations[:2]):  # Show first 2\n",
    "                        print(f\"      {j+1}. {text[:100]}...\")\n",
    "                        \n",
    "                return True, data\n",
    "            else:\n",
    "                print(f\"   ‚ùå No 'generated_texts' field in response: {data}\")\n",
    "                return False, data\n",
    "        else:\n",
    "            print(f\"   ‚ùå Error response: {response.text}\")\n",
    "            return False, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Request failed: {e}\")\n",
    "        return False, None\n",
    "\n",
    "# Run the test\n",
    "batch_generate_success, batch_generate_data = test_generate_batch_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Error Handling and Retry Logic\n",
    "Test how the API handles various error conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test error handling scenarios\n",
    "def test_error_scenarios():\n",
    "    print(\"\\nüß™ Testing error handling scenarios:\")\n",
    "    \n",
    "    # Test 1: Invalid endpoint\n",
    "    print(\"\\n1. Testing invalid endpoint...\")\n",
    "    try:\n",
    "        response = requests.get(f\"{base_url}/invalid_endpoint\", timeout=10)\n",
    "        print(f\"   Status: {response.status_code}\")\n",
    "        print(f\"   Response: {response.text[:100]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    # Test 2: Invalid JSON payload\n",
    "    print(\"\\n2. Testing invalid JSON payload...\")\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{base_url}/embeddings\",\n",
    "            json={\"invalid_field\": \"test\"},\n",
    "            timeout=10\n",
    "        )\n",
    "        print(f\"   Status: {response.status_code}\")\n",
    "        print(f\"   Response: {response.text[:200]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    # Test 3: Empty text\n",
    "    print(\"\\n3. Testing empty text...\")\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{base_url}/embeddings\",\n",
    "            json={\"text\": \"\"},\n",
    "            timeout=10\n",
    "        )\n",
    "        print(f\"   Status: {response.status_code}\")\n",
    "        print(f\"   Response: {response.text[:200]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    # Test 4: Very large batch\n",
    "    print(\"\\n4. Testing very large batch...\")\n",
    "    large_batch = [\"E119 I10 N6320\"] * 1000  # 1000 identical claims\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            f\"{base_url}/embeddings_batch\",\n",
    "            json={\"texts\": large_batch},\n",
    "            timeout=30  # Longer timeout for large batch\n",
    "        )\n",
    "        response_time = time.time() - start_time\n",
    "        print(f\"   Status: {response.status_code}\")\n",
    "        print(f\"   Response time: {response_time:.2f}s\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"   Embeddings received: {len(data.get('embeddings', []))}\")\n",
    "        else:\n",
    "            print(f\"   Response: {response.text[:200]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "test_error_scenarios()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Performance Testing\n",
    "Test API performance with different batch sizes and measure throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance testing with different batch sizes\n",
    "def test_performance():\n",
    "    print(\"\\n‚ö° Performance testing:\")\n",
    "    \n",
    "    batch_sizes = [1, 5, 10, 20, 32]\n",
    "    performance_results = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\nüìä Testing batch size: {batch_size}\")\n",
    "        \n",
    "        # Create test batch\n",
    "        test_batch = (sample_claims * ((batch_size // len(sample_claims)) + 1))[:batch_size]\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = requests.post(\n",
    "                f\"{base_url}/embeddings_batch\",\n",
    "                json={\"texts\": test_batch},\n",
    "                timeout=timeout\n",
    "            )\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                throughput = batch_size / response_time\n",
    "                avg_time_per_item = response_time / batch_size\n",
    "                \n",
    "                result = {\n",
    "                    'batch_size': batch_size,\n",
    "                    'response_time': response_time,\n",
    "                    'throughput': throughput,\n",
    "                    'avg_time_per_item': avg_time_per_item,\n",
    "                    'status': 'success'\n",
    "                }\n",
    "                \n",
    "                print(f\"   ‚úÖ Success: {response_time:.2f}s, {throughput:.1f} items/sec\")\n",
    "            else:\n",
    "                result = {\n",
    "                    'batch_size': batch_size,\n",
    "                    'status': 'failed',\n",
    "                    'error': response.text[:100]\n",
    "                }\n",
    "                print(f\"   ‚ùå Failed: {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                'batch_size': batch_size,\n",
    "                'status': 'error',\n",
    "                'error': str(e)\n",
    "            }\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "        \n",
    "        performance_results.append(result)\n",
    "        \n",
    "        # Brief pause between tests\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return performance_results\n",
    "\n",
    "# Run performance tests\n",
    "if batch_embeddings_success:  # Only run if basic batch endpoint works\n",
    "    perf_results = test_performance()\n",
    "    \n",
    "    # Display results summary\n",
    "    print(\"\\nüìà Performance Summary:\")\n",
    "    successful_results = [r for r in perf_results if r['status'] == 'success']\n",
    "    \n",
    "    if successful_results:\n",
    "        best_throughput = max(successful_results, key=lambda x: x['throughput'])\n",
    "        print(f\"   üèÜ Best throughput: {best_throughput['throughput']:.1f} items/sec (batch size {best_throughput['batch_size']})\")\n",
    "        \n",
    "        fastest_response = min(successful_results, key=lambda x: x['avg_time_per_item'])\n",
    "        print(f\"   ‚ö° Fastest per item: {fastest_response['avg_time_per_item']:.3f}s (batch size {fastest_response['batch_size']})\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Skipping performance tests (batch endpoint not working)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Retry Logic\n",
    "Test the retry mechanism by simulating network issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retry logic with timeout simulation\n",
    "def test_retry_logic():\n",
    "    print(\"\\nüîÑ Testing retry logic:\")\n",
    "    \n",
    "    max_retries = api_config.max_retries\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        print(f\"\\n   Attempt {attempt + 1}/{max_retries + 1}:\")\n",
    "        \n",
    "        try:\n",
    "            # Use very short timeout to simulate network issues\n",
    "            response = requests.post(\n",
    "                f\"{base_url}/embeddings\",\n",
    "                json={\"text\": sample_claims[0]},\n",
    "                timeout=0.001  # Very short timeout to force failure\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                print(f\"      ‚úÖ Success on attempt {attempt + 1}\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"      ‚ö†Ô∏è  HTTP {response.status_code} on attempt {attempt + 1}\")\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"      ‚è±Ô∏è  Timeout on attempt {attempt + 1}\")\n",
    "            if attempt < max_retries:\n",
    "                wait_time = 2 ** attempt  # Exponential backoff\n",
    "                print(f\"      üîÑ Waiting {wait_time}s before retry...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"      ‚ùå Max retries ({max_retries}) exceeded\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ùå Error on attempt {attempt + 1}: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\nüí° In production, the pipeline implements exponential backoff with jitter\")\n",
    "    print(\"   to handle temporary network issues gracefully.\")\n",
    "\n",
    "# Run retry test\n",
    "test_retry_logic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Summary and Recommendations\n",
    "Analyze all test results and provide recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize test results\n",
    "print(\"\\nüìã API Connectivity Test Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test results summary\n",
    "tests = [\n",
    "    (\"Single Embedding\", embeddings_success),\n",
    "    (\"Batch Embeddings\", batch_embeddings_success),\n",
    "    (\"Single Generation\", generate_success),\n",
    "    (\"Batch Generation\", batch_generate_success)\n",
    "]\n",
    "\n",
    "passed = sum(1 for _, success in tests if success)\n",
    "total = len(tests)\n",
    "\n",
    "print(f\"\\nüéØ Overall Success Rate: {passed}/{total} ({passed/total*100:.0f}%)\")\n",
    "\n",
    "for test_name, success in tests:\n",
    "    status = \"‚úÖ PASS\" if success else \"‚ùå FAIL\"\n",
    "    print(f\"   {status} {test_name}\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nüí° Recommendations:\")\n",
    "\n",
    "if passed == total:\n",
    "    print(\"   üéâ All tests passed! Your API is ready for pipeline integration.\")\n",
    "    print(\"   ‚úÖ Proceed to test the embedding pipeline with this configuration.\")\n",
    "elif passed >= total * 0.5:\n",
    "    print(\"   ‚ö†Ô∏è  Some tests failed. Check the specific endpoint issues above.\")\n",
    "    print(\"   üîß Focus on fixing the failed endpoints before running the full pipeline.\")\n",
    "else:\n",
    "    print(\"   üö® Major connectivity issues detected.\")\n",
    "    print(\"   üîß Check server status, URL configuration, and network connectivity.\")\n",
    "\n",
    "print(\"\\nüîß Configuration for pipeline:\")\n",
    "print(f\"   model_api:\")\n",
    "print(f\"     base_url: \\\"{base_url}\\\"\")\n",
    "print(f\"     batch_size: {api_config.batch_size}\")\n",
    "print(f\"     timeout: {api_config.timeout}\")\n",
    "print(f\"     max_retries: {api_config.max_retries}\")\n",
    "\n",
    "if 'perf_results' in locals() and successful_results:\n",
    "    optimal_batch = best_throughput['batch_size']\n",
    "    print(f\"\\n‚ö° Performance optimization:\")\n",
    "    print(f\"   üí° Consider using batch_size: {optimal_batch} for optimal throughput\")\n",
    "    print(f\"   üìä Expected throughput: {best_throughput['throughput']:.1f} items/sec\")\n",
    "\n",
    "print(\"\\nüìö Next steps:\")\n",
    "print(\"   1. Fix any failed endpoint tests\")\n",
    "print(\"   2. Run test_02_Data_Loading_Validation.ipynb\")\n",
    "print(\"   3. Then proceed to test_03_Embedding_Pipeline_Debug.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Debug Cell (Run if needed)\n",
    "Use this cell to test specific scenarios or debug issues found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug cell - modify as needed for specific testing\n",
    "\n",
    "# Example: Test with custom URL\n",
    "# custom_url = \"http://your-server:8000\"\n",
    "# response = requests.get(f\"{custom_url}/health\", timeout=10)\n",
    "# print(f\"Custom URL test: {response.status_code}\")\n",
    "\n",
    "# Example: Test with custom payload\n",
    "# custom_payload = {\"text\": \"Your custom medical claim here\"}\n",
    "# response = requests.post(f\"{base_url}/embeddings\", json=custom_payload, timeout=30)\n",
    "# print(f\"Custom payload test: {response.status_code}\")\n",
    "\n",
    "# Example: Check response headers for debugging\n",
    "# if 'response' in locals():\n",
    "#     print(\"Response headers:\")\n",
    "#     for key, value in response.headers.items():\n",
    "#         print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"üí° Use this cell to run custom tests and debug specific issues.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}