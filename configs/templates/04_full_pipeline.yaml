# =============================================================================
# COMPLETE END-TO-END PIPELINE WORKFLOW
# =============================================================================
# Purpose: Full evaluation pipeline comparing embedding-based vs target word methods
# Cost: HIGH (requires both embedding generation and text generation API calls)
# Use case: Comprehensive model evaluation with method comparison
# Prerequisites: Dataset and target medical codes

# =============================================================================
# INPUT CONFIGURATION
# =============================================================================
input:
  dataset_path: "data/medical_claims.csv"  # üëà UPDATE: Path to your CSV file
  split_ratio: 0.8                        # 80% train, 20% test

# =============================================================================
# JOB CONFIGURATION
# =============================================================================
job:
  name: "full_pipeline_evaluation"
  output_dir: "outputs"
  random_seed: 42

# =============================================================================
# MODEL API CONFIGURATION
# =============================================================================
model_api:
  base_url: "http://localhost:8000"        # üëà UPDATE: Your model server URL
  batch_size: 32                           # Requests per batch
  timeout: 300                             # 5 minute timeout
  max_retries: 3

# =============================================================================
# PIPELINE STAGES - Run everything
# =============================================================================
pipeline_stages:
  embeddings: true           # ‚úÖ Generate embeddings
  classification: true       # ‚úÖ Train classifiers
  evaluation: true           # ‚úÖ Evaluate models
  target_word_eval: true     # ‚úÖ Run target word evaluation
  summary_report: true       # ‚úÖ Create summary
  method_comparison: true    # ‚úÖ Compare both methods

# =============================================================================
# DATA PROCESSING CONFIGURATION
# =============================================================================
data_processing:
  random_seed: 42
  max_sequence_length: 512
  include_mcid: true
  output_format: "json"
  train_test_split: 0.8

# =============================================================================
# EMBEDDING GENERATION CONFIGURATION
# =============================================================================
embedding_generation:
  batch_size: 16                           # Claims per processing batch
  save_interval: 100                       # Save progress every N batches
  checkpoint_dir: "outputs/checkpoints"
  resume_from_checkpoint: true             # Resume from checkpoint if available
  tokenizer_path: "/app/tokenizer"

# =============================================================================
# CLASSIFICATION CONFIGURATION
# =============================================================================
classification:
  models: ["logistic_regression", "svm", "random_forest"]  # Train all three
  
  cross_validation:
    n_folds: 5
    scoring: "roc_auc"
    n_jobs: -1                          # Use all CPU cores
  
  hyperparameter_search:
    logistic_regression:
      C: [0.001, 0.01, 0.1, 1, 10, 100]
      penalty: ["l1", "l2"]
      solver: ["liblinear", "saga"]
    
    svm:
      C: [0.1, 1, 10]
      kernel: ["rbf", "linear"]
      gamma: ["scale", "auto"]
    
    random_forest:
      n_estimators: [100, 200, 300]
      max_depth: [10, 20, 30, null]
      min_samples_split: [2, 5, 10]

# =============================================================================
# EVALUATION CONFIGURATION
# =============================================================================
evaluation:
  metrics: ["accuracy", "precision", "recall", "f1_score", "roc_auc", "confusion_matrix"]
  
  visualization:
    generate_plots: true
    plot_formats: ["png", "pdf"]
    dpi: 300

# =============================================================================
# TARGET WORD EVALUATION CONFIGURATION
# =============================================================================
target_word_evaluation:
  enable: true                             # ‚úÖ Enable for comparison
  
  # üëà UPDATE: Your target medical codes
  target_codes: ["E119", "76642", "N6320", "K9289", "O0903", "Z91048", "M1710"]
  # Alternative: Load codes from file
  # target_codes_file: "configs/target_codes.txt"
  
  generations_per_prompt: 10               # Generate 10 times per input
  max_new_tokens: 200                      # Generate up to 200 tokens
  temperature: 0.8                         # Sampling temperature
  top_k: 50                               # Top-k sampling
  search_method: "exact"                   # Exact code matching

# =============================================================================
# OUTPUT CONFIGURATION
# =============================================================================
output:
  embeddings_dir: "outputs/embeddings"
  models_dir: "outputs/models"
  metrics_dir: "outputs/metrics"
  logs_dir: "outputs/logs"
  save_best_model_only: false
  model_format: "pickle"

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
logging:
  level: "INFO"
  console_level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "outputs/logs/pipeline.log"

# =============================================================================
# OUTPUTS - What you'll get
# =============================================================================
# üìÅ outputs/full_pipeline_evaluation/
# ‚îú‚îÄ‚îÄ embeddings/
# ‚îÇ   ‚îú‚îÄ‚îÄ train_embeddings.json          ‚Üê Generated embeddings
# ‚îÇ   ‚îî‚îÄ‚îÄ test_embeddings.json
# ‚îú‚îÄ‚îÄ models/
# ‚îÇ   ‚îú‚îÄ‚îÄ logistic_regression_model.pkl  ‚Üê Trained models
# ‚îÇ   ‚îú‚îÄ‚îÄ svm_model.pkl
# ‚îÇ   ‚îî‚îÄ‚îÄ random_forest_model.pkl
# ‚îú‚îÄ‚îÄ metrics/
# ‚îÇ   ‚îú‚îÄ‚îÄ logistic_regression/            ‚Üê Classifier evaluation results
# ‚îÇ   ‚îú‚îÄ‚îÄ svm/
# ‚îÇ   ‚îú‚îÄ‚îÄ random_forest/
# ‚îÇ   ‚îî‚îÄ‚îÄ target_word_evaluation/         ‚Üê Target word evaluation results
# ‚îú‚îÄ‚îÄ summary/
# ‚îÇ   ‚îú‚îÄ‚îÄ pipeline_summary.json          ‚Üê Overall results
# ‚îÇ   ‚îî‚îÄ‚îÄ method_comparison.json         ‚Üê Method comparison analysis
# ‚îî‚îÄ‚îÄ logs/
#     ‚îî‚îÄ‚îÄ pipeline.log

# =============================================================================
# USAGE
# =============================================================================
# 1. Update input.dataset_path with your CSV file
# 2. Update model_api.base_url with your model server URL
# 3. Update target_word_evaluation.target_codes with your medical codes
# 4. Run: python main.py run-all --config configs/templates/04_full_pipeline.yaml
# 5. Check method_comparison.json for which approach performs better
#
# WHAT THIS PIPELINE DOES:
# 1. Generates embeddings from your medical claims data
# 2. Trains multiple classifiers on the embeddings
# 3. Evaluates each classifier on test data
# 4. Runs target word evaluation using text generation
# 5. Compares both approaches and recommends the best method
#
# EXPECTED RUNTIME:
# - Embedding generation: 10-60 minutes (depends on dataset size)
# - Classification training: 5-15 minutes
# - Model evaluation: 2-5 minutes
# - Target word evaluation: 15-45 minutes (depends on dataset size)
# - Total: 30-120 minutes for typical datasets